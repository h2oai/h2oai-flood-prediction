{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI-Powered Flood Prediction System with h2oGPTe and NVIDIA NIM\n",
    "\n",
    "[![Deploy on NVIDIA](https://img.shields.io/badge/Deploy%20on-NVIDIA%20AI%20Blueprints-76B900?logo=nvidia&logoColor=white)](https://build.nvidia.com)\n",
    "[![H2O.ai](https://img.shields.io/badge/Powered%20by-H2O.ai-FFD500)](https://h2o.ai)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒŠ Overview\n",
    "\n",
    "This blueprint demonstrates an **AI-powered flood prediction and disaster response system** that combines:\n",
    "\n",
    "- **h2oGPTe Agent-to-Agent (A2A)**: Advanced AutoML capabilities with Driverless AI for model training and feature engineering\n",
    "- **NVIDIA NIM**: State-of-the-art inference with `nvidia/llama-3.3-nemotron-super-49b-v1.5` and other NVIDIA models\n",
    "- **NVIDIA NAT Pipeline**: React Agent workflows for multi-agent orchestration\n",
    "- **FastMCP Server**: 20+ specialized tools across 5 intelligent agents\n",
    "- **Real-time Data Integration**: USGS Water Services, NOAA Forecasts, and Weather APIs\n",
    "\n",
    "### ğŸ¯ Use Case: AI for Good - Disaster Response\n",
    "\n",
    "This system provides:\n",
    "- **Real-time flood monitoring** with live data from watersheds and monitoring stations\n",
    "- **AI-powered risk assessment** using advanced machine learning models\n",
    "- **Emergency response coordination** with automated alerts and evacuation planning\n",
    "- **Predictive analytics** for flood forecasting 24-72 hours ahead\n",
    "- **AutoML model training** for continuous improvement of prediction accuracy\n",
    "\n",
    "### ğŸ—ï¸ Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      Flood Prediction System                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚   h2oGPTe    â”‚  â”‚  NVIDIA NIM  â”‚  â”‚  FastMCP     â”‚         â”‚\n",
    "â”‚  â”‚  (A2A Mode)  â”‚  â”‚  (Nemotron)  â”‚  â”‚   Server     â”‚         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚         â”‚                 â”‚                 â”‚                   â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                  â”‚                 â”‚                            â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚         â”‚    NVIDIA NAT Agent Pipeline      â”‚                  â”‚\n",
    "â”‚         â”‚      (React Agent Workflow)       â”‚                  â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                  â”‚                                              â”‚\n",
    "â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\n",
    "â”‚    â”‚    5 Specialized Agents   â”‚                               â”‚\n",
    "â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â”‚\n",
    "â”‚    â”‚  1. Data Collector        â”‚ â—„â”€â”€ USGS Water Data           â”‚\n",
    "â”‚    â”‚  2. Risk Analyzer         â”‚ â—„â”€â”€ NOAA Flood Alerts         â”‚\n",
    "â”‚    â”‚  3. Emergency Responder   â”‚ â—„â”€â”€ Weather APIs              â”‚\n",
    "â”‚    â”‚  4. AI Predictor          â”‚                               â”‚\n",
    "â”‚    â”‚  5. H2OGPTE ML Agent      â”‚                               â”‚\n",
    "â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  Output: Real-time Monitoring, Alerts, Predictions, ML Models  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ”‘ Key Technologies\n",
    "\n",
    "1. **h2oGPTe**: Enterprise AI platform with agent mode for AutoML and advanced analytics\n",
    "2. **NVIDIA NIM**: Optimized inference microservices for AI models\n",
    "3. **NVIDIA NAT**: Agent orchestration framework with React-based workflows\n",
    "4. **FastMCP**: Model Context Protocol server for tool integration\n",
    "5. **FastAPI**: High-performance API server for real-time operations\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ What You'll Learn\n",
    "\n",
    "- Setting up multi-agent AI systems for disaster response\n",
    "- Integrating h2oGPTe for AutoML and model training\n",
    "- Using NVIDIA NIM for high-performance inference\n",
    "- Building NAT agent workflows with React patterns\n",
    "- Implementing FastMCP servers with custom tools\n",
    "- Real-time data integration from government APIs\n",
    "- Coordinating multiple AI agents for complex tasks\n",
    "\n",
    "Let's get started! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "Before running this setup, make sure you have:\n",
    "\n",
    "### Required:\n",
    "- âœ… **NVIDIA API Key** - Get it from [build.nvidia.com](https://build.nvidia.com)\n",
    "  - This key is used for NVIDIA NIM models and services\n",
    "\n",
    "### Optional (but recommended):\n",
    "- ğŸ”¹ **H2OGPTE API Key** - For AutoML and advanced AI features\n",
    "  - Get access at [h2o.ai](https://h2o.ai/platform/enterprise-h2ogpte/)\n",
    "  - If you don't have this, you can skip it - the app will still work with reduced features\n",
    "\n",
    "---\n",
    "\n",
    "Ready? Let's collect your API keys! ğŸ”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Dependencies\n",
    "\n",
    "Please restart the kernel after this step. Do not repeat this step after restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "python = sys.executable\n",
    "\n",
    "!{python} -m ensurepip --upgrade\n",
    "!{python} -m pip install --upgrade pip setuptools wheel\n",
    "!{python} -m pip install --upgrade --force-reinstall pandas openai requests python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” Step 1: Collect API Keys\n",
    "\n",
    "Run the cells below to securely enter your API keys. Your inputs will be hidden for security.\n",
    "\n",
    "### What you'll provide:\n",
    "1. **NVIDIA API Key** (required)\n",
    "2. **H2OGPTE API Key** (optional)\n",
    "3. **H2OGPTE URL** (optional, default provided)\n",
    "\n",
    "**Note**: Ports are pre-configured in this Launchable (8090 for Web UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import getpass\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ Enter your NVIDIA API Key\n",
      "   Get it from: https://build.nvidia.com or https://catalog.ngc.nvidia.com/\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "NVIDIA API Key:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NVIDIA API Key collected successfully!\n",
      "   Preview: nvapi-hR2s...o3QS\n"
     ]
    }
   ],
   "source": [
    "# Collect NVIDIA API Key (Required)\n",
    "print(\"ğŸ”‘ Enter your NVIDIA API Key\")\n",
    "print(\"   Get it from: https://build.nvidia.com or https://catalog.ngc.nvidia.com/\")\n",
    "print()\n",
    "\n",
    "nvidia_api_key = getpass.getpass(\"NVIDIA API Key: \")\n",
    "\n",
    "if not nvidia_api_key or nvidia_api_key.strip() == \"\":\n",
    "    raise ValueError(\n",
    "        \"âŒ NVIDIA API Key is required! Please run this cell again and provide the key.\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… NVIDIA API Key collected successfully!\")\n",
    "print(f\"   Preview: {nvidia_api_key[:10]}...{nvidia_api_key[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ Enter your NGC API Key\n",
      "   Get it from: https://catalog.ngc.nvidia.com/\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "NGC API Key:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NGC API Key collected successfully!\n",
      "   Preview: nvapi-aoBi...R1Ep\n"
     ]
    }
   ],
   "source": [
    "# Collect NGC API Key (Optional)\n",
    "print(\"ğŸ”‘ Enter your NGC API Key\")\n",
    "print(\"   Get it from: https://catalog.ngc.nvidia.com/\")\n",
    "print()\n",
    "\n",
    "ngc_api_key = getpass.getpass(\"NGC API Key: \")\n",
    "\n",
    "if not ngc_api_key or ngc_api_key.strip() == \"\":\n",
    "    print(\"âš ï¸  No API key provided, skipping Local NIM LLM setup\")\n",
    "else:\n",
    "    print(\"âœ… NGC API Key collected successfully!\")\n",
    "    print(f\"   Preview: {ngc_api_key[:10]}...{ngc_api_key[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– H2OGPTE Configuration (Optional)\n",
      "   H2OGPTE provides advanced AutoML and AI agent capabilities\n",
      "   If you don't have access, just press Enter to skip\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you have H2OGPTE access? (yes/no) [no]:  yes\n",
      "H2OGPTE URL [https://h2ogpte.cloud-dev.h2o.dev]:  \n",
      "H2OGPTE API Key:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… H2OGPTE configured successfully!\n",
      "   URL: https://h2ogpte.cloud-dev.h2o.dev\n",
      "   API Key Preview: sk-ktl41qb...tiqB\n"
     ]
    }
   ],
   "source": [
    "# Collect H2OGPTE Credentials (Optional)\n",
    "print(\"ğŸ¤– H2OGPTE Configuration (Optional)\")\n",
    "print(\"   H2OGPTE provides advanced AutoML and AI agent capabilities\")\n",
    "print(\"   If you don't have access, just press Enter to skip\")\n",
    "print()\n",
    "\n",
    "use_h2ogpte = input(\"Do you have H2OGPTE access? (yes/no) [no]: \").strip().lower()\n",
    "\n",
    "if use_h2ogpte in [\"yes\", \"y\"]:\n",
    "    h2ogpte_url = input(\"H2OGPTE URL [https://h2ogpte.cloud-dev.h2o.dev]: \").strip()\n",
    "    h2ogpte_api_key = getpass.getpass(\"H2OGPTE API Key: \")\n",
    "\n",
    "    if not h2ogpte_url:\n",
    "        h2ogpte_url = \"https://h2ogpte.cloud-dev.h2o.dev\"\n",
    "\n",
    "    if not h2ogpte_api_key or h2ogpte_api_key.strip() == \"\":\n",
    "        print(\"âš ï¸  No API key provided, skipping H2OGPTE setup\")\n",
    "        h2ogpte_api_key = \"\"\n",
    "        h2ogpte_url = \"\"\n",
    "    else:\n",
    "        print(f\"âœ… H2OGPTE configured successfully!\")\n",
    "        print(f\"   URL: {h2ogpte_url}\")\n",
    "        print(f\"   API Key Preview: {h2ogpte_api_key[:10]}...{h2ogpte_api_key[-4:]}\")\n",
    "else:\n",
    "    h2ogpte_api_key = \"\"\n",
    "    h2ogpte_url = \"https://h2ogpte.cloud-dev.h2o.dev\"\n",
    "    print(\"â­ï¸  Skipping H2OGPTE setup - application will run with NVIDIA NIM only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Step 2: Generate Configuration File\n",
    "\n",
    "Now we'll create the environment configuration file with all the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper function created successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_env_file(\n",
    "    nvidia_api_key,\n",
    "    ngc_api_key,\n",
    "    h2ogpte_api_key,\n",
    "    h2ogpte_url,\n",
    "    output_path=\"./flood_prediction.env\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create the environment configuration file for the Flood Prediction application.\n",
    "\n",
    "    Parameters:\n",
    "    - nvidia_api_key: NVIDIA API key (used for both NVIDIA_API_KEY)\n",
    "    - ngc_api_key: NGC API key (used for NGC_API_KEY)\n",
    "    - h2ogpte_api_key: H2OGPTE API key (can be empty string if not used)\n",
    "    - h2ogpte_url: H2OGPTE service URL\n",
    "    - output_path: Where to save the env file\n",
    "\n",
    "    Note: Ports are pre-configured in the Launchable:\n",
    "    - WEB_PORT: 8090 (fixed)\n",
    "    \"\"\"\n",
    "\n",
    "    env_content = f\"\"\"# ==============================================================================\n",
    "# API KEYS\n",
    "# ==============================================================================\n",
    "\n",
    "NVIDIA_API_KEY={nvidia_api_key}\n",
    "NGC_API_KEY={ngc_api_key}\n",
    "H2OGPTE_API_KEY={h2ogpte_api_key}\n",
    "\n",
    "# ==============================================================================\n",
    "# H2OGPTE CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# H2OGPTE service URL\n",
    "H2OGPTE_URL={h2ogpte_url}\n",
    "\n",
    "# H2OGPTE model to use\n",
    "H2OGPTE_MODEL=claude-sonnet-4-20250514\n",
    "\n",
    "# ==============================================================================\n",
    "# WEB APPLICATION IMAGE CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "WEB_IMAGE_REGISTRY=h2oairelease\n",
    "WEB_IMAGE_REPOSITORY=h2oai-floodprediction-app\n",
    "WEB_IMAGE_TAG=v1.0.0\n",
    "\n",
    "WEB_PORT=8090\n",
    "\n",
    "# ==============================================================================\n",
    "# REDIS CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "REDIS_IMAGE_REGISTRY=docker.io\n",
    "REDIS_IMAGE_REPOSITORY=redis\n",
    "REDIS_IMAGE_TAG=8.2.1\n",
    "\n",
    "REDIS_ENABLED=true\n",
    "\n",
    "# ==============================================================================\n",
    "# NVIDIA NIM LLM CONFIGURATION (Optional)\n",
    "# ==============================================================================\n",
    "\n",
    "NIMLLM_IMAGE=nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0\n",
    "\n",
    "NIMLLM_PORT=8989\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(env_content)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "print(\"âœ… Helper function created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¨ Generating environment configuration file...\n",
      "\n",
      "âœ… Configuration file created successfully!\n",
      "   Location: /home/shadeform/h2oai-flood-prediction-agent/notebooks/flood_prediction.env\n",
      "\n",
      "ğŸ“‹ Configuration Summary:\n",
      "   NVIDIA API Key: nvapi-hR2s...o3QS\n",
      "   H2OGPTE API Key: sk-ktl41qb...tiqB\n",
      "   H2OGPTE URL: https://h2ogpte.cloud-dev.h2o.dev\n",
      "\n",
      "ğŸ”Œ Pre-configured Ports:\n",
      "   Web UI Port: 8090 (fixed)\n"
     ]
    }
   ],
   "source": [
    "# Generate the environment file\n",
    "print(\"ğŸ”¨ Generating environment configuration file...\")\n",
    "print()\n",
    "\n",
    "env_file_path = create_env_file(\n",
    "    nvidia_api_key=nvidia_api_key,\n",
    "    ngc_api_key=ngc_api_key,\n",
    "    h2ogpte_api_key=h2ogpte_api_key,\n",
    "    h2ogpte_url=h2ogpte_url,\n",
    ")\n",
    "\n",
    "print(\"âœ… Configuration file created successfully!\")\n",
    "print(f\"   Location: {os.path.abspath(env_file_path)}\")\n",
    "print()\n",
    "print(\"ğŸ“‹ Configuration Summary:\")\n",
    "print(f\"   NVIDIA API Key: {nvidia_api_key[:10]}...{nvidia_api_key[-4:]}\")\n",
    "if h2ogpte_api_key:\n",
    "    print(f\"   H2OGPTE API Key: {h2ogpte_api_key[:10]}...{h2ogpte_api_key[-4:]}\")\n",
    "    print(f\"   H2OGPTE URL: {h2ogpte_url}\")\n",
    "else:\n",
    "    print(\"   H2OGPTE: Not configured (optional)\")\n",
    "print()\n",
    "print(\"ğŸ”Œ Pre-configured Ports:\")\n",
    "print(\"   Web UI Port: 8090 (fixed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./flood_prediction.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ³ Step 3: Pull Docker Images\n",
    "\n",
    "Now we'll authenticate with Docker registries and pull the required images.\n",
    "\n",
    "This may take 5-10 minutes depending on your connection speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull WEB APPLICATION IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.0.0: Pulling from h2oairelease/h2oai-floodprediction-app\n",
      "\n",
      "\u001b[1B16127147: Pulling fs layer \n",
      "\u001b[1B56726626: Pulling fs layer \n",
      "\u001b[1B3c681ade: Pulling fs layer \n",
      "\u001b[1B1c640d63: Pulling fs layer \n",
      "\u001b[1Bc61f0cf1: Pulling fs layer \n",
      "\u001b[1B39069d96: Pulling fs layer \n",
      "\u001b[1B1602d0cd: Pulling fs layer \n",
      "\u001b[1B01a03a30: Pulling fs layer \n",
      "\u001b[1B25125ae4: Pulling fs layer \n",
      "\u001b[1B4a6396a4: Pulling fs layer \n",
      "\u001b[1Bd1d3fea4: Pulling fs layer \n",
      "\u001b[1Ba80146bc: Pulling fs layer \n",
      "\u001b[10Bc640d63: Waiting fs layer \n",
      "\u001b[1Bf482c79d: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:9aafbdba37ad48cd9d7fd02a8e8f30cfa2ea349e3bf6e976892818e6190ee0c9\n",
      "Status: Downloaded newer image for h2oairelease/h2oai-floodprediction-app:v1.0.0\n",
      "docker.io/h2oairelease/h2oai-floodprediction-app:v1.0.0\n"
     ]
    }
   ],
   "source": [
    "!docker pull h2oairelease/h2oai-floodprediction-app:v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                               TAG       IMAGE ID       CREATED       SIZE\n",
      "h2oairelease/h2oai-floodprediction-app   v1.0.0    92070ae047ee   5 hours ago   1.84GB\n"
     ]
    }
   ],
   "source": [
    "!docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional - If Nvidia H200 GPU or equivalent is available, Deploy a local NIM LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if Nvidia Drivers are installed and GPU is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA H200 (UUID: GPU-9d54e21b-2dda-9575-880e-e46227f39c92)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate Docker with NGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING! Your credentials are stored unencrypted in '/home/shadeform/.docker/config.json'.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/go/credential-store/\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull Nvidia NIM LLM Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The image is defined in the env file as NIMLLM_IMAGE\n",
    "\n",
    "nim_llm_image = os.getenv(\"NIMLLM_IMAGE\", \"nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0\")\n",
    "nim_llm_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0: Pulling from nim/nvidia/llama-3_3-nemotron-super-49b-v1_5\n",
      "\n",
      "\u001b[1B659d501e: Pulling fs layer \n",
      "\u001b[1B62216b43: Pulling fs layer \n",
      "\u001b[1B1deb19fb: Pulling fs layer \n",
      "\u001b[1B5756930d: Pulling fs layer \n",
      "\u001b[1B29b885e2: Pulling fs layer \n",
      "\u001b[1B57a5ed7e: Pulling fs layer \n",
      "\u001b[1Bdbc78db2: Pulling fs layer \n",
      "\u001b[1Ba918a236: Pulling fs layer \n",
      "\u001b[1B0e8d8ddf: Pulling fs layer \n",
      "\u001b[1B49471103: Pulling fs layer \n",
      "\u001b[1Bf057cbd8: Pulling fs layer \n",
      "\u001b[1B358f4877: Pulling fs layer \n",
      "\u001b[1B1bcaafa5: Pulling fs layer \n",
      "\u001b[1Bf839ffe1: Pulling fs layer \n",
      "\u001b[1Bf6dc426b: Pulling fs layer \n",
      "\u001b[1Be464fcb1: Pulling fs layer \n",
      "\u001b[1Bb1118d17: Pulling fs layer \n",
      "\u001b[1B85d69fc8: Pulling fs layer \n",
      "\u001b[1B41334f2f: Pulling fs layer \n",
      "\u001b[14Bbc78db2: Waiting fs layer \n",
      "\u001b[1B4adbabc8: Pulling fs layer \n",
      "\u001b[15B918a236: Waiting fs layer \n",
      "\u001b[15Be8d8ddf: Waiting fs layer \n",
      "\u001b[1B6dc2ebc7: Pulling fs layer \n",
      "\u001b[14B58f4877: Waiting fs layer \n",
      "\u001b[14Bbcaafa5: Waiting fs layer \n",
      "\u001b[14B839ffe1: Waiting fs layer \n",
      "\u001b[14B6dc426b: Waiting fs layer \n",
      "\u001b[14B464fcb1: Waiting fs layer \n",
      "\u001b[14B1118d17: Waiting fs layer \n",
      "\u001b[14B5d69fc8: Waiting fs layer \n",
      "\u001b[14B1334f2f: Waiting fs layer \n",
      "\u001b[1B580c0ce5: Pulling fs layer \n",
      "\u001b[31B756930d: Waiting fs layer \n",
      "\u001b[31B9b885e2: Waiting fs layer \n",
      "\u001b[16Badbabc8: Waiting fs layer \n",
      "\u001b[32B7a5ed7e: Waiting fs layer \n",
      "\u001b[16Bc629334: Waiting fs layer \n",
      "\u001b[9B179383c8: Waiting fs layer \n",
      "\u001b[6Bb7a8c760: Waiting fs layer \n",
      "\u001b[10B4dd13ec: Waiting fs layer \n",
      "\u001b[6Ba6a208b9: Waiting fs layer \n",
      "\u001b[1B1f207990: Waiting fs layer \n",
      "\u001b[1Bb7913af2: Pulling fs layer \n",
      "\u001b[1B96164f4a: Pulling fs layer \n",
      "\u001b[1B552913f6: Pulling fs layer \n",
      "\u001b[3B96164f4a: Waiting fs layer \n",
      "\u001b[5Bb7913af2: Waiting fs layer \n",
      "\u001b[1B1ea4d518: Pulling fs layer \n",
      "\u001b[1Bc59ddade: Pulling fs layer \n",
      "\u001b[1Bbb097532: Pulling fs layer \n",
      "\u001b[4B1ea4d518: Waiting fs layer \n",
      "\u001b[1B0daaa29a: Pulling fs layer \n",
      "\u001b[4Bbb097532: Waiting fs layer \n",
      "\u001b[1B2d16c627: Pulling fs layer \n",
      "\u001b[2B2d16c627: Waiting fs layer \n",
      "\u001b[1Bb9a56da9: Pulling fs layer \n",
      "\u001b[1Bd880e47f: Pulling fs layer \n",
      "\u001b[1Bc05c9aa2: Pulling fs layer \n",
      "\u001b[1B326d07d5: Pulling fs layer \n",
      "\u001b[3Bc05c9aa2: Waiting fs layer \n",
      "\u001b[5Bd880e47f: Waiting fs layer \n",
      "\u001b[3B9ad8979c: Waiting fs layer \n",
      "\u001b[3B17978ee0: Waiting fs layer \n",
      "\u001b[2B4eadc402: Waiting fs layer \n",
      "\u001b[1B4875b4b8: Pulling fs layer \n",
      "\u001b[1Bda2882dc: Pulling fs layer \n",
      "\u001b[1B549692fd: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:06af8d8467217fd6f344a53cc37ba2d1d234cb98d918b1ecdf85bf1c16854262\n",
      "Status: Downloaded newer image for nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0\n",
      "nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0\n"
     ]
    }
   ],
   "source": [
    "# pull image\n",
    "\n",
    "!docker pull \"${NIMLLM_IMAGE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                             TAG       IMAGE ID       CREATED        SIZE\n",
      "h2oairelease/h2oai-floodprediction-app                 v1.0.0    92070ae047ee   5 hours ago    1.84GB\n",
      "nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5   1.12.0    efe42d99222d   2 months ago   24.4GB\n"
     ]
    }
   ],
   "source": [
    "# check available images\n",
    "\n",
    "!docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the application using Docker Compose\n",
    "\n",
    "Run only one of the `docker compose` commands below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy with a local NVIDIA NIM LLM - **Only if NVIDIA GPU is available and NIM LLM Image was successfully pulled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 0/1\n",
      " \u001b[33mâ ‹\u001b[0m redis Pulling                                                           \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/1\n",
      " \u001b[33mâ ™\u001b[0m redis Pulling                                                           \u001b[34m0.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/1\n",
      " \u001b[33mâ ¹\u001b[0m redis Pulling                                                           \u001b[34m0.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/1\n",
      " \u001b[33mâ ¸\u001b[0m redis Pulling                                                           \u001b[34m0.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/1\n",
      " \u001b[33mâ ¼\u001b[0m redis Pulling                                                           \u001b[34m0.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/1\n",
      " \u001b[33mâ ´\u001b[0m redis Pulling                                                           \u001b[34m0.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/8\n",
      " \u001b[33mâ ¦\u001b[0m redis [\u001b[32mâ €â €â €â €â €â €â €\u001b[0m] Pulling                                                 \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[33mâ ‹\u001b[0m 5c32499ab806 Pulling fs layer                                         \u001b[34m0.1s \u001b[0m\n",
      "   \u001b[33mâ ‹\u001b[0m ebb6b8512082 Pulling fs layer                                         \u001b[34m0.1s \u001b[0m\n",
      "   \u001b[33mâ ‹\u001b[0m 7555a6149d73 Pulling fs layer                                         \u001b[34m0.1s \u001b[0m\n",
      "   \u001b[33mâ ‹\u001b[0m faa43fe37ea4 Waiting                                                  \u001b[34m0.1s \u001b[0m\n",
      "   \u001b[33mâ ‹\u001b[0m fa3ef5872501 Waiting                                                  \u001b[34m0.1s \u001b[0m\n",
      "   \u001b[33mâ ‹\u001b[0m 4f4fb700ef54 Waiting                                                  \u001b[34m0.1s \u001b[0m\n",
      "   \u001b[33mâ ‹\u001b[0m 54e1e08620c3 Waiting                                                  \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 1/8\n",
      " \u001b[33mâ §\u001b[0m redis [\u001b[32mâ €â €â£¿â €â €â €â €\u001b[0m] Pulling                                                 \u001b[34m0.8s \u001b[0m\n",
      "   \u001b[33mâ ™\u001b[0m 5c32499ab806 Pulling fs layer                                         \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[33mâ ™\u001b[0m ebb6b8512082 Pulling fs layer                                         \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[33mâ ™\u001b[0m faa43fe37ea4 Waiting                                                  \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[33mâ ™\u001b[0m fa3ef5872501 Waiting                                                  \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[33mâ ™\u001b[0m 4f4fb700ef54 Waiting                                                  \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[33mâ ™\u001b[0m 54e1e08620c3 Waiting                                                  \u001b[34m0.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/8\n",
      " \u001b[33mâ ‡\u001b[0m redis [\u001b[32mâ €â£¿â£¿â €â €â €â €\u001b[0m] Pulling                                                 \u001b[34m0.9s \u001b[0m\n",
      "   \u001b[33mâ ¹\u001b[0m 5c32499ab806 Downloading  286.7kB/28.23MB                             \u001b[34m0.3s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[33mâ ¹\u001b[0m faa43fe37ea4 Waiting                                                  \u001b[34m0.3s \u001b[0m\n",
      "   \u001b[33mâ ¹\u001b[0m fa3ef5872501 Waiting                                                  \u001b[34m0.3s \u001b[0m\n",
      "   \u001b[33mâ ¹\u001b[0m 4f4fb700ef54 Waiting                                                  \u001b[34m0.3s \u001b[0m\n",
      "   \u001b[33mâ ¹\u001b[0m 54e1e08620c3 Waiting                                                  \u001b[34m0.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/8\n",
      " \u001b[33mâ \u001b[0m redis [\u001b[32mâ£€â£¿â£¿â €â €â €â €\u001b[0m] Pulling                                                 \u001b[34m1.0s \u001b[0m\n",
      "   \u001b[33mâ ¸\u001b[0m 5c32499ab806 Downloading  8.061MB/28.23MB                             \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[33mâ ¸\u001b[0m faa43fe37ea4 Downloading  256.3kB/24.19MB                             \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[33mâ ¸\u001b[0m fa3ef5872501 Waiting                                                  \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[33mâ ¸\u001b[0m 4f4fb700ef54 Waiting                                                  \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[33mâ ¸\u001b[0m 54e1e08620c3 Waiting                                                  \u001b[34m0.4s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 3/8\n",
      " \u001b[33mâ ‹\u001b[0m redis [\u001b[32mâ£„â£¿â£¿â£„â£¿â €â €\u001b[0m] Pulling                                                 \u001b[34m1.1s \u001b[0m\n",
      "   \u001b[33mâ ¼\u001b[0m 5c32499ab806 Downloading  12.11MB/28.23MB                             \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[33mâ ¼\u001b[0m faa43fe37ea4 Downloading  9.518MB/24.19MB                             \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[33mâ ¼\u001b[0m 4f4fb700ef54 Waiting                                                  \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[33mâ ¼\u001b[0m 54e1e08620c3 Waiting                                                  \u001b[34m0.5s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 4/8\n",
      " \u001b[33mâ ™\u001b[0m redis [\u001b[32mâ£¤â£¿â£¿â£¿â£¿â €â €\u001b[0m] Pulling                                                 \u001b[34m1.2s \u001b[0m\n",
      "   \u001b[33mâ ´\u001b[0m 5c32499ab806 Downloading  15.84MB/28.23MB                             \u001b[34m0.6s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[33mâ ´\u001b[0m 4f4fb700ef54 Waiting                                                  \u001b[34m0.6s \u001b[0m\n",
      "   \u001b[33mâ ´\u001b[0m 54e1e08620c3 Waiting                                                  \u001b[34m0.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/8\n",
      " \u001b[33mâ ¹\u001b[0m redis [\u001b[32mâ£¶â£¿â£¿â£¿â£¿â£¿â €\u001b[0m] Pulling                                                 \u001b[34m1.3s \u001b[0m\n",
      "   \u001b[33mâ ¦\u001b[0m 5c32499ab806 Downloading    21.91MB/28.23M...                         \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[33mâ ¦\u001b[0m 54e1e08620c3 Waiting                                                  \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ ¸\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 43.19MB / 52.42MB Pulling                               \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[33mâ §\u001b[0m 5c32499ab806 Extracting       294.9kB/28.23...                        \u001b[34m0.8s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ ¼\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 43.19MB / 52.42MB Pulling                               \u001b[34m1.5s \u001b[0m\n",
      "   \u001b[33mâ ‡\u001b[0m 5c32499ab806 Extracting       7.668MB/28.23...                        \u001b[34m0.9s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ ´\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 43.19MB / 52.42MB Pulling                               \u001b[34m1.6s \u001b[0m\n",
      "   \u001b[33mâ \u001b[0m 5c32499ab806 Extracting       13.86MB/28.23...                        \u001b[34m1.0s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ ¦\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 44.29MB / 52.42MB Pulling                               \u001b[34m1.7s \u001b[0m\n",
      "   \u001b[33mâ ‹\u001b[0m 5c32499ab806 Extracting          23MB/28.23...                        \u001b[34m1.1s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ §\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 46.94MB / 52.42MB Pulling                               \u001b[34m1.8s \u001b[0m\n",
      "   \u001b[33mâ ™\u001b[0m 5c32499ab806 Extracting       25.66MB/28.23...                        \u001b[34m1.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ ‡\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 46.94MB / 52.42MB Pulling                               \u001b[34m1.9s \u001b[0m\n",
      "   \u001b[33mâ ¹\u001b[0m 5c32499ab806 Extracting       25.66MB/28.23...                        \u001b[34m1.3s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ \u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 49.51MB / 52.42MB Pulling                               \u001b[34m2.0s \u001b[0m\n",
      "   \u001b[33mâ ¸\u001b[0m 5c32499ab806 Extracting       28.23MB/28.23...                        \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 7/8\n",
      " \u001b[33mâ ‹\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 49.51MB / 52.42MB Pulling                               \u001b[34m2.1s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 5c32499ab806 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ ™\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 49.51MB / 52.42MB Pulling                               \u001b[34m2.2s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 5c32499ab806 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.5s \u001b[0m\n",
      "   \u001b[33mâ ´\u001b[0m faa43fe37ea4 Extracting       262.1kB/24.19...                        \u001b[34m1.6s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ ¹\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 49.51MB / 52.42MB Pulling                               \u001b[34m2.3s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 5c32499ab806 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.5s \u001b[0m\n",
      "   \u001b[33mâ ¦\u001b[0m faa43fe37ea4 Extracting       8.389MB/24.19...                        \u001b[34m1.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 6/8\n",
      " \u001b[33mâ ¸\u001b[0m redis [\u001b[32mâ£¿â£¿â£¿â£¿â£¿â£¿â£¿\u001b[0m] 52.42MB / 52.42MB Pulling                               \u001b[34m2.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 5c32499ab806 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Pull complete   \u001b[32m\u001b[0m                                         \u001b[34m1.8s \u001b[0m\n",
      "   \u001b[33mâ ¼\u001b[0m fa3ef5872501 Extracting           96B/96B                             \u001b[34m1.8s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Download complete \u001b[32m\u001b[0m                                       \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 8/8\u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m redis Pulled             \u001b[32m\u001b[0m                                               \u001b[34m2.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 5c32499ab806 Pull complete \u001b[32m\u001b[0m                                           \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m ebb6b8512082 Pull complete \u001b[32m\u001b[0m                                           \u001b[34m1.4s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 7555a6149d73 Pull complete \u001b[32m\u001b[0m                                           \u001b[34m1.5s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m faa43fe37ea4 Pull complete \u001b[32m\u001b[0m                                           \u001b[34m1.8s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m fa3ef5872501 Pull complete \u001b[32m\u001b[0m                                           \u001b[34m1.8s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 4f4fb700ef54 Pull complete \u001b[32m\u001b[0m                                           \u001b[34m1.8s \u001b[0m\n",
      "   \u001b[32mâœ”\u001b[0m 54e1e08620c3 Pull complete \u001b[32m\u001b[0m                                           \u001b[34m1.8s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 3/5\n",
      " \u001b[32mâœ”\u001b[0m Network flood-prediction-network      \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-redis-data\"  \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-nim-cache\"   \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ‹\u001b[0m Container flood-prediction-nimllm     Creating                          \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ‹\u001b[0m Container flood-prediction-redis      Creating                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 5/6\u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Network flood-prediction-network      \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-redis-data\"  \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-nim-cache\"   \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-nimllm     \u001b[32mCreated\u001b[0m                           \u001b[34m0.1s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis      \u001b[32mCreated\u001b[0m                           \u001b[34m0.1s \u001b[0m\n",
      " \u001b[33mâ ‹\u001b[0m Container flood-prediction-web        Creating                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 4/6\n",
      " \u001b[32mâœ”\u001b[0m Network flood-prediction-network      \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-redis-data\"  \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-nim-cache\"   \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ™\u001b[0m Container flood-prediction-nimllm     Starting                          \u001b[34m0.2s \u001b[0m\n",
      " \u001b[33mâ ™\u001b[0m Container flood-prediction-redis      Starting                          \u001b[34m0.2s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web        \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 4/6\n",
      " \u001b[32mâœ”\u001b[0m Network flood-prediction-network      \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-redis-data\"  \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-nim-cache\"   \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ¹\u001b[0m Container flood-prediction-nimllm     Starting                          \u001b[34m0.3s \u001b[0m\n",
      " \u001b[33mâ ¹\u001b[0m Container flood-prediction-redis      Starting                          \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web        \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 4/6\n",
      " \u001b[32mâœ”\u001b[0m Network flood-prediction-network      \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-redis-data\"  \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-nim-cache\"   \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ¸\u001b[0m Container flood-prediction-nimllm     Starting                          \u001b[34m0.4s \u001b[0m\n",
      " \u001b[33mâ ¸\u001b[0m Container flood-prediction-redis      Starting                          \u001b[34m0.4s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web        \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32mâœ”\u001b[0m Network flood-prediction-network      \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-redis-data\"  \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-nim-cache\"   \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ¼\u001b[0m Container flood-prediction-nimllm     Starting                          \u001b[34m0.5s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis      \u001b[32mStarted\u001b[0m                           \u001b[34m0.5s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web        \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32mâœ”\u001b[0m Network flood-prediction-network      \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-redis-data\"  \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-nim-cache\"   \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ´\u001b[0m Container flood-prediction-nimllm     Starting                          \u001b[34m0.6s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis      \u001b[32mStarted\u001b[0m                           \u001b[34m0.5s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web        \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32mâœ”\u001b[0m Network flood-prediction-network      \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-redis-data\"  \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-nim-cache\"   \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-nimllm     \u001b[32mStarted\u001b[0m                           \u001b[34m0.7s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis      \u001b[32mStarted\u001b[0m                           \u001b[34m0.5s \u001b[0m\n",
      " \u001b[33mâ ™\u001b[0m Container flood-prediction-web        Starting                          \u001b[34m0.6s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 6/6\u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Network flood-prediction-network      \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-redis-data\"  \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Volume \"flood-prediction-nim-cache\"   \u001b[32mCreated\u001b[0m                           \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-nimllm     \u001b[32mStarted\u001b[0m                           \u001b[34m0.7s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis      \u001b[32mStarted\u001b[0m                           \u001b[34m0.5s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web        \u001b[32mStarted\u001b[0m                           \u001b[34m0.7s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker compose -f ../deployment/nvidia-launchable/docker-compose.yml -f ../deployment/nvidia-launchable/docker-compose.nimllm.yml --env-file ./flood_prediction.env up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If not using local NVIDIA NIM LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f ../deployment/nvidia-launchable/docker-compose.yml --env-file ./flood_prediction.env up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for all containers to be healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                         COMMAND                  CREATED          STATUS                    PORTS                    NAMES\n",
      "d11b62f9a505   h2oairelease/h2oai-floodprediction-app:v1.0.0                 \"./docker-entrypointâ€¦\"   14 minutes ago   Up 14 minutes (healthy)   0.0.0.0:8090->8000/tcp   flood-prediction-web\n",
      "e6727015634f   nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0   \"/opt/nvidia/nvidia_â€¦\"   14 minutes ago   Up 14 minutes (healthy)   0.0.0.0:8989->8000/tcp   flood-prediction-nimllm\n",
      "4a571d706e3a   redis:8.2.1                                                   \"docker-entrypoint.sâ€¦\"   14 minutes ago   Up 14 minutes (healthy)   6379/tcp                 flood-prediction-redis\n"
     ]
    }
   ],
   "source": [
    "# Check the status of the deployed containers. Wait till the STATUS is healthy\n",
    "\n",
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_key(value: str) -> str:\n",
    "    if not value:\n",
    "        return \"Not set\"\n",
    "    if len(value) <= 14:\n",
    "        return f\"{value[:4]}{'.' * len(value) - 4}\"\n",
    "    dots = max(1, len(value) - 14)\n",
    "    return f\"{value[:10]}{'.' * dots}{value[-4:]}\"\n",
    "\n",
    "def check_service(path, name, headers=None):\n",
    "    url = f\"{base_url}/{path.lstrip('/')}\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "    except requests.exceptions.RequestException as exc:\n",
    "        print(f\"âŒ {name} is not responding: {exc}\")\n",
    "        return False\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f\"âœ… {name} is running\")\n",
    "        return True\n",
    "    if response.status_code == 401:\n",
    "        print(f\"âš ï¸  {name} requires authentication (401)\")\n",
    "        return True\n",
    "\n",
    "    print(f\"âš ï¸  {name} responded with status {response.status_code}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NVIDIA_API_KEY: nvapi-hR2s........................................................o3QS\n",
      "âœ… H2OGPTE_URL: https://h2ogpte.cloud-dev.h2o.dev\n",
      "âœ… H2OGPTE_API_KEY: sk-ktl41qb.....................................tiqB\n",
      "âœ… h2oGPTe credentials found\n",
      "âœ… API_SERVER_URL detected: http://localhost:8090\n"
     ]
    }
   ],
   "source": [
    "NVIDIA_API_KEY = os.getenv(\"NVIDIA_API_KEY\")\n",
    "if NVIDIA_API_KEY:\n",
    "    print(f\"âœ… NVIDIA_API_KEY: {preview_key(NVIDIA_API_KEY)}\")\n",
    "else:\n",
    "    print(f\"â— NVIDIA_API_KEY NOT SET\")\n",
    "\n",
    "H2OGPTE_URL = os.getenv(\"H2OGPTE_URL\")\n",
    "if H2OGPTE_URL:\n",
    "    print(f\"âœ… H2OGPTE_URL: {H2OGPTE_URL}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ H2OGPTE_URL NOT SET\")\n",
    "\n",
    "H2OGPTE_API_KEY = os.getenv(\"H2OGPTE_API_KEY\")\n",
    "if H2OGPTE_API_KEY:\n",
    "    print(f\"âœ… H2OGPTE_API_KEY: {preview_key(H2OGPTE_API_KEY)}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ H2OGPTE_API_KEY NOT SET\")\n",
    "\n",
    "if H2OGPTE_URL and H2OGPTE_API_KEY:\n",
    "    print(\"âœ… h2oGPTe credentials found\")\n",
    "    H2OGPTE_AVAILABLE = True\n",
    "else:\n",
    "    print(\"âš ï¸  h2oGPTe credentials not set\")\n",
    "    print(\"This section will be skipped. To enable:\")\n",
    "    print(\"  export H2OGPTE_URL='<your-url>'\")\n",
    "    print(\"  export H2OGPTE_API_KEY='<your-key>'\")\n",
    "    H2OGPTE_AVAILABLE = False\n",
    "\n",
    "api_port = os.getenv(\"WEB_PORT\")\n",
    "if not api_port:\n",
    "    print(\"âš ï¸ WEB_PORT not set. API interactions will be skipped.\")\n",
    "\n",
    "api_server_base_url = os.getenv(\"API_SERVER_URL\", f\"http://localhost:{api_port}\")\n",
    "if api_server_base_url:\n",
    "    print(f\"âœ… API_SERVER_URL detected: {api_server_base_url}\")\n",
    "else:\n",
    "    print(\"âš ï¸ API_SERVER_URL not set. API interactions will be skipped.\")\n",
    "\n",
    "# Set this to False Here.\n",
    "# Later, we will verify that a Local NIM LLM is deployed and is reachable. If yes, we will set this to True\n",
    "LOCAL_NIM_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Service Endpoints:\n",
      "   - FastAPI Server: http://localhost:8090\n",
      "   - API Docs: http://localhost:8090/docs\n",
      "   - Agents API: http://localhost:8090/api/agents\n",
      "\n",
      "Use the next cell to verify service health.\n"
     ]
    }
   ],
   "source": [
    "if not api_server_base_url:\n",
    "    print(\"âš ï¸  API_SERVER_URL not set. Export API_SERVER_URL before continuing.\")\n",
    "else:\n",
    "    base_url = api_server_base_url.rstrip('/')\n",
    "    print(\"ğŸ“ Service Endpoints:\")\n",
    "    print(f\"   - FastAPI Server: {base_url}\")\n",
    "    print(f\"   - API Docs: {base_url}/docs\")\n",
    "    print(f\"   - Agents API: {base_url}/api/agents\")\n",
    "    print(\"\\nUse the next cell to verify service health.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify Services\n",
    "\n",
    "Let's check that all services are running correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking service health...\n",
      "\n",
      "âœ… FastAPI Dashboard is running\n",
      "âœ… Agents API is running\n",
      "âœ… Watersheds API is running\n",
      "\n",
      "==================================================\n",
      "âœ… All services are responding!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Checking service health...\\n\")\n",
    "\n",
    "if not api_server_base_url:\n",
    "    print(\"âš ï¸  API_SERVER_URL not set. Skipping health checks.\")\n",
    "    fastapi_ok = False\n",
    "else:\n",
    "    base_url = api_server_base_url.rstrip('/')\n",
    "\n",
    "    fastapi_ok = check_service('api/dashboard', 'FastAPI Dashboard')\n",
    "    agents_ok = check_service('api/agents', 'Agents API', headers={'Authorization': 'Bearer local-token'})\n",
    "    watersheds_ok = check_service('api/watersheds', 'Watersheds API')\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    if fastapi_ok and agents_ok and watersheds_ok:\n",
    "        print(\"âœ… All services are responding!\")\n",
    "    else:\n",
    "        print(\"âš ï¸  One or more endpoints did not respond as expected.\")\n",
    "        print(\"Review Helm deployment status or API logs if needed.\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that a Local NIM LLM is deployed and is reachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:8989/v1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the base url for NIM\n",
    "nimllm_port = os.getenv(\"NIMLLM_PORT\", \"8989\")\n",
    "nim_llm_base_url = f\"http://localhost:{nimllm_port}/v1\"\n",
    "nim_llm_base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Local NIM LLM is running\n"
     ]
    }
   ],
   "source": [
    "# Test if the NIM container is up. This will take 10 to 15 mins. Repeat this step until you see a positive message.\n",
    "if nim_llm_base_url:\n",
    "    response = requests.get(f\"{nim_llm_base_url}/models\", timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"âœ… Local NIM LLM is running\")\n",
    "        LOCAL_NIM_AVAILABLE = True\n",
    "    else:\n",
    "        print(\"âš ï¸ No locally deployed NIM LLM is found. Will be using NIMs via API.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No locally deployed NIM LLM is found. Will be using NIMs via API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: NVIDIA NIM Integration\n",
    "\n",
    "## ğŸš€ NVIDIA NIM - Optimized Inference Microservices\n",
    "\n",
    "NVIDIA NIM provides high-performance inference for state-of-the-art language models. Our flood prediction system uses several NVIDIA models:\n",
    "\n",
    "### Available Models\n",
    "\n",
    "1. **nvidia/llama-3.3-nemotron-super-49b-v1.5** (Default)\n",
    "   - Latest Nemotron model optimized for instruction following\n",
    "   - Excellent for agent workflows and tool calling\n",
    "   - 49B parameters with superior efficiency\n",
    "\n",
    "2. **meta/llama-3.1-70b-instruct**\n",
    "   - Strong general-purpose reasoning\n",
    "   - Great for complex analysis tasks\n",
    "\n",
    "### Let's test NVIDIA NIM integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local NIM LLM base URL: http://localhost:8989/v1\n"
     ]
    }
   ],
   "source": [
    "# Initialize NVIDIA NIM client\n",
    "# If LOCAL_NIM_AVAILABLE is True, it will be preferred. Please modify this cell to use external NVIDIA API.\n",
    "\n",
    "USE_NVIDIA_API = False\n",
    "\n",
    "if LOCAL_NIM_AVAILABLE:\n",
    "    print(f\"Using local NIM LLM base URL: {nim_llm_base_url}\")\n",
    "    model = \"nvidia/llama-3_3-nemotron-super-49b-v1_5\"\n",
    "    client = OpenAI(\n",
    "        base_url=nim_llm_base_url,\n",
    "        api_key=\"no-key\",\n",
    "    )\n",
    "else:\n",
    "\n",
    "    USE_NVIDIA_API = True\n",
    "\n",
    "    # Test with Nemotron Super 49B\n",
    "    model = \"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "\n",
    "    # From Nvidia API\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key=NVIDIA_API_KEY,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Testing NVIDIA NIM with nvidia/llama-3_3-nemotron-super-49b-v1_5\n",
      "\n",
      "ğŸ“ Response:\n",
      "<think>\n",
      "Okay, so I need to figure out the key factors that indicate an increased risk of flooding in a river basin. Let me start by recalling what I know about floods and river basins. A river basin is the area of land that drains water into a particular river or stream. Flooding happens when the river can't hold the amount of water it's receiving, right?\n",
      "\n",
      "First, precipitation comes to mind. Heavy rainfall over a short period or prolonged rain can cause the river to rise. But how much exactly? I think the intensity and duration of rainfall are important. Also, if the ground is already saturated from previous rain, it can't absorb more water, leading to runoff that flows into the river.\n",
      "\n",
      "Then there's the topography of the basin. Steep terrain might cause water to flow quickly into the river, increasing the risk of flash floods. Conversely, flat areas might allow water to spread out more, but could also lead to more widespread flooding if the water has nowhere to go.\n",
      "\n",
      "Soil type and vegetation are probably factors too. If the soil is clay-heavy, it doesn't absorb water well, leading to more runoff. Sandy soil might absorb more, reducing runoff. Vegetation can help absorb water and reduce erosion, so areas with less vegetation might have higher runoff.\n",
      "\n",
      "Land use changes. Urban areas with lots of impermeable surfaces like concrete and asphalt can't absorb water, so all the rain runs off into storm drains and rivers, increasing flood risk. Deforestation might also contribute because trees help absorb water and their roots hold soil in place. Without them, there's more runoff and potential for landslides which can block rivers and cause flooding.\n",
      "\n",
      "River channel characteristics. If the river is narrow or has a lot of bends, it might not be able to handle a large volume of water. Obstructions like bridges, dams, or debris can block the flow, causing water to back up. Also, if the river has been straightened or altered, it might flow faster, which could be good for moving water but might also lead to more erosion downstream.\n",
      "\n",
      "Climate change factors. I've heard that climate change can lead to more extreme weather events, including heavier rainfall in some regions. This could increase the frequency and severity of floods. Also, rising sea levels might affect tidal rivers, making it harder for water to drain into the ocean, thus increasing flood risk during high tides or storms.\n",
      "\n",
      "Human activities like building in floodplains. If people construct homes\n",
      "\n",
      "ğŸ“Š Tokens used: 540\n"
     ]
    }
   ],
   "source": [
    "print(f\"ğŸ¤– Testing NVIDIA NIM with {model}\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert flood prediction assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the key factors that indicate an increased risk of flooding in a river basin?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nğŸ“Š Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Response Example\n",
    "\n",
    "NVIDIA NIM supports streaming for real-time responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒŠ Streaming response about flood prediction...\n",
      "\n",
      "<think>\n",
      "Okay, so I need to explain how AI can improve flood forecasting accuracy. Let me start by recalling what I know about flood forecasting and AI. Flood forecasting involves predicting the occurrence and extent of floods, which is crucial for disaster management. Traditional methods might use hydrological models that simulate how water moves through a catchment area, considering factors like rainfall, soil type, topography, etc. But these models can have limitations, like needing a lot of data, being computationally intensive, or not capturing complex interactions well.\n",
      "\n",
      "Now, AI, especially machine learning and deep learning, is good at finding patterns in large datasets. So maybe AI can process more data more efficiently. For example, AI models can integrate real-time data from various sources like satellites, rain gauges, river gauges, and even social media. That could provide a more comprehensive picture. Also, AI might handle non-linear relationships better than traditional models. Like, the way rainfall translates to runoff might not be a straight-line relationship, and neural networks can model that complexity.\n",
      "\n",
      "Another point is that AI can adapt over time. Traditional models might need manual recalibration, but machine learning models can learn from new data continuously, improving their predictions as more information becomes available. That could make forecasts more accurate over time.\n",
      "\n",
      "I should also think about specific AI techniques. For instance, recurrent neural networks (RNNs) or LSTMs (Long Short-Term Memory networks) are good for time-series data, which is relevant\n",
      "\n",
      "âœ… Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸŒŠ Streaming response about flood prediction...\\n\")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise flood prediction expert.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain how AI can improve flood forecasting accuracy.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nâœ… Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "Let's compare responses from different NVIDIA models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Comparing model responses for:\n",
      "'Given streamflow of 2500 CFS and rising 200 CFS/hour, should we issue a flood alert?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– Model: llama-3_3-nemotron-super-49b-v1_5\n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking if they should issue a flood alert given a streamflow of 2500 CFS that's rising at 200 CFS per hour. Hmm, first, I need to recall what factors are involved in determining a flood alert. \n",
      "\n",
      "I know that flood alerts are typically based on specific thresholds set by local authorities or agencies like the National Weather Service in the US. These thresholds can vary depending on the location, the size of the river, and the area's flood history. But since the user hasn't provided specific location details, I might have to make some general assumptions.\n",
      "\n",
      "Streamflow of 2500 CFSâ€”CFS stands for cubic feet per second, which measures the volume of water flowing per second. The rate of rise is 200 CFS/hour, which is pretty significant. But whether that's enough to issue an alert depends on the context. For example, a small stream might flood at much lower levels compared\n",
      "\n",
      "ğŸ“ˆ Tokens: 250\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if USE_NVIDIA_API:\n",
    "    models_to_test = [\n",
    "        \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "        \"meta/llama-3.1-70b-instruct\"\n",
    "    ]\n",
    "else:\n",
    "    models_to_test = [model]\n",
    "\n",
    "question = \"Given streamflow of 2500 CFS and rising 200 CFS/hour, should we issue a flood alert?\"\n",
    "\n",
    "print(f\"ğŸ“Š Comparing model responses for:\\n'{question}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\nğŸ¤– Model: {model_name.split('/')[-1]}\\n\")\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a flood emergency expert. Be concise.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        print(response.choices[0].message.content)\n",
    "        print(f\"\\nğŸ“ˆ Tokens: {response.usage.total_tokens}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-Judge Evaluation\n",
    "\n",
    "The system includes an automatic evaluation feature using **cross-provider LLM-as-Judge**:\n",
    "- When NVIDIA models generate responses, h2oGPTe judges them\n",
    "- When h2oGPTe generates responses, NVIDIA models judge them\n",
    "- This provides unbiased evaluation of response quality\n",
    "\n",
    "Let's evaluate the NVIDIA model responses using our evaluation API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Evaluating NVIDIA model response using LLM-as-Judge...\n",
      "\n",
      "âœ… Evaluation Complete!\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š Evaluation Metrics:\n",
      "\n",
      "   ğŸ¯ Overall Score:    6.0/10\n",
      "   ğŸ¤ Helpfulness:      6.0/10\n",
      "   âœ… Accuracy:         4.0/10\n",
      "   ğŸ¯ Relevance:        8.0/10\n",
      "   ğŸ“ Coherence:        8.0/10\n",
      "   ğŸ›¡ï¸  Safety:           7.0/10\n",
      "   ğŸ’ª Confidence:       85.0%\n",
      "\n",
      "ğŸ’­ Judge's Reasoning:\n",
      "   The response is well-structured and directly addresses the question with appropriate urgency. However, it lacks critical context - flood thresholds vary dramatically by location, watershed characteristics, and baseline conditions. 2500 CFS could be normal flow for a large river or extreme flooding for a small stream. The response assumes high risk without considering flood stage levels, historical data, or local thresholds. While the precautionary approach is generally safe, making definitive recommendations without proper context could lead to unnecessary evacuations or alert fatigue. The response would benefit from requesting additional watershed-specific information before making alert recommendations.\n",
      "\n",
      "â±ï¸  Evaluation Duration: 7440ms\n",
      "ğŸ†” Evaluation ID: 1e5080d0-8356-4c92-ab30-45a37ba62ed2\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's evaluate one of the model responses from above\n",
    "question = \"Given streamflow of 2500 CFS and rising 200 CFS/hour, should we issue a flood alert?\"\n",
    "\n",
    "# Response from meta/llama-3.1-70b-instruct (from cell 12)\n",
    "response_text = \"\"\"Streamflow is already high (2500 CFS) and rising rapidly (200 CFS/hour). I recommend issuing a flood alert immediately. The rapid increase in streamflow indicates a high risk of flooding, and prompt action is necessary to protect people and property.\"\"\"\n",
    "\n",
    "print(\"ğŸ” Evaluating NVIDIA model response using LLM-as-Judge...\\n\")\n",
    "\n",
    "# Call the evaluation API\n",
    "eval_payload = {\n",
    "    \"question\": question,\n",
    "    \"response\": response_text,\n",
    "    \"model\": \"meta/llama-3.1-70b-instruct\",\n",
    "    \"agent_used\": False,\n",
    "    \"response_provider\": \"nvidia\"  # This will trigger h2oGPTe as the judge\n",
    "}\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/evaluation/evaluate\",\n",
    "    json=eval_payload,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    eval_result = response.json()\n",
    "    \n",
    "    print(\"âœ… Evaluation Complete!\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ“Š Evaluation Metrics:\\n\")\n",
    "    \n",
    "    metrics = eval_result.get('metrics', {})\n",
    "    print(f\"   ğŸ¯ Overall Score:    {metrics.get('overall', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ¤ Helpfulness:      {metrics.get('helpfulness', 0):.1f}/10\")\n",
    "    print(f\"   âœ… Accuracy:         {metrics.get('accuracy', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ¯ Relevance:        {metrics.get('relevance', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ“ Coherence:        {metrics.get('coherence', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ›¡ï¸  Safety:           {metrics.get('safety', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ’ª Confidence:       {metrics.get('confidence', 0):.1%}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’­ Judge's Reasoning:\")\n",
    "    print(f\"   {eval_result.get('reasoning', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  Evaluation Duration: {eval_result.get('duration_ms', 0)}ms\")\n",
    "    print(f\"ğŸ†” Evaluation ID: {eval_result.get('evaluation_id', 'N/A')}\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: h2oGPTe Agent (A2A) Integration\n",
    "\n",
    "## ğŸ§  h2oGPTe - Enterprise AI with Agent Mode\n",
    "\n",
    "h2oGPTe provides advanced AutoML capabilities through its agent mode, enabling:\n",
    "\n",
    "- **Driverless AI Integration**: Automated machine learning with minimal code\n",
    "- **Agent-to-Agent (A2A)**: AI agents that can invoke other AI agents\n",
    "- **Feature Engineering**: Automatic feature creation for time-series data\n",
    "- **Model Interpretability**: Explainable AI for emergency response decisions\n",
    "\n",
    "### Setting up h2oGPTe Client\n",
    "\n",
    "**Note**: This section requires h2oGPTe credentials. If you don't have access, you can skip to the next section.\n",
    "\n",
    "Get your credentials at: [H2O.ai Enterprise](https://h2o.ai/platform/enterprise-h2ogpte/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h2oGPTe for Flood Prediction ML\n",
    "\n",
    "Let's use h2oGPTe's agent mode to get guidance on training a flood prediction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Consulting h2oGPTe agent for AutoML guidance...\n",
      "\n",
      "ğŸ“¡ Provider: h2ogpte\n",
      "ğŸ¤– Model: claude-sonnet-4-20250514\n",
      "\n",
      "ğŸ“ Response:\n",
      "\n",
      "#### Agentic Analysis  \n",
      "#### Starting Agent  \n",
      "I'll help you build a comprehensive ML model for flood risk prediction. Let me start by analyzing your data structure and then provide recommendations for feature engineering and model building approaches.\n",
      "\n",
      "```python\n",
      "# filename: flood_prediction_analysis.py\n",
      "# execution: true\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
      "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "# Set up plotting style\n",
      "plt.style.use('default')\n",
      "sns.set_palette(\"husl\")\n",
      "\n",
      "print(\"=== FLOOD PREDICTION MODEL DEVELOPMENT GUIDE ===\\n\")\n",
      "\n",
      "# First, let's create a synthetic dataset that matches your features to demonstrate the approach\n",
      "print(\"1. CREATING SYNTHETIC FLOOD PREDICTION DATASET\")\n",
      "print(\"=\" * 50)\n",
      "\n",
      "np.random.seed(42)\n",
      "n_samples = 5000\n",
      "\n",
      "# Generate realistic flood prediction data\n",
      "data = {\n",
      "    'streamflow_cfs': np.random.lognormal(mean=4, sigma=1.5, size=n_samples),\n",
      "    'rainfall_24h': np.random.exponential(scale=0.8, size=n_samples),\n",
      "    'river_stage_ft': np.random.normal(loc=15, scale=5, size=n_samples),\n",
      "    'soil_moisture': np.random.beta(a=2, b=2, size=n_samples),\n",
      "    'elevation_ft': np.random.normal(loc=500, scale=200, size=n_samples)\n",
      "}\n",
      "\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Ensure realistic ranges\n",
      "df['streamflow_cfs'] = np.clip(df['streamflow_cfs'], 10, 50000)\n",
      "df['rainfall_24h'] = np.clip(df['rainfall_24h'], 0, 8)\n",
      "df['river_stage_ft'] = np.clip(df['river_stage_ft'], 2, 35)\n",
      "df['soil_moisture'] = np.clip(df['soil_moisture'], 0, 1)\n",
      "df['elevation_ft'] = np.clip(df['elevation_ft'], 50, 1500)\n",
      "\n",
      "print(f\"Dataset created with {len(df)} samples\")\n",
      "print(\"\\nBasic statistics of raw features:\")\n",
      "print(df.describe())\n",
      "\n",
      "# Create flood risk target based on realistic thresholds\n",
      "def create_flood_risk(row):\n",
      "    risk_score = 0\n",
      "    \n",
      "    # High streamflow increases risk\n",
      "    if row['streamflow_cfs'] > 5000:\n",
      "        risk_score += 3\n",
      "    elif row['streamflow_cfs'] > 2000:\n",
      "        risk_score += 2\n",
      "    elif row['streamflow_cfs'] > 1000:\n",
      "        risk_score += 1\n",
      "    \n",
      "    # High rainfall increases risk\n",
      "    if row['rainfall_24h'] > 3:\n",
      "        risk_score += 3\n",
      "    elif row['rainfall_24h'] > 1.5:\n",
      "        risk_score += 2\n",
      "    elif row['rainfall_24h'] > 0.5:\n",
      "        risk_score += 1\n",
      "    \n",
      "    # High river stage increases risk\n",
      "    if row['river_stage_ft'] > 25:\n",
      "        risk_score += 3\n",
      "    elif row['river_stage_ft'] > 20:\n",
      "        risk_score += 2\n",
      "    elif row['river_stage_ft'] > 15:\n",
      "        risk_score += 1\n",
      "    \n",
      "    # High soil moisture increases risk\n",
      "    if row['soil_moisture'] > 0.8:\n",
      "        risk_score += 2\n",
      "    elif row['soil_moisture'] > 0.6:\n",
      "        risk_score += 1\n",
      "    \n",
      "    # Lower elevation increases risk\n",
      "    if row['elevation_ft'] < 200:\n",
      "        risk_score += 2\n",
      "    elif row['elevation_ft'] < 400:\n",
      "        risk_score += 1\n",
      "    \n",
      "    # Convert to binary classification (high risk if score >= 5)\n",
      "    return 1 if risk_score >= 5 else 0\n",
      "\n",
      "df['flood_risk_24h'] = df.apply(create_flood_risk, axis=1)\n",
      "\n",
      "print(f\"\\nFlood risk distribution:\")\n",
      "print(df['flood_risk_24h'].value_counts())\n",
      "print(f\"Flood risk rate: {df['flood_risk_24h'].mean():.3f}\")\n",
      "\n",
      "# Save the synthetic dataset\n",
      "df.to_csv('flood_prediction_data.csv', index=False)\n",
      "print(\"\\nSynthetic dataset saved as 'flood_prediction_data.csv'\")\n",
      "```\n",
      "\n",
      "**LLM Call Info:**\n",
      "\n",
      " Turn Time: 16.01s out of 120s.\n",
      "\n",
      " Turns: 1 out of 20.\n",
      " Time: 16.0 out of 3600.\n",
      " Cost: Turn: $0.1086, Total: $0.1086, Remaining: $474.48.\n",
      "\n",
      "\n",
      "ENDOFTURN\n",
      "\n",
      "\n",
      "**Executing python code blocks**\n",
      "\n",
      "=== FLOOD PREDICTION MODEL DEVELOPMENT GUIDE ===\n",
      "\n",
      "1. CREATING SYNTHETIC FLOOD PREDICTION DATASET\n",
      "==================================================\n",
      "Dataset created with 5000 samples\n",
      "\n",
      "Basic statistics of raw features:\n",
      "       streamflow_cfs  rainfall_24h  ...  soil_moisture  elevation_ft\n",
      "count     5000.000000   5000.000000  ...    5000.000000   5000.000000\n",
      "mean       170.909516      0.775323  ...       0.505449    497.939389\n",
      "std        539.062885      0.759004  ...       0.221674    200.694399\n",
      "min         10.000000      0.000042  ...       0.001511     50.000000\n",
      "25%         20.351253      0.231739  ...       0.334843    355.913323\n",
      "50%         55.712171      0.541609  ...       0.505851    496.262014\n",
      "75%        148.267183      1.072618  ...       0.677444    634.768324\n",
      "max      19719.367619      6.420943  ...       0.989759   1245.566669\n",
      "\n",
      "[8 rows x 5 columns]\n",
      "\n",
      "Flood risk distribution:\n",
      "0    4625\n",
      "1     375\n",
      "Name: flood_risk_24h, dtype: int64\n",
      "Flood risk rate: 0.075\n",
      "\n",
      "Synthetic dataset saved as 'flood_prediction_data.csv'\n",
      "\n",
      "\n",
      "** [Monday, October 20, 2025 - 06:25:38.2 PM PDT] Completed execution of code block using python in 2.42 seconds after 1 out of 20 turns and time 19.2s out of 3600s and minimum runtime 0s.**\n",
      "\n",
      "ENDOFTURN\n",
      "Now let me continue with the comprehensive feature engineering and model building approach:\n",
      "\n",
      "```python\n",
      "# filename: flood_prediction_feature_engineering.py\n",
      "# execution: true\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
      "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('flood_prediction_data.csv')\n",
      "\n",
      "print(\"2. COMPREHENSIVE FEATURE ENGINEERING\")\n",
      "print(\"=\" * 50)\n",
      "\n",
      "# Create a copy for feature engineering\n",
      "df_features = df.copy()\n",
      "\n",
      "# ===== DOMAIN-SPECIFIC FEATURE ENGINEERING =====\n",
      "\n",
      "print(\"\\n2.1 Creating Domain-Specific Features:\")\n",
      "print(\"-\" * 40)\n",
      "\n",
      "# 1. Flow-based features\n",
      "df_features['streamflow_log'] = np.log1p(df_features['streamflow_cfs'])\n",
      "df_features['streamflow_sqrt'] = np.sqrt(df_features['streamflow_cfs'])\n",
      "\n",
      "# 2. Rainfall intensity categories\n",
      "df_features['rainfall_intensity'] = pd.cut(df_features['rainfall_24h'], \n",
      "                                         bins=[0, 0.1, 0.5, 1.0, 2.0, np.inf],\n",
      "                                         labels=['none', 'light', 'moderate', 'heavy', 'extreme'])\n",
      "df_features['rainfall_intensity_encoded'] = df_features['rainfall_intensity'].cat.codes\n",
      "\n",
      "# 3. River stage relative to elevation (flood potential)\n",
      "df_features['stage_elevation_ratio'] = df_features['river_stage_ft'] / df_features['elevation_ft']\n",
      "df_features['elevation_above_stage'] = df_features['elevation_ft'] - df_features['river_stage_ft']\n",
      "\n",
      "# 4. Soil saturation levels\n",
      "df_features['soil_saturation_level'] = pd.cut(df_features['soil_moisture'],\n",
      "                                             bins=[0, 0.3, 0.6, 0.8, 1.0],\n",
      "                                             labels=['dry', 'moderate', 'wet', 'saturated'])\n",
      "df_features['soil_saturation_encoded'] = df_features['soil_saturation_level'].cat.codes\n",
      "\n",
      "# 5. Combined risk indicators\n",
      "df_features['flow_rainfall_interaction'] = df_features['streamflow_cfs'] * df_features['rainfall_24h']\n",
      "df_features['stage_soil_interaction'] = df_features['river_stage_ft'] * df_features['soil_moisture']\n",
      "\n",
      "# 6. Normalized features (percentile-based)\n",
      "for col in ['streamflow_cfs', 'rainfall_24h', 'river_stage_ft', 'soil_moisture']:\n",
      "    df_features[f'{col}_percentile'] = df_features[col].rank(pct=True)\n",
      "\n",
      "# 7. Threshold-based binary features\n",
      "df_features['high_flow'] = (df_features['streamflow_cfs'] > df_features['streamflow_cfs'].quantile(0.8)).astype(int)\n",
      "df_features['heavy_rain'] = (df_features['rainfall_24h'] > df_features['rainfall_24h'].quantile(0.8)).astype(int)\n",
      "df_features['high_stage'] = (df_features['river_stage_ft'] > df_features['river_stage_ft'].quantile(0.8)).astype(int)\n",
      "df_features['saturated_soil'] = (df_features['soil_moisture'] > 0.7).astype(int)\n",
      "df_features['low_elevation'] = (df_features['elevation_ft'] < df_features['elevation_ft'].quantile(0.3)).astype(int)\n",
      "\n",
      "# 8. Composite risk score\n",
      "df_features['composite_risk_score'] = (\n",
      "    df_features['streamflow_cfs_percentile'] * 0.3 +\n",
      "    df_features['rainfall_24h_percentile'] * 0.25 +\n",
      "    df_features['river_stage_ft_percentile'] * 0.25 +\n",
      "    df_features['soil_moisture_percentile'] * 0.15 +\n",
      "    (1 - df_features['elevation_ft_percentile']) * 0.05  # Lower elevation = higher risk\n",
      ")\n",
      "\n",
      "print(\"Created features:\")\n",
      "new_features = [col for col in df_features.columns if col not in df.columns]\n",
      "for feature in new_features:\n",
      "    print(f\"  - {feature}\")\n",
      "\n",
      "print(f\"\\nTotal features: {len(df_features.columns)} (original: {len(df.columns)})\")\n",
      "\n",
      "# ===== FEATURE ANALYSIS =====\n",
      "print(\"\\n2.2 Feature Correlation Analysis:\")\n",
      "print(\"-\" * 40)\n",
      "\n",
      "# Select numeric features for correlation analysis\n",
      "numeric_features = df_features.select_dtypes(include=[np.number]).columns\n",
      "correlation_matrix = df_features[numeric_features].corr()\n",
      "\n",
      "# Plot correlation heatmap\n",
      "plt.figure(figsize=(16, 12))\n",
      "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
      "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
      "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
      "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
      "plt.tight_layout()\n",
      "plt.savefig('feature_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
      "plt.close()\n",
      "\n",
      "# Find highly correlated features with target\n",
      "target_correlations = correlation_matrix['flood_risk_24h'].abs().sort_values(ascending=False)\n",
      "print(\"Top 10 features correlated with flood risk:\")\n",
      "for i, (feature, corr) in enumerate(target_correlations.head(11).items()):\n",
      "    if feature != 'flood_risk_24h':\n",
      "        print(f\"  {i}. {feature}: {corr:.4f}\")\n",
      "\n",
      "print(\"\\nCorrelation matrix saved as 'feature_correlation_matrix.png'\")\n",
      "```\n",
      "\n",
      "**LLM Call Info:**\n",
      "\n",
      " Turn Time: 19.89s out of 120s.\n",
      "\n",
      " Turns: 2 out of 20.\n",
      " Time: 39.0 out of 3600.\n",
      " Cost: Turn: $0.0393, Total: $0.1479, Remaining: $474.44.\n",
      "\n",
      "\n",
      "ENDOFTURN\n",
      "\n",
      "\n",
      "**Executing python code blocks**\n",
      "\n",
      "2. COMPREHENSIVE FEATURE ENGINEERING\n",
      "==================================================\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n",
      "2.1 Creating Domain-Specific Features:\n",
      "    return self._engine.get_loc(casted_key)\n",
      "----------------------------------------\n",
      "  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'elevation_ft_percentile'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.user_files/0b562a412c45/966a1fac8e75/.work/flood_prediction_feature_engineering.py\", line 81, in <module>\n",
      "    (1 - df_features['elevation_ft_percentile']) * 0.05  # Lower elevation = higher risk\n",
      "  File \"/usr/lib/python3.10/site-packages/pandas/core/frame.py\", line 3807, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/usr/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'elevation_ft_percentile'\n",
      "\n",
      "\n",
      "** [Monday, October 20, 2025 - 06:26:00.5 PM PDT] Completed execution of code block using python in 2.12 seconds after 2 out of 20 turns and time 41.5s out of 3600s and minimum runtime 0s.**\n",
      "\n",
      "ENDOFTURN\n",
      "Let me fix the error by properly creating the percentile features first:\n",
      "\n",
      "```python\n",
      "# filename: flood_prediction_feature_engineering_fixed.py\n",
      "# execution: true\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
      "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('flood_prediction_data.csv')\n",
      "\n",
      "print(\"2. COMPREHENSIVE FEATURE ENGINEERING\")\n",
      "print(\"=\" * 50)\n",
      "\n",
      "# First, let's examine the data structure\n",
      "print(\"Original dataset structure:\")\n",
      "print(df.head())\n",
      "print(f\"\\nDataset shape: {df.shape}\")\n",
      "print(f\"Columns: {list(df.columns)}\")\n",
      "\n",
      "# Create a copy for feature engineering\n",
      "df_features = df.copy()\n",
      "\n",
      "print(\"\\n2.1 Creating Domain-Specific Features:\")\n",
      "print(\"-\" * 40)\n",
      "\n",
      "# 1. Flow-based features\n",
      "df_features['streamflow_log'] = np.log1p(df_features['streamflow_cfs'])\n",
      "df_features['streamflow_sqrt'] = np.sqrt(df_features['streamflow_cfs'])\n",
      "\n",
      "# 2. Rainfall intensity categories\n",
      "df_features['rainfall_intensity'] = pd.cut(df_features['rainfall_24h'], \n",
      "                                         bins=[0, 0.1, 0.5, 1.0, 2.0, np.inf],\n",
      "                                         labels=['none', 'light', 'moderate', 'heavy', 'extreme'])\n",
      "df_features['rainfall_intensity_encoded'] = df_features['rainfall_intensity'].cat.codes\n",
      "\n",
      "# 3. River stage relative to elevation (flood potential)\n",
      "df_features['stage_elevation_ratio'] = df_features['river_stage_ft'] / df_features['elevation_ft']\n",
      "df_features['elevation_above_stage'] = df_features['elevation_ft'] - df_features['river_stage_ft']\n",
      "\n",
      "# 4. Soil saturation levels\n",
      "df_features['soil_saturation_level'] = pd.cut(df_features['soil_moisture'],\n",
      "                                             bins=[0, 0.3, 0.6, 0.8, 1.0],\n",
      "                                             labels=['dry', 'moderate', 'wet', 'saturated'])\n",
      "df_features['soil_saturation_encoded'] = df_features['soil_saturation_level'].cat.codes\n",
      "\n",
      "# 5. Combined risk indicators\n",
      "df_features['flow_rainfall_interaction'] = df_features['streamflow_cfs'] * df_features['rainfall_24h']\n",
      "df_features['stage_soil_interaction'] = df_features['river_stage_ft'] * df_features['soil_moisture']\n",
      "\n",
      "# 6. Normalized features (percentile-based) - Create these BEFORE using them\n",
      "print(\"Creating percentile features...\")\n",
      "for col in ['streamflow_cfs', 'rainfall_24h', 'river_stage_ft', 'soil_moisture', 'elevation_ft']:\n",
      "    df_features[f'{col}_percentile'] = df_features[col].rank(pct=True)\n",
      "    print(f\"  - Created {col}_percentile\")\n",
      "\n",
      "# 7. Threshold-based binary features\n",
      "df_features['high_flow'] = (df_features['streamflow_cfs'] > df_features['streamflow_cfs'].quantile(0.8)).astype(int)\n",
      "df_features['heavy_rain'] = (df_features['rainfall_24h'] > df_features['rainfall_24h'].quantile(0.8)).astype(int)\n",
      "df_features['high_stage'] = (df_features['river_stage_ft'] > df_features['river_stage_ft'].quantile(0.8)).astype(int)\n",
      "df_features['saturated_soil'] = (df_features['soil_moisture'] > 0.7).astype(int)\n",
      "df_features['low_elevation'] = (df_features['elevation_ft'] < df_features['elevation_ft'].quantile(0.3)).astype(int)\n",
      "\n",
      "# 8. Composite risk score (now that percentile features exist)\n",
      "df_features['composite_risk_score'] = (\n",
      "    df_features['streamflow_cfs_percentile'] * 0.3 +\n",
      "    df_features['rainfall_24h_percentile'] * 0.25 +\n",
      "    df_features['river_stage_ft_percentile'] * 0.25 +\n",
      "    df_features['soil_moisture_percentile'] * 0.15 +\n",
      "    (1 - df_features['elevation_ft_percentile']) * 0.05  # Lower elevation = higher risk\n",
      ")\n",
      "\n",
      "# 9. Additional engineered features\n",
      "df_features['flow_per_rainfall'] = df_features['streamflow_cfs'] / (df_features['rainfall_24h'] + 0.01)  # Avoid division by zero\n",
      "df_features['stage_per_elevation'] = df_features['river_stage_ft'] / df_features['elevation_ft']\n",
      "df_features['risk_multiplier'] = df_features['high_flow'] * df_features['heavy_rain'] * df_features['saturated_soil']\n",
      "\n",
      "print(\"\\nAll engineered features created successfully!\")\n",
      "\n",
      "# List all new features\n",
      "new_features = [col for col in df_features.columns if col not in df.columns]\n",
      "print(f\"\\nCre\n",
      "\n",
      "âœ… Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "if H2OGPTE_AVAILABLE:\n",
    "    headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "    # Using FastAPI streaming endpoint for h2oGPTe\n",
    "    url = f\"{api_server_base_url}/api/ai/chat/enhanced/stream\"\n",
    "    \n",
    "    payload = {\n",
    "        \"message\": \"\"\"I have flood prediction data with these features:\n",
    "        - streamflow_cfs: Current river flow rate\n",
    "        - rainfall_24h: Rainfall in last 24 hours\n",
    "        - river_stage_ft: Water level\n",
    "        - soil_moisture: Ground saturation\n",
    "        - elevation_ft: Location elevation\n",
    "        \n",
    "        How should I approach building an ML model to predict flood risk in the next 24 hours?\n",
    "        What feature engineering would you recommend?\"\"\",\n",
    "        \"provider\": \"h2ogpte\",\n",
    "        \"use_agent\": True,\n",
    "        \"max_tokens\": 8192*10,\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ§  Consulting h2oGPTe agent for AutoML guidance...\\n\")\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        last_content = \"\"\n",
    "        # Stream the response\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode('utf-8')\n",
    "                if line_str.startswith('data: '):\n",
    "                    data_str = line_str[6:]  # Remove 'data: ' prefix\n",
    "                    try:\n",
    "                        data = json.loads(data_str)\n",
    "                        \n",
    "                        # First message contains provider info\n",
    "                        if 'provider' in data:\n",
    "                            print(f\"ğŸ“¡ Provider: {data.get('provider')}\")\n",
    "                            print(f\"ğŸ¤– Model: {data.get('model')}\\n\")\n",
    "                            print(\"ğŸ“ Response:\\n\")\n",
    "                        \n",
    "                        # h2oGPTe sends incremental chunks with full content\n",
    "                        elif 'chunk' in data and not data.get('done', False):\n",
    "                            new_content = data['chunk']\n",
    "                            # Only print the new portion\n",
    "                            if new_content.startswith(last_content):\n",
    "                                new_part = new_content[len(last_content):]\n",
    "                                print(new_part, end='', flush=True)\n",
    "                                last_content = new_content\n",
    "                        \n",
    "                        # Check for completion\n",
    "                        elif data.get('done', False):\n",
    "                            break\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "        \n",
    "        print(\"\\n\\nâœ… Streaming complete!\")\n",
    "    else:\n",
    "        print(f\"âŒ Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping h2oGPTe demo (credentials not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Multi-Agent System with FastMCP\n",
    "\n",
    "## ğŸ¤ FastMCP - Model Context Protocol Server\n",
    "\n",
    "Our flood prediction system uses FastMCP to expose 20+ specialized tools across 5 intelligent agents:\n",
    "\n",
    "### The 5 Agents\n",
    "\n",
    "1. **Data Collector Agent** ğŸ“Š\n",
    "   - Collects USGS water data\n",
    "   - Retrieves NOAA flood forecasts\n",
    "   - Gathers weather information\n",
    "   - Monitors data quality\n",
    "\n",
    "2. **Risk Analyzer Agent** âš ï¸\n",
    "   - Calculates flood risk scores\n",
    "   - Analyzes trends and patterns\n",
    "   - Identifies high-risk areas\n",
    "\n",
    "3. **Emergency Responder Agent** ğŸš¨\n",
    "   - Assesses emergency readiness\n",
    "   - Activates alerts\n",
    "   - Coordinates evacuations\n",
    "\n",
    "4. **AI Predictor Agent** ğŸ”®\n",
    "   - Generates flood forecasts\n",
    "   - Predicts critical conditions\n",
    "   - Analyzes prediction accuracy\n",
    "\n",
    "5. **H2OGPTE ML Agent** ğŸ§ \n",
    "   - Trains ML models\n",
    "   - Optimizes features\n",
    "   - Analyzes model performance\n",
    "\n",
    "### Let's explore the available tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Available Agents and Their Status\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸŸ¢ Data Collector\n",
      "   Description: Continuously pulls real-time flood data from USGS, NOAA, and weather APIs\n",
      "   Status: Running\n",
      "   Last Check: 2025-10-21T01:21:37.480173+00:00\n",
      "   Check Interval: 300 seconds\n",
      "   Insights: 4\n",
      "\n",
      "ğŸŸ¢ Risk Analyzer\n",
      "   Description: AI-powered analysis of flood risk conditions and trend detection\n",
      "   Status: Running\n",
      "   Last Check: 2025-10-21T01:16:25.547930+00:00\n",
      "   Check Interval: 600 seconds\n",
      "   Insights: 4\n",
      "   Active Alerts: 1\n",
      "\n",
      "ğŸŸ¢ Emergency Responder\n",
      "   Description: Coordinates emergency response activities and manages critical alerts\n",
      "   Status: Running\n",
      "   Last Check: 2025-10-21T01:24:25.548667+00:00\n",
      "   Check Interval: 180 seconds\n",
      "   Insights: 5\n",
      "   Active Alerts: 2\n",
      "\n",
      "ğŸŸ¢ AI Predictor\n",
      "   Description: Advanced AI forecasting and predictive analysis for flood conditions\n",
      "   Status: Running\n",
      "   Last Check: 2025-10-21T01:21:25.549140+00:00\n",
      "   Check Interval: 900 seconds\n",
      "   Insights: 4\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get list of all agents and their capabilities\n",
    "response = requests.get(f\"{api_server_base_url}/api/agents\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    print(\"ğŸ¤– Available Agents and Their Status\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # The API returns a nested structure with agents dictionary\n",
    "    agents_dict = data.get('agents', {})\n",
    "    \n",
    "    for agent_key, agent_data in agents_dict.items():\n",
    "        status = \"ğŸŸ¢\" if agent_data.get('is_running') else \"ğŸ”´\"\n",
    "        print(f\"\\n{status} {agent_data.get('name', agent_key)}\")\n",
    "        print(f\"   Description: {agent_data.get('description', 'N/A')}\")\n",
    "        print(f\"   Status: {'Running' if agent_data.get('is_running') else 'Stopped'}\")\n",
    "        if agent_data.get('last_check'):\n",
    "            print(f\"   Last Check: {agent_data.get('last_check')}\")\n",
    "        if agent_data.get('check_interval'):\n",
    "            print(f\"   Check Interval: {agent_data.get('check_interval')} seconds\")\n",
    "        if agent_data.get('insights_count'):\n",
    "            print(f\"   Insights: {agent_data.get('insights_count')}\")\n",
    "        if agent_data.get('active_alerts_count'):\n",
    "            print(f\"   Active Alerts: {agent_data.get('active_alerts_count')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(f\"âŒ Error fetching agents: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Agent Insights\n",
    "\n",
    "Agents continuously monitor flood conditions and generate insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Refreshing USGS data first...\n",
      "\n",
      "âš ï¸  Data refresh returned status 500\n",
      "   Proceeding with existing data...\n",
      "\n",
      "ğŸ’¡ Agent Insights\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– Data Collector\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸŸ¡ ğŸ”„ Data Freshness: 24% current\n",
      "\n",
      "ğŸ”µ ğŸŒ API Connectivity: 5/5 active\n",
      "\n",
      "ğŸ”µ ğŸ“Š Data Quality: 6.8/10\n",
      "   Change: +0.0\n",
      "\n",
      "ğŸ”µ âš¡ Update Frequency: 12 updates/hour\n",
      "\n",
      "ğŸ¤– Risk Analyzer\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”µ ğŸ¯ Overall Risk Level: MODERATE (5.0/10)\n",
      "   Change: +0.0\n",
      "\n",
      "ğŸ”µ ğŸš¨ Critical Watersheds: 1 areas\n",
      "\n",
      "ğŸ”µ ğŸ“ˆ Risk Trend Analysis: Stable\n",
      "   Change: 0.0% per hour\n",
      "\n",
      "ğŸ”µ ğŸ§  AI Confidence: 0%\n",
      "   Change: Low\n",
      "\n",
      "ğŸ¤– Emergency Responder\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”µ ğŸš¨ Active Incidents: 0 ongoing\n",
      "\n",
      "ğŸ”µ ğŸš Response Readiness: GOOD (85%)\n",
      "   Change: 8 teams ready\n",
      "\n",
      "ğŸ”µ ğŸƒ Evacuation Status: No active evacuations\n",
      "   Change: 0 zones active\n",
      "\n",
      "ğŸ”µ ğŸ“¡ Communication Systems: 100% operational\n",
      "   Change: 6 channels active\n",
      "\n",
      "ğŸ”µ ğŸ“¢ Alert Distribution: 0 alerts sent\n",
      "   Change: 0 today\n",
      "\n",
      "ğŸ¤– Predictor\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”µ ğŸ¯ Model Accuracy: 85.0%\n",
      "   Change: +0.0% vs yesterday\n",
      "\n",
      "ğŸ”µ ğŸ§  Prediction Confidence: 72%\n",
      "   Change: High\n",
      "\n",
      "================================================================================\n",
      "Generated at: 2025-10-21T01:26:16.397186+00:00\n"
     ]
    }
   ],
   "source": [
    "# Refresh USGS data before getting insights\n",
    "print(\"ğŸ”„ Refreshing USGS data first...\\n\")\n",
    "\n",
    "# Note: Using local-token for development mode (server.py allows this when OIDC is disabled)\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "refresh_response = requests.post(\n",
    "    f\"{api_server_base_url}/api/dashboard/refresh-usgs-data\",\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if refresh_response.status_code == 200:\n",
    "    refresh_result = refresh_response.json()\n",
    "    print(f\"âœ… {refresh_result.get('message', 'Data refresh initiated')}\")\n",
    "    \n",
    "    # Wait a moment for background job to start\n",
    "    print(\"â³ Waiting for data refresh to process...\\n\")\n",
    "    time.sleep(3)\n",
    "else:\n",
    "    print(f\"âš ï¸  Data refresh returned status {refresh_response.status_code}\")\n",
    "    print(f\"   Proceeding with existing data...\\n\")\n",
    "\n",
    "# Get insights from all agents\n",
    "response = requests.get(f\"{api_server_base_url}/api/agents/insights\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    print(\"ğŸ’¡ Agent Insights\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # The API returns insights grouped by agent\n",
    "    insights_by_agent = data.get('insights', {})\n",
    "    \n",
    "    count = 0\n",
    "    for agent_name, agent_insights in insights_by_agent.items():\n",
    "        print(f\"\\nğŸ¤– {agent_name.replace('_', ' ').title()}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for insight in agent_insights:\n",
    "            title = insight.get('title', 'N/A')\n",
    "            value = insight.get('value', 'N/A')\n",
    "            change = insight.get('change')\n",
    "            urgency = insight.get('urgency', 'normal')\n",
    "            timestamp = insight.get('timestamp', '')\n",
    "            \n",
    "            urgency_icon = {\n",
    "                'critical': 'ğŸ”´',\n",
    "                'high': 'ğŸŸ¡',\n",
    "                'normal': 'ğŸ”µ',\n",
    "                'low': 'ğŸŸ¢'\n",
    "            }.get(urgency, 'âšª')\n",
    "            \n",
    "            print(f\"\\n{urgency_icon} {title}: {value}\")\n",
    "            if change:\n",
    "                print(f\"   Change: {change}\")\n",
    "            \n",
    "            count += 1\n",
    "            if count >= 15:  # Limit total insights shown\n",
    "                break\n",
    "        \n",
    "        if count >= 15:\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Generated at: {data.get('generated_at', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"âŒ Error fetching insights: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Risk Analyzer Agent\n",
    "\n",
    "Calculates comprehensive flood risk scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Risk Analyzer Agent...\n",
      "\n",
      "ğŸš€ Starting risk_analyzer agent...\n",
      "\n",
      "[WARNING] Using provided input_schema for multi-argument function\n",
      "[WARNING] Using provided input_schema for multi-argument function\n",
      "[WARNING] Using provided input_schema for multi-argument function\n",
      "ğŸ’¬ \n",
      "------------------------------\n",
      "[AGENT]\n",
      "\u001b[33mAgent input: Analyze current flood risk for the Texas:\n",
      "    1. Calculate detailed risk scores for all factors\n",
      "    2. Identify the highest risk components\n",
      "    3. Provide trend analysis\n",
      "    4. Give recommendations for monitoring\n",
      "\n",
      "    Be specific about the risk levels and factors.\n",
      "\u001b[36mAgent's thoughts: \n",
      "k>\n",
      "\n",
      "Action: comprehensive_flood_analysis_tool\n",
      "Action Input: {\"location\": \"Texas\"}\u001b[39m\n",
      "------------------------------\n",
      "ğŸ’¬ \n",
      "------------------------------\n",
      "[AGENT]\n",
      "\u001b[37mCalling tools: comprehensive_flood_analysis_tool\n",
      "\u001b[33mTool's input: {'location': 'Texas'}\n",
      "\u001b[36mTool's response: \n",
      "{\"status\":\"success\",\"location\":\"Texas\",\"analysis_timestamp\":\"2025-10-21T01:26:31.974090+00:00\",\"data_collection\":{\"agent\":\"Data Collector\",\"data_sources\":{\"usgs_sites\":10,\"noaa_alerts\":0,\"weather_locations\":7},\"insights\":[{\"title\":\"ğŸ”„ Data Freshness\",\"value\":\"100% current\",\"trend\":\"up\",\"urgency\":\"normal\"},{\"title\":\"ğŸŒ API Connectivity\",\"value\":\"5/5 active\",\"trend\":\"up\",\"urgency\":\"normal\"},{\"title\":\"ğŸ“Š Data Quality\",\"value\":\"9.3/10\",\"trend\":\"up\",\"urgency\":\"normal\"},{\"title\":\"âš¡ Update Frequency\",\"value\":\"12 updates/hour\",\"trend\":\"stable\",\"urgency\":\"normal\"}],\"alerts\":[]},\"risk_analysis\":{\"agent\":\"Risk Analyzer\",\"insights\":[{\"title\":\"ğŸ¯ Overall Risk Level\",\"value\":\"Unknown (0.0/10)\",\"trend\":\"stable\",\"urgency\":\"normal\"},{\"title\":\"ğŸš¨ Critical Watersheds\",\"value\":\"0 areas\",\"trend\":\"stable\",\"urgency\":\"normal\"},{\"title\":\"ğŸ“ˆ Risk Trend Analysis\",\"value\":\"Insufficient data\",\"trend\":\"stable\",\"urgency\":\"normal\"},{\"title\":\"ğŸ§  AI Confidence\",\"value\":\"0%\",\"trend\":\"stable\",\"urgency\":\"normal\"}],\"alerts\":[]},\"...(rest of response truncated)\u001b[39m\n",
      "------------------------------\n",
      "ğŸ’¬ \n",
      "------------------------------\n",
      "[AGENT]\n",
      "\u001b[33mAgent input: Analyze current flood risk for the Texas:\n",
      "    1. Calculate detailed risk scores for all factors\n",
      "    2. Identify the highest risk components\n",
      "    3. Provide trend analysis\n",
      "    4. Give recommendations for monitoring\n",
      "\n",
      "    Be specific about the risk levels and factors.\n",
      "\u001b[36mAgent's thoughts: \n",
      "k>\n",
      "\n",
      "Action: analyze_flood_risk_tool\n",
      "Action Input: {\"watershed_data\": \"Texas watershed data from USGS sites and NOAA alerts\"}\u001b[39m\n",
      "------------------------------\n",
      "[WARNING] Error calling tool analyze_flood_risk with input: {'watershed_data': 'Texas watershed data from USGS sites and NOAA alerts'}\n",
      "ğŸ’¬ \n",
      "------------------------------\n",
      "[AGENT]\n",
      "\u001b[37mCalling tools: analyze_flood_risk_tool\n",
      "\u001b[33mTool's input: {'watershed_data': 'Texas watershed data from USGS sites and NOAA alerts'}\n",
      "\u001b[36mTool's response: \n",
      "Input validation error: 'Texas watershed data from USGS sites and NOAA alerts' is not valid under any of the given schemas\u001b[39m\n",
      "------------------------------\n",
      "ğŸ’¬ \n",
      "------------------------------\n",
      "[AGENT]\n",
      "\u001b[33mAgent input: Analyze current flood risk for the Texas:\n",
      "    1. Calculate detailed risk scores for all factors\n",
      "    2. Identify the highest risk components\n",
      "    3. Provide trend analysis\n",
      "    4. Give recommendations for monitoring\n",
      "\n",
      "    Be specific about the risk levels and factors.\n",
      "\u001b[36mAgent's thoughts: \n",
      "think>\n",
      "\n",
      "The input validation error occurred because the provided 'watershed_data' was not in the correct format. The analyze_flood_risk_tool requires a valid JSON object with properly structured data, such as specific USGS site identifiers or dataset references, rather than a descriptive string. To proceed, you need to supply the data in the exact format the tool expects, which may involve accessing specific datasets or formatting the input correctly. \n",
      "\n",
      "Final Answer: The analysis cannot be completed due to invalid input formatting for the watershed data. Please provide the data in the correct JSON schema as required by the tool.\u001b[39m\n",
      "------------------------------\n",
      "\n",
      "================================================================================\n",
      "âœ… Data Collection Complete!\n",
      "\n",
      "The analysis cannot be completed due to invalid input formatting for the watershed data. Please provide the data in the correct JSON schema as required by the tool.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run Risk Analyzer Agent via NAT\n",
    "payload = {\n",
    "    \"agent\": \"risk_analyzer\",\n",
    "    \"message\": \"\"\"Analyze current flood risk for the Texas:\n",
    "    1. Calculate detailed risk scores for all factors\n",
    "    2. Identify the highest risk components\n",
    "    3. Provide trend analysis\n",
    "    4. Give recommendations for monitoring\n",
    "    \n",
    "    Be specific about the risk levels and factors.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Running Risk Analyzer Agent...\\n\")\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/nat/chat/stream\",\n",
    "    json=payload,\n",
    "    headers=headers,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    final_output = None\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode('utf-8')\n",
    "            if line_str.startswith('data: '):\n",
    "                data_str = line_str[6:]  # Remove 'data: ' prefix\n",
    "                try:\n",
    "                    data = json.loads(data_str)\n",
    "                    \n",
    "                    # Handle different event types\n",
    "                    event_type = data.get('type')\n",
    "                    \n",
    "                    if event_type == 'start':\n",
    "                        print(f\"ğŸš€ Starting {data.get('agent_type')} agent...\")\n",
    "                        print()\n",
    "                    \n",
    "                    elif event_type == 'log':\n",
    "                        log_entry = data.get('log', {})\n",
    "                        level = log_entry.get('level', 'INFO')\n",
    "                        message = log_entry.get('message', '')\n",
    "                        \n",
    "                        # Show important logs\n",
    "                        if level in ['WARNING', 'ERROR']:\n",
    "                            print(f\"[{level}] {message}\")\n",
    "                        elif 'Agent' in message or 'Final Answer' in message or 'Tool' in message:\n",
    "                            print(f\"ğŸ’¬ {message}\")\n",
    "                    \n",
    "                    elif event_type == 'result':\n",
    "                        final_output = data.get('output')\n",
    "                        print(\"\\n\" + \"=\"*80)\n",
    "                        print(\"âœ… Data Collection Complete!\\n\")\n",
    "                    \n",
    "                    elif event_type == 'error':\n",
    "                        print(f\"\\nâŒ Error: {data.get('error')}\")\n",
    "                    \n",
    "                    elif event_type == 'done':\n",
    "                        break\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "    \n",
    "    # Display final output\n",
    "    if final_output:\n",
    "        print(final_output)\n",
    "        print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: H2OGPTE ML Agent\n",
    "\n",
    "AutoML agent for model training and optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Running H2OGPTE ML Agent...\n",
      "\n",
      "âœ… ML Recommendations Complete!\n",
      "\n",
      "================================================================================\n",
      "{'output': '**1. Feature Engineering for Raw Sensor Data**  \\n- **Temporal Features**: Extract statistical metrics (mean, max, moving averages) and time-based patterns (e.g., rainfall rate, duration).  \\n- **Spatial Features**: Incorporate proximity to water bodies, elevation, and spatial correlations between sensors.  \\n- **Domain-Specific**: Soil moisture levels, river flow rates, and weather indicators (e.g., humidity, wind speed).  \\n- **Spectral Analysis**: Use FFT to identify periodic patterns in rainfall or water level data.  \\n\\n**2. Model Recommendations**  \\n- **Baseline**: ARIMA/SARIMA for simplicity and interpretability.  \\n- **Deep Learning**: **LSTM/GRU** for sequential patterns; **Transformers** (e.g., Informer) for long-term dependencies.  \\n- **Hybrid Models**: Combine physical hydrological models (e.g., HEC-HMS) with ML for improved accuracy.  \\n- **AutoML Tools**: Use **AutoTS** or **H2O AutoML** for automated model selection and hyperparameter tuning.  \\n\\n**3. Handling Imbalanced Data**  \\n- **Oversampling**: SMOTE or synthetic flood event generation using GANs.  \\n- **Class Weights**: Assign higher weights to flood events during training.  \\n- **Anomaly Detection**: Frame flood prediction as an anomaly detection task (e.g., Isolation Forest, Autoencoders).  \\n- **Evaluation Metrics**: Focus on F1-score, precision-recall curves, and ROC-AUC.  \\n\\n**4. Validation Strategy**  \\n- **Time-Series Cross-Validation**: Use purged walk-forward validation to avoid look-ahead bias.  \\n- **Multi-Horizon Forecasting**: Validate performance across short-term (hours) and long-term (days) predictions.  \\n- **Uncertainty Quantification**: Implement probabilistic forecasting (e.g., Monte Carlo dropout, Bayesian NNs).  \\n\\n**AutoML Workflow**  \\n1. **Data Preprocessing**: Automated feature engineering with tools like **Featuretools**.  \\n2. **Model Selection**: Run **TPOT** or **AutoGluon** for rapid benchmarking of ML models.  \\n3. **Hyperparameter Tuning**: Optuna or Ray Tune for distributed Bayesian optimization.  \\n4. **Deployment**: Containerize the pipeline using Docker and monitor drift with **Evidently AI**.  \\n\\n**Example Pipeline**  \\n```python\\nfrom autots import AutoTS\\nimport pandas as pd\\n\\n# Load sensor data (timestamp, rainfall_mm, flow_cfs, ...)\\ndf = pd.read_csv(\"sensor_data.csv\")\\n\\n# Initialize AutoTS\\nmodel = AutoTS(\\n    forecast_length=24,  # 24-hour forecast\\n    frequency=\"1H\",\\n    prediction_interval=0.95,\\n    seasonalities=None  # Automatically detected\\n)\\n\\n# Fit and predict\\nmodel = model.fit(df)\\nforecast = model.predict(df)\\n```  \\n\\n**Key Considerations**  \\n- **Real-Time Inference**: Deploy lightweight models (e.g., pruned LSTMs) for edge devices.  \\n- **Explainability**: Use SHAP values or LIME to interpret predictions for stakeholders.  \\n- **Data Quality**: Implement anomaly detection for sensor malfunctions (e.g., sudden flatlining).  \\n\\nLet me know if you need help implementing specific components!', 'agent_type': 'risk_analyzer', 'status': 'success', 'logs': [], 'timestamp': '2025-10-21T01:27:47.473230+00:00'}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run H2OGPTE ML Agent via NAT\n",
    "payload = {\n",
    "    \"agent\": \"h2ogpte_agent\",\n",
    "    \"message\": \"\"\"Help me design an ML pipeline for flood prediction:\n",
    "    1. What features should I engineer from raw sensor data?\n",
    "    2. What model types work best for flood forecasting?\n",
    "    3. How should I handle imbalanced flood event data?\n",
    "    4. What validation strategy is appropriate for time-series?\n",
    "    \n",
    "    Provide actionable AutoML recommendations.\"\"\"\n",
    "}\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "print(\"ğŸ§  Running H2OGPTE ML Agent...\\n\")\n",
    "\n",
    "response = requests.post(f\"{api_server_base_url}/api/nat/chat\", json=payload, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"âœ… ML Recommendations Complete!\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(result.get('response', result))\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Real-World Data Integration\n",
    "\n",
    "## ğŸŒ Live Data from Government APIs\n",
    "\n",
    "Our system integrates with real-time data sources:\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "1. **USGS Water Services**\n",
    "   - Real-time streamflow (CFS)\n",
    "   - Gage height (feet)\n",
    "   - 12 monitoring stations in Texas\n",
    "   - Updated every 15 minutes\n",
    "\n",
    "2. **NOAA Weather Service**\n",
    "   - Flood warnings and watches\n",
    "   - Weather alerts\n",
    "   - Forecast data\n",
    "\n",
    "3. **Open-Meteo**\n",
    "   - Weather forecasts\n",
    "   - Flood API predictions\n",
    "   - Historical data\n",
    "\n",
    "### Let's view live watershed data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refresh USGS Data Manually\n",
    "\n",
    "Trigger a fresh data collection from USGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Triggering USGS data refresh...\n",
      "\n",
      "âŒ Error: 500\n",
      "ğŸŒŠ Live Watershed Data\n",
      "\n",
      "====================================================================================================\n",
      "                                                name  current_streamflow_cfs  risk_score  trend_rate_cfs_per_hour        last_updated\n",
      "      Guadalupe River at Comfort, TX (USGS 08167000)                  2850.0         8.2                     45.2 2025-10-21 01:06:25\n",
      "        Colorado River at Austin, TX (USGS 08158000)                  3200.5         7.9                     38.7 2025-10-21 01:06:25\n",
      "    Sabine River near Gladewater, TX (USGS 08020000)                  1890.5         6.3                     22.1 2025-10-21 01:06:25\n",
      "      Brazos River near Rosharon, TX (USGS 08116650)                  1245.7         5.8                     12.4 2025-10-21 01:06:25\n",
      "        Red River at Arthur City, TX (USGS 07335500)                  1567.9         5.4                     18.6 2025-10-21 01:06:25\n",
      "        Nueces River near Mathis, TX (USGS 08211000)                   812.3         5.1                     14.7 2025-10-21 01:06:25\n",
      "         Trinity River at Dallas, TX (USGS 08057000)                   890.3         4.7                    -15.3 2025-10-21 01:06:25\n",
      "       Llano River near Junction, TX (USGS 08150000)                   423.7         4.1                      8.9 2025-10-21 01:06:25\n",
      "    Canadian River near Amarillo, TX (USGS 07227500)                   678.4         3.2                      5.1 2025-10-21 01:06:25\n",
      "         Neches River at Evadale, TX (USGS 08041000)                   445.2         2.8                     -2.3 2025-10-21 01:06:25\n",
      "San Antonio River at San Antonio, TX (USGS 08178000)                   156.8         2.1                      0.8 2025-10-21 01:06:25\n",
      "      Pecos River near Sheffield, TX (USGS 08447000)                   234.1         1.9                     -8.7 2025-10-21 01:06:25\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š Total Watersheds Monitored: 12\n",
      "\n",
      "âš ï¸  Risk Distribution:\n",
      "   ğŸ”´ High Risk (>7.0): 2\n",
      "   ğŸŸ¡ Medium Risk (4.0-7.0): 6\n",
      "   ğŸŸ¢ Low Risk (<4.0): 4\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ Triggering USGS data refresh...\\n\")\n",
    "header = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "response = requests.post(f\"{api_server_base_url}/api/dashboard/refresh-usgs-data\", headers=header)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    time.sleep(5)\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "\n",
    "# Get current watershed data\n",
    "response = requests.get(f\"{api_server_base_url}/api/watersheds\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    watersheds = response.json()\n",
    "    \n",
    "    # Convert to DataFrame for nice display\n",
    "    df = pd.DataFrame(watersheds)\n",
    "    \n",
    "    # Select key columns\n",
    "    display_cols = ['name', 'current_streamflow_cfs', 'risk_score', \n",
    "                    'trend_rate_cfs_per_hour', 'last_updated']\n",
    "    \n",
    "    available_cols = [col for col in display_cols if col in df.columns]\n",
    "    \n",
    "    print(\"ğŸŒŠ Live Watershed Data\\n\")\n",
    "    print(\"=\"*100)\n",
    "    print(df[available_cols].to_string(index=False))\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nğŸ“Š Total Watersheds Monitored: {len(watersheds)}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if 'risk_score' in df.columns:\n",
    "        high_risk = len(df[df['risk_score'] > 7.0])\n",
    "        medium_risk = len(df[(df['risk_score'] >= 4.0) & (df['risk_score'] <= 7.0)])\n",
    "        low_risk = len(df[df['risk_score'] < 4.0])\n",
    "        \n",
    "        print(f\"\\nâš ï¸  Risk Distribution:\")\n",
    "        print(f\"   ğŸ”´ High Risk (>7.0): {high_risk}\")\n",
    "        print(f\"   ğŸŸ¡ Medium Risk (4.0-7.0): {medium_risk}\")\n",
    "        print(f\"   ğŸŸ¢ Low Risk (<4.0): {low_risk}\")\n",
    "else:\n",
    "    print(f\"âŒ Error fetching watersheds: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Summary & Next Steps\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "In this notebook, you've learned how to:\n",
    "\n",
    "âœ… **Set up a multi-agent AI system** for disaster response  \n",
    "âœ… **Integrate h2oGPTe** for AutoML and model training  \n",
    "âœ… **Use NVIDIA NIM** for high-performance inference  \n",
    "âœ… **Build NAT agent workflows** with React patterns  \n",
    "âœ… **Implement FastMCP servers** with custom tools  \n",
    "âœ… **Integrate real-time data** from government APIs  \n",
    "âœ… **Coordinate multiple AI agents** for complex tasks  \n",
    "âœ… **Evaluate responses** using LLM-as-Judge  \n",
    "\n",
    "## Architecture Highlights\n",
    "\n",
    "- **5 Specialized Agents**: Data Collector, Risk Analyzer, Emergency Responder, Predictor, H2OGPTE ML\n",
    "- **20+ MCP Tools**: Via FastMCP server on port 8001\n",
    "- **NVIDIA NIM Models**: Nemotron Super 49B, Llama 3.1 variants\n",
    "- **h2oGPTe A2A**: Agent-to-agent AutoML capabilities\n",
    "- **Real-time Data**: USGS, NOAA, Weather APIs\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **NVIDIA NIM**: [build.nvidia.com](https://build.nvidia.com)\n",
    "- **h2oGPTe**: [h2o.ai/platform/enterprise-h2ogpte](https://h2o.ai/platform/enterprise-h2ogpte/)\n",
    "- **NVIDIA NAT**: [docs.nvidia.com/nat](https://docs.nvidia.com/nat)\n",
    "- **FastMCP**: [github.com/jlowin/fastmcp](https://github.com/jlowin/fastmcp)\n",
    "- **USGS Water Data**: [waterdata.usgs.gov](https://waterdata.usgs.gov)\n",
    "- **NOAA Weather**: [weather.gov](https://weather.gov)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Customize Agents**: Modify agent configs for your specific use case\n",
    "2. **Add Data Sources**: Integrate additional APIs and sensors\n",
    "3. **Train ML Models**: Use h2oGPTe to train production models\n",
    "4. **Deploy to Production**: Use Docker/Kubernetes deployment\n",
    "5. **Monitor Performance**: Add logging and metrics\n",
    "\n",
    "## Contributing\n",
    "\n",
    "This is an open-source AI for Good project. Contributions welcome!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŠ Thank you for exploring the Flood Prediction Blueprint!\n",
    "\n",
    "**Built with â¤ï¸ using h2oGPTe and NVIDIA NIM**\n",
    "\n",
    "For questions and support, please open an issue in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
