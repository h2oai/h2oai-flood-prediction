{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI-Powered Flood Prediction System with h2oGPTe and NVIDIA NIM\n",
    "\n",
    "[![Deploy on NVIDIA](https://img.shields.io/badge/Deploy%20on-NVIDIA%20AI%20Blueprints-76B900?logo=nvidia&logoColor=white)](https://build.nvidia.com)\n",
    "[![H2O.ai](https://img.shields.io/badge/Powered%20by-H2O.ai-FFD500)](https://h2o.ai)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒŠ Overview\n",
    "\n",
    "This blueprint demonstrates an **AI-powered flood prediction and disaster response system** that combines:\n",
    "\n",
    "- **h2oGPTe Agent-to-Agent (A2A)**: Advanced AutoML capabilities with Driverless AI for model training and feature engineering\n",
    "- **NVIDIA NIM**: State-of-the-art inference with `nvidia/llama-3.3-nemotron-super-49b-v1.5` and other NVIDIA models\n",
    "- **NVIDIA NAT Pipeline**: React Agent workflows for multi-agent orchestration\n",
    "- **FastMCP Server**: 20+ specialized tools across 5 intelligent agents\n",
    "- **Real-time Data Integration**: USGS Water Services, NOAA Forecasts, and Weather APIs\n",
    "\n",
    "### ğŸ¯ Use Case: AI for Good - Disaster Response\n",
    "\n",
    "This system provides:\n",
    "- **Real-time flood monitoring** with live data from watersheds and monitoring stations\n",
    "- **AI-powered risk assessment** using advanced machine learning models\n",
    "- **Emergency response coordination** with automated alerts and evacuation planning\n",
    "- **Predictive analytics** for flood forecasting 24-72 hours ahead\n",
    "- **AutoML model training** for continuous improvement of prediction accuracy\n",
    "\n",
    "### ğŸ—ï¸ Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      Flood Prediction System                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚   h2oGPTe    â”‚  â”‚  NVIDIA NIM  â”‚  â”‚  FastMCP     â”‚         â”‚\n",
    "â”‚  â”‚  (A2A Mode)  â”‚  â”‚  (Nemotron)  â”‚  â”‚   Server     â”‚         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚         â”‚                 â”‚                 â”‚                   â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                  â”‚                 â”‚                            â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚         â”‚    NVIDIA NAT Agent Pipeline      â”‚                  â”‚\n",
    "â”‚         â”‚      (React Agent Workflow)       â”‚                  â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                  â”‚                                              â”‚\n",
    "â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\n",
    "â”‚    â”‚    5 Specialized Agents   â”‚                               â”‚\n",
    "â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â”‚\n",
    "â”‚    â”‚  1. Data Collector        â”‚ â—„â”€â”€ USGS Water Data           â”‚\n",
    "â”‚    â”‚  2. Risk Analyzer         â”‚ â—„â”€â”€ NOAA Flood Alerts         â”‚\n",
    "â”‚    â”‚  3. Emergency Responder   â”‚ â—„â”€â”€ Weather APIs              â”‚\n",
    "â”‚    â”‚  4. AI Predictor          â”‚                               â”‚\n",
    "â”‚    â”‚  5. H2OGPTE ML Agent      â”‚                               â”‚\n",
    "â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  Output: Real-time Monitoring, Alerts, Predictions, ML Models  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ”‘ Key Technologies\n",
    "\n",
    "1. **h2oGPTe**: Enterprise AI platform with agent mode for AutoML and advanced analytics\n",
    "2. **NVIDIA NIM**: Optimized inference microservices for AI models\n",
    "3. **NVIDIA NAT**: Agent orchestration framework with React-based workflows\n",
    "4. **FastMCP**: Model Context Protocol server for tool integration\n",
    "5. **FastAPI**: High-performance API server for real-time operations\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ What You'll Learn\n",
    "\n",
    "- Setting up multi-agent AI systems for disaster response\n",
    "- Integrating h2oGPTe for AutoML and model training\n",
    "- Using NVIDIA NIM for high-performance inference\n",
    "- Building NAT agent workflows with React patterns\n",
    "- Implementing FastMCP servers with custom tools\n",
    "- Real-time data integration from government APIs\n",
    "- Coordinating multiple AI agents for complex tasks\n",
    "\n",
    "Let's get started! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "Before running this setup, make sure you have:\n",
    "\n",
    "### Required:\n",
    "- âœ… **NVIDIA API Key** - Get it from [build.nvidia.com](https://build.nvidia.com)\n",
    "  - This key is used for NVIDIA NIM models and services\n",
    "\n",
    "### Optional (but recommended):\n",
    "- ğŸ”¹ **H2OGPTE API Key** - For AutoML and advanced AI features\n",
    "  - Get access at [h2o.ai](https://h2o.ai/platform/enterprise-h2ogpte/)\n",
    "  - If you don't have this, you can skip it - the app will still work with reduced features\n",
    "\n",
    "---\n",
    "\n",
    "Ready? Let's collect your API keys! ğŸ”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Dependencies\n",
    "\n",
    "*Please **restart** the kernel after this step. Do not repeat this step after restarting the kernel.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "python = sys.executable\n",
    "\n",
    "!{python} -m ensurepip --upgrade\n",
    "!{python} -m pip install --upgrade pip setuptools wheel\n",
    "!{python} -m pip install --upgrade --force-reinstall pandas openai requests python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” Step 1: Collect API Keys\n",
    "\n",
    "Run the cells below to securely enter your API keys. Your inputs will be hidden for security.\n",
    "\n",
    "### What you'll provide:\n",
    "1. **NVIDIA API Key** (required)\n",
    "2. **H2OGPTE API Key** (optional)\n",
    "3. **H2OGPTE URL** (optional, default provided)\n",
    "\n",
    "**Note**: Ports are pre-configured in this Launchable (8090 for Web UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API keys and URLs\n",
    "nvidia_api_key = \"\"\n",
    "ngc_api_key = \"\"\n",
    "h2ogpte_url = \"https://h2ogpte.cloud-dev.h2o.dev\"\n",
    "h2ogpte_api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ Enter your NVIDIA API Key\n",
      "   Get it from: https://build.nvidia.com\n",
      "\n",
      "âœ… NVIDIA API Key collected successfully!\n",
      "   Preview: nvapi-hR2s...o3QS\n"
     ]
    }
   ],
   "source": [
    "# Collect NVIDIA API Key\n",
    "print(\"ğŸ”‘ Enter your NVIDIA API Key\")\n",
    "print(\"   Get it from: https://build.nvidia.com\")\n",
    "print()\n",
    "\n",
    "nvidia_api_key = getpass.getpass(\"NVIDIA API Key: \")\n",
    "\n",
    "if not nvidia_api_key or nvidia_api_key.strip() == \"\":\n",
    "    raise ValueError(\n",
    "        \"âŒ NVIDIA API Key is required! Please run this cell again and provide the key.\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… NVIDIA API Key collected successfully!\")\n",
    "print(f\"   Preview: {nvidia_api_key[:10]}...{nvidia_api_key[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect NGC API Key (optional)\n",
    "print(\"ğŸ”‘ Enter your NGC API Key\")\n",
    "print(\"   Get it from: https://catalog.ngc.nvidia.com/\")\n",
    "print()\n",
    "\n",
    "ngc_api_key = getpass.getpass(\"NGC API Key: \")\n",
    "\n",
    "if not ngc_api_key or ngc_api_key.strip() == \"\":\n",
    "    ngc_api_key = \"\"\n",
    "    print(\"âš ï¸  No API key provided, skipping Local NIM LLM setup\")\n",
    "else:\n",
    "    print(\"âœ… NGC API Key collected successfully!\")\n",
    "    print(f\"   Preview: {ngc_api_key[:10]}...{ngc_api_key[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– H2OGPTE Configuration (Optional)\n",
      "   H2OGPTE provides advanced AutoML and AI agent capabilities\n",
      "   If you don't have access, just press Enter to skip\n",
      "\n",
      "âœ… H2OGPTE configured successfully!\n",
      "   URL: https://h2ogpte.cloud-dev.h2o.dev\n",
      "   API Key Preview: nvapi-hR2s...o3QS\n"
     ]
    }
   ],
   "source": [
    "# Collect H2OGPTE Credentials (Optional)\n",
    "print(\"ğŸ¤– H2OGPTE Configuration (Optional)\")\n",
    "print(\"   H2OGPTE provides advanced AutoML and AI agent capabilities\")\n",
    "print(\"   If you don't have access, just press Enter to skip\")\n",
    "print()\n",
    "\n",
    "use_h2ogpte = input(\"Do you have H2OGPTE access? (yes/no) [no]: \").strip().lower()\n",
    "\n",
    "if use_h2ogpte in [\"yes\", \"y\", \"Yes\", \"Y\", \"YES\"]:\n",
    "    h2ogpte_url = input(\"H2OGPTE URL [https://h2ogpte.cloud-dev.h2o.dev]: \").strip()\n",
    "    h2ogpte_api_key = getpass.getpass(\"H2OGPTE API Key: \")\n",
    "\n",
    "    if not h2ogpte_url:\n",
    "        h2ogpte_url = \"https://h2ogpte.cloud-dev.h2o.dev\"\n",
    "\n",
    "    if not h2ogpte_api_key or h2ogpte_api_key.strip() == \"\":\n",
    "        print(\"âš ï¸  No API key provided, skipping H2OGPTE setup\")\n",
    "        h2ogpte_api_key = \"\"\n",
    "        h2ogpte_url = \"\"\n",
    "    else:\n",
    "        print(\"âœ… H2OGPTE configured successfully!\")\n",
    "        print(f\"   URL: {h2ogpte_url}\")\n",
    "        print(f\"   API Key Preview: {h2ogpte_api_key[:10]}...{h2ogpte_api_key[-4:]}\")\n",
    "else:\n",
    "    h2ogpte_api_key = \"\"\n",
    "    h2ogpte_url = \"https://h2ogpte.cloud-dev.h2o.dev\"\n",
    "    print(\"â­ï¸  Skipping H2OGPTE setup - application will run with NVIDIA NIM only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Step 2: Generate Configuration File\n",
    "\n",
    "Now we'll create the environment configuration file with all the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper function created successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_env_file(\n",
    "    nvidia_api_key,\n",
    "    ngc_api_key,\n",
    "    h2ogpte_api_key,\n",
    "    h2ogpte_url,\n",
    "    output_path=\"./flood_prediction.env\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create the environment configuration file for the Flood Prediction application.\n",
    "\n",
    "    Parameters:\n",
    "    - nvidia_api_key: NVIDIA API key (used for both NVIDIA_API_KEY)\n",
    "    - ngc_api_key: NGC API key (used for NGC_API_KEY)\n",
    "    - h2ogpte_api_key: H2OGPTE API key (can be empty string if not used)\n",
    "    - h2ogpte_url: H2OGPTE service URL\n",
    "    - output_path: Where to save the env file\n",
    "\n",
    "    Note: Ports are pre-configured in the Launchable:\n",
    "    - WEB_PORT: 8090 (fixed)\n",
    "    \"\"\"\n",
    "\n",
    "    env_content = f\"\"\"# ==============================================================================\n",
    "# API KEYS\n",
    "# ==============================================================================\n",
    "\n",
    "NVIDIA_API_KEY={nvidia_api_key}\n",
    "NGC_API_KEY={ngc_api_key}\n",
    "H2OGPTE_API_KEY={h2ogpte_api_key}\n",
    "\n",
    "# ==============================================================================\n",
    "# H2OGPTE CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# H2OGPTE service URL\n",
    "H2OGPTE_URL={h2ogpte_url}\n",
    "\n",
    "# H2OGPTE model to use\n",
    "H2OGPTE_MODEL=claude-sonnet-4-20250514\n",
    "\n",
    "# ==============================================================================\n",
    "# WEB APPLICATION IMAGE CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "WEB_IMAGE_REGISTRY=h2oairelease\n",
    "WEB_IMAGE_REPOSITORY=h2oai-floodprediction-app\n",
    "WEB_IMAGE_TAG=v1.0.0\n",
    "\n",
    "WEB_PORT=8090\n",
    "\n",
    "# ==============================================================================\n",
    "# REDIS CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "REDIS_IMAGE_REGISTRY=docker.io\n",
    "REDIS_IMAGE_REPOSITORY=redis\n",
    "REDIS_IMAGE_TAG=8.2.1\n",
    "\n",
    "REDIS_ENABLED=true\n",
    "\n",
    "# ==============================================================================\n",
    "# NVIDIA NIM LLM CONFIGURATION (Optional)\n",
    "# ==============================================================================\n",
    "\n",
    "NIMLLM_IMAGE=nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0\n",
    "\n",
    "NIMLLM_PORT=8989\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(env_content)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "print(\"âœ… Helper function created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¨ Generating environment configuration file...\n",
      "\n",
      "âœ… Configuration file created successfully!\n",
      "   Location: /Users/srini/code/github.com/h2oai/h2oai-flood-prediction-agent/flood_prediction.env\n",
      "\n",
      "ğŸ“‹ Configuration Summary:\n",
      "   NVIDIA API Key: nvapi-hR2s...o3QS\n",
      "   H2OGPTE API Key: nvapi-hR2s...o3QS\n",
      "   H2OGPTE URL: https://h2ogpte.cloud-dev.h2o.dev\n",
      "\n",
      "ğŸ”Œ Pre-configured Ports:\n",
      "   Web UI Port: 8090 (fixed)\n"
     ]
    }
   ],
   "source": [
    "# Generate the environment file\n",
    "print(\"ğŸ”¨ Generating environment configuration file...\")\n",
    "print()\n",
    "\n",
    "env_file_path = create_env_file(\n",
    "    nvidia_api_key=nvidia_api_key,\n",
    "    ngc_api_key=ngc_api_key,\n",
    "    h2ogpte_api_key=h2ogpte_api_key,\n",
    "    h2ogpte_url=h2ogpte_url,\n",
    ")\n",
    "\n",
    "print(\"âœ… Configuration file created successfully!\")\n",
    "print(f\"   Location: {os.path.abspath(env_file_path)}\")\n",
    "print()\n",
    "print(\"ğŸ“‹ Configuration Summary:\")\n",
    "print(f\"   NVIDIA API Key: {nvidia_api_key[:10]}...{nvidia_api_key[-4:]}\")\n",
    "if h2ogpte_api_key:\n",
    "    print(f\"   H2OGPTE API Key: {h2ogpte_api_key[:10]}...{h2ogpte_api_key[-4:]}\")\n",
    "    print(f\"   H2OGPTE URL: {h2ogpte_url}\")\n",
    "else:\n",
    "    print(\"   H2OGPTE: Not configured (optional)\")\n",
    "print()\n",
    "print(\"ğŸ”Œ Pre-configured Ports:\")\n",
    "print(\"   Web UI Port: 8090 (fixed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variables from the created file\n",
    "\n",
    "load_dotenv(\"./flood_prediction.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ³ Step 3: Pull Docker Images\n",
    "\n",
    "Now we'll authenticate with Docker registries and pull the required images.\n",
    "\n",
    "This may take 5-10 minutes depending on your connection speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Web application image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.0.0: Pulling from h2oairelease/h2oai-floodprediction-app\n",
      "Digest: sha256:9aafbdba37ad48cd9d7fd02a8e8f30cfa2ea349e3bf6e976892818e6190ee0c9\n",
      "Status: Image is up to date for h2oairelease/h2oai-floodprediction-app:v1.0.0\n",
      "docker.io/h2oairelease/h2oai-floodprediction-app:v1.0.0\n"
     ]
    }
   ],
   "source": [
    "!docker pull h2oairelease/h2oai-floodprediction-app:v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                               TAG       IMAGE ID       CREATED        SIZE\n",
      "h2oairelease/h2oai-floodprediction-app   v1.0.0    9aafbdba37ad   26 hours ago   2.68GB\n",
      "redis                                    8.2.1     5fa2edb1e408   2 months ago   222MB\n"
     ]
    }
   ],
   "source": [
    "!docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### OPTIONAL - Locally deployed NIM Microservices. \n",
    "\n",
    "\n",
    "\n",
    "<p style=\"background-color: #bdf55d;\">\n",
    "By default, this notebook will use hosted endpoints for the NVIDIA LLMs. If Nvidia H200 GPU or equivalent is available, you may host a local NIM LLM. \n",
    "Otherwise, skip the next few cells\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if Nvidia Drivers are installed and GPU is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate Docker with NGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull Nvidia NIM LLM Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "\n",
    "# The image is defined in the env file as NIMLLM_IMAGE\n",
    "nim_llm_image = os.getenv(\n",
    "    \"NIMLLM_IMAGE\", \"nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0\"\n",
    ")\n",
    "nim_llm_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "\n",
    "# pull image\n",
    "!docker pull \"${NIMLLM_IMAGE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "\n",
    "# check available images\n",
    "!docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the application using Docker Compose\n",
    "\n",
    "Run only one of the `docker compose` commands below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy with a local NVIDIA NIM LLM - **Only if NVIDIA GPU is available and NIM LLM Image was successfully pulled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "\n",
    "!docker compose -f ./deployment/nvidia-launchable/docker-compose.yml -f ./deployment/nvidia-launchable/docker-compose.nimllm.yml --env-file ./flood_prediction.env up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If not using local NVIDIA NIM LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mRunning\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ‹\u001b[0m Container flood-prediction-web                                                                                                                     Recreate\u001b[34m0.1s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mRunning\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ™\u001b[0m Container flood-prediction-web                                                                                                                     Recreate\u001b[34m0.2s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mRunning\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ¹\u001b[0m Container flood-prediction-web                                                                                                                     Recreate\u001b[34m0.3s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mRunning\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ¸\u001b[0m Container flood-prediction-web                                                                                                                     Recreate\u001b[34m0.4s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mRunning\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ¼\u001b[0m Container flood-prediction-web                                                                                                                     Recreate\u001b[34m0.5s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mRunning\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ´\u001b[0m Container flood-prediction-web                                                                                                                     Recreate\u001b[34m0.6s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mRunning\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ¦\u001b[0m Container flood-prediction-web                                                                                                                     Recreate\u001b[34m0.7s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mRunning\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ §\u001b[0m Container flood-prediction-web                                                                                                                     Recreate\u001b[34m0.8s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mRunning\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      " \u001b[33mâ ‡\u001b[0m Container flood-prediction-web                                                                                                                     Recreate\u001b[34m0.9s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[33mâ ™\u001b[0m Container flood-prediction-redis                                                                                                                   Waiting\u001b[34m1.0s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web                                                                                                                     \u001b[32mRecreated\u001b[0m\u001b[34m0.9s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[33mâ ¹\u001b[0m Container flood-prediction-redis                                                                                                                   Waiting\u001b[34m1.1s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web                                                                                                                     \u001b[32mRecreated\u001b[0m\u001b[34m0.9s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[33mâ ¸\u001b[0m Container flood-prediction-redis                                                                                                                   Waiting\u001b[34m1.2s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web                                                                                                                     \u001b[32mRecreated\u001b[0m\u001b[34m0.9s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[33mâ ¼\u001b[0m Container flood-prediction-redis                                                                                                                   Waiting\u001b[34m1.3s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web                                                                                                                     \u001b[32mRecreated\u001b[0m\u001b[34m0.9s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[33mâ ´\u001b[0m Container flood-prediction-redis                                                                                                                   Waiting\u001b[34m1.4s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web                                                                                                                     \u001b[32mRecreated\u001b[0m\u001b[34m0.9s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mHealthy\u001b[0m\u001b[34m1.5s \u001b[0m\n",
      " \u001b[33mâ \u001b[0m Container flood-prediction-web                                                                                                                     Starting\u001b[34m1.5s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 3/3\u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-redis                                                                                                                   \u001b[32mHealthy\u001b[0m\u001b[34m1.5s \u001b[0m\n",
      " \u001b[32mâœ”\u001b[0m Container flood-prediction-web                                                                                                                     \u001b[32mStarted\u001b[0m\u001b[34m1.6s \u001b[0m\n",
      " \u001b[33m\u001b[1m!\u001b[0m web The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested \u001b[33m\u001b[1m\u001b[0m\u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker compose -f ./deployment/nvidia-launchable/docker-compose.yml --env-file ./flood_prediction.env up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for all containers to be healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                           COMMAND                  CREATED          STATUS                    PORTS                    NAMES\n",
      "e0fa514aa459   h2oairelease/h2oai-floodprediction-app:v1.0.0   \"./docker-entrypointâ€¦\"   27 seconds ago   Up 25 seconds (healthy)   0.0.0.0:8090->8000/tcp   flood-prediction-web\n",
      "c3cefd4e9e5c   redis:8.2.1                                     \"docker-entrypoint.sâ€¦\"   19 minutes ago   Up 19 minutes (healthy)   6379/tcp                 flood-prediction-redis\n"
     ]
    }
   ],
   "source": [
    "# Check the status of the deployed containers. Wait till the STATUS is healthy\n",
    "\n",
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_key(value: str) -> str:\n",
    "    if not value:\n",
    "        return \"Not set\"\n",
    "    if len(value) <= 14:\n",
    "        return f\"{value[:4]}{'.' * len(value) - 4}\"\n",
    "    dots = max(1, len(value) - 14)\n",
    "    return f\"{value[:10]}{'.' * dots}{value[-4:]}\"\n",
    "\n",
    "\n",
    "def check_service(base_url, path, name, headers=None):\n",
    "    url = f\"{base_url}/{path.lstrip('/')}\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "    except requests.exceptions.RequestException as exc:\n",
    "        print(f\"âŒ {name} is not responding: {exc}\")\n",
    "        return False\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f\"âœ… {name} is running\")\n",
    "        return True\n",
    "    if response.status_code == 401:\n",
    "        print(f\"âš ï¸  {name} requires authentication (401)\")\n",
    "        return True\n",
    "\n",
    "    print(f\"âš ï¸  {name} responded with status {response.status_code}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NVIDIA_API_KEY: nvapi-hR2s........................................................o3QS\n",
      "âœ… H2OGPTE_URL: https://h2ogpte.cloud-dev.h2o.dev\n",
      "âœ… H2OGPTE_API_KEY: nvapi-hR2s........................................................o3QS\n",
      "âœ… h2oGPTe credentials found\n",
      "âœ… API_SERVER_URL detected: http://localhost:8090\n"
     ]
    }
   ],
   "source": [
    "NVIDIA_API_KEY = os.getenv(\"NVIDIA_API_KEY\")\n",
    "if NVIDIA_API_KEY:\n",
    "    print(f\"âœ… NVIDIA_API_KEY: {preview_key(NVIDIA_API_KEY)}\")\n",
    "else:\n",
    "    print(\"â— NVIDIA_API_KEY NOT SET\")\n",
    "\n",
    "H2OGPTE_URL = os.getenv(\"H2OGPTE_URL\")\n",
    "if H2OGPTE_URL:\n",
    "    print(f\"âœ… H2OGPTE_URL: {H2OGPTE_URL}\")\n",
    "else:\n",
    "    print(\"âš ï¸ H2OGPTE_URL NOT SET\")\n",
    "\n",
    "H2OGPTE_API_KEY = os.getenv(\"H2OGPTE_API_KEY\")\n",
    "if H2OGPTE_API_KEY:\n",
    "    print(f\"âœ… H2OGPTE_API_KEY: {preview_key(H2OGPTE_API_KEY)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ H2OGPTE_API_KEY NOT SET\")\n",
    "\n",
    "if H2OGPTE_URL and H2OGPTE_API_KEY:\n",
    "    print(\"âœ… h2oGPTe credentials found\")\n",
    "    H2OGPTE_AVAILABLE = True\n",
    "else:\n",
    "    print(\"âš ï¸  h2oGPTe credentials not set\")\n",
    "    print(\"This section will be skipped. To enable:\")\n",
    "    print(\"  export H2OGPTE_URL='<your-url>'\")\n",
    "    print(\"  export H2OGPTE_API_KEY='<your-key>'\")\n",
    "    H2OGPTE_AVAILABLE = False\n",
    "\n",
    "api_port = os.getenv(\"WEB_PORT\")\n",
    "if not api_port:\n",
    "    print(\"âš ï¸ WEB_PORT not set. API interactions will be skipped.\")\n",
    "\n",
    "api_server_base_url = os.getenv(\"API_SERVER_URL\", f\"http://localhost:{api_port}\")\n",
    "if api_port and api_server_base_url:\n",
    "    print(f\"âœ… API_SERVER_URL detected: {api_server_base_url}\")\n",
    "else:\n",
    "    print(\"âš ï¸ API_SERVER_URL not set. API interactions will be skipped.\")\n",
    "\n",
    "# Set this to False Here.\n",
    "# Later, we will verify that a Local NIM LLM is deployed and is reachable. If yes, we will set this to True\n",
    "LOCAL_NIM_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Service Endpoints:\n",
      "   - FastAPI Server: http://localhost:8090\n",
      "   - API Docs: http://localhost:8090/docs\n",
      "   - Agents API: http://localhost:8090/api/agents\n",
      "\n",
      "Use the next cell to verify service health.\n"
     ]
    }
   ],
   "source": [
    "if not api_server_base_url:\n",
    "    print(\"âš ï¸  API_SERVER_URL not set. Export API_SERVER_URL before continuing.\")\n",
    "else:\n",
    "    base_url = api_server_base_url.rstrip(\"/\")\n",
    "    print(\"ğŸ“ Service Endpoints:\")\n",
    "    print(f\"   - FastAPI Server: {base_url}\")\n",
    "    print(f\"   - API Docs: {base_url}/docs\")\n",
    "    print(f\"   - Agents API: {base_url}/api/agents\")\n",
    "    print(\"\\nUse the next cell to verify service health.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify Services\n",
    "\n",
    "Let's check that all services are running correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking service health...\n",
      "\n",
      "âœ… FastAPI Dashboard is running\n",
      "âœ… Agents API is running\n",
      "âœ… Watersheds API is running\n",
      "\n",
      "==================================================\n",
      "âœ… All services are responding!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Checking service health...\\n\")\n",
    "\n",
    "if not api_server_base_url:\n",
    "    print(\"âš ï¸  API_SERVER_URL not set. Skipping health checks.\")\n",
    "    fastapi_ok = False\n",
    "else:\n",
    "    base_url = api_server_base_url.rstrip(\"/\")\n",
    "\n",
    "    fastapi_ok = check_service(\n",
    "        base_url,\n",
    "        \"api/dashboard\",\n",
    "        \"FastAPI Dashboard\",\n",
    "    )\n",
    "    agents_ok = check_service(\n",
    "        base_url,\n",
    "        \"api/agents\",\n",
    "        \"Agents API\",\n",
    "        headers={\"Authorization\": \"Bearer local-token\"},\n",
    "    )\n",
    "    watersheds_ok = check_service(\n",
    "        base_url,\n",
    "        \"api/watersheds\",\n",
    "        \"Watersheds API\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    if fastapi_ok and agents_ok and watersheds_ok:\n",
    "        print(\"âœ… All services are responding!\")\n",
    "    else:\n",
    "        print(\"âš ï¸  One or more endpoints did not respond as expected.\")\n",
    "        print(\"Review Helm deployment status or API logs if needed.\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that a Local NIM LLM is deployed and is reachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Local NIM LLM is running, it will be available at: http://localhost:8989/v1\n"
     ]
    }
   ],
   "source": [
    "# Set the base url for NIM\n",
    "nimllm_port = os.getenv(\"NIMLLM_PORT\", \"8989\")\n",
    "nim_llm_base_url = f\"http://localhost:{nimllm_port}/v1\"\n",
    "print(f\"If Local NIM LLM is running, it will be available at: {nim_llm_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No locally deployed NIM LLM is found. Will be using NIMs via API.\n"
     ]
    }
   ],
   "source": [
    "# Test if the NIM container is up. This will take 15 to 30 mins. Repeat this step until you see a positive message.\n",
    "if nim_llm_base_url:\n",
    "    try:\n",
    "        response = requests.get(f\"{nim_llm_base_url}/models\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"âœ… Local NIM LLM is running\")\n",
    "            LOCAL_NIM_AVAILABLE = True\n",
    "        else:\n",
    "            print(\"âš ï¸ No locally deployed NIM LLM is found. Will be using NIMs via API.\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ No locally deployed NIM LLM is found. Will be using NIMs via API.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No locally deployed NIM LLM is found. Will be using NIMs via API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: NVIDIA NIM Integration\n",
    "\n",
    "## ğŸš€ NVIDIA NIM - Optimized Inference Microservices\n",
    "\n",
    "NVIDIA NIM provides high-performance inference for state-of-the-art language models. Our flood prediction system uses several NVIDIA models:\n",
    "\n",
    "### Available Models\n",
    "\n",
    "1. **nvidia/llama-3.3-nemotron-super-49b-v1.5** (Default)\n",
    "   - Latest Nemotron model optimized for instruction following\n",
    "   - Excellent for agent workflows and tool calling\n",
    "   - 49B parameters with superior efficiency\n",
    "\n",
    "2. **meta/llama-3.1-70b-instruct**\n",
    "   - Strong general-purpose reasoning\n",
    "   - Great for complex analysis tasks\n",
    "\n",
    "### Let's test NVIDIA NIM integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NVIDIA NIM client\n",
    "# If LOCAL_NIM_AVAILABLE is True, it will be preferred. Please modify this cell to use external NVIDIA API.\n",
    "\n",
    "USE_NVIDIA_API = False\n",
    "\n",
    "if LOCAL_NIM_AVAILABLE:\n",
    "    print(f\"Using local NIM LLM base URL: {nim_llm_base_url}\")\n",
    "    model = \"nvidia/llama-3_3-nemotron-super-49b-v1_5\"\n",
    "    client = OpenAI(\n",
    "        base_url=nim_llm_base_url,\n",
    "        api_key=\"no-key\",\n",
    "    )\n",
    "else:\n",
    "    USE_NVIDIA_API = True\n",
    "\n",
    "    # Test with Nemotron Super 49B\n",
    "    model = \"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "\n",
    "    # From Nvidia API\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key=NVIDIA_API_KEY,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Testing NVIDIA NIM with nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "\n",
      "ğŸ“ Response:\n",
      "<think>\n",
      "Okay, so the user is asking about the key factors that indicate an increased risk of flooding in a river basin. Let me start by recalling what I know about flood prediction. Flooding in a river basin is usually a result of several interconnected factors. First, there's precipitation. Heavy rainfall over a short period or prolonged rain can cause rivers to overflow. But it's not just the amount of rain; the intensity and duration matter too. Then there's the terrain. If the river basin has steep slopes, water runs off quickly, which can lead to faster rising river levels. Conversely, flat areas might allow water to spread out more, but if the soil is saturated or impermeable, like in urban areas with lots of concrete, runoff increases.\n",
      "\n",
      "Soil type is another factor. Sandy soils drain quickly, while clay soils hold water longer, which might reduce runoff but could lead to saturation over time. Vegetation cover also plays a role. Areas with dense vegetation can absorb more water and reduce runoff because roots help water infiltrate the soil. Deforestation or urbanization would reduce this effect, increasing surface runoff.\n",
      "\n",
      "Land use and human activities are important too. Urban areas with impervious surfaces like roads and buildings increase runoff because water can't soak into the ground. Agricultural practices might either help or hinder; for example, terracing can reduce erosion but might not affect runoff much. Dams and reservoirs can mitigate flooding by holding back water, but if they fail or are poorly managed, they can cause catastrophic floods.\n",
      "\n",
      "Climate change is a broader factor. It can lead to more extreme weather events, including heavier rainfall and more frequent storms, which contribute to higher flood risks. Sea level rise can also exacerbate flooding in coastal river basins by pushing water upstream.\n",
      "\n",
      "River basin characteristics themselves, like the size, shape, and how interconnected the waterways are. A basin with a lot of tributaries might respond faster to rainfall, leading to quicker flood peaks. The river's channel capacityâ€”whether it's narrow, meandering, or has obstructions like debris or sediment buildupâ€”can affect how much water it can hold.\n",
      "\n",
      "Previous soil moisture conditions are crucial. If the ground is already saturated from prior rain, even a moderate downpour can cause flooding because the soil can't absorb more water. Similarly, snowmelt can contribute to flooding, especially in spring when temperatures rise, adding meltwater to the river system.\n",
      "\n",
      "Human infrastructure like levees, floodwalls, and\n",
      "\n",
      "ğŸ“Š Tokens used: 540\n"
     ]
    }
   ],
   "source": [
    "print(f\"ğŸ¤– Testing NVIDIA NIM with {model}\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert flood prediction assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the key factors that indicate an increased risk of flooding in a river basin?\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nğŸ“Š Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Response Example\n",
    "\n",
    "NVIDIA NIM supports streaming for real-time responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒŠ Streaming response about flood prediction...\n",
      "\n",
      "<think>\n",
      "Okay, so I need to explain how AI can improve flood forecasting accuracy. Let me start by recalling what I know about flood forecasting and AI. Flood forecasting involves predicting the occurrence and extent of floods, right? Traditional methods probably use things like historical data, weather forecasts, and physical models of river systems.\n",
      "\n",
      "Now, AI, which includes machine learning and deep learning, can process large amounts of data quickly. Maybe AI can analyze more variables than traditional models. For example, instead of just looking at rainfall and river levels, AI might incorporate data from satellites, soil moisture sensors, or even social media reports. That could provide a more comprehensive view.\n",
      "\n",
      "Another thought: AI can handle non-linear relationships. Flood systems are complex with many interacting factors. Traditional models might simplify these interactions, but AI models like neural networks can capture complex patterns. For instance, the relationship between rainfall intensity, topography, and urban infrastructure might not be straightforward, but a neural network could learn that from data.\n",
      "\n",
      "Real-time data processing is another area. AI can integrate real-time data from various sourcesâ€”like IoT sensors in rivers, weather radars, and stream gaugesâ€”to update forecasts more frequently. This could lead to earlier warnings. Also, AI might be better at handling missing data or noisy data, which is common in environmental monitoring.\n",
      "\n",
      "What about ensemble methods? AI can run multiple models and combine their results, which might improve accuracy by accounting for uncertainties. Traditional models might have fixed parameters, but AI can adjust\n",
      "\n",
      "âœ… Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸŒŠ Streaming response about flood prediction...\\n\")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise flood prediction expert.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain how AI can improve flood forecasting accuracy.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nâœ… Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "Let's compare responses from different NVIDIA models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Comparing model responses for:\n",
      "'Given streamflow of 2500 CFS and rising 200 CFS/hour, should we issue a flood alert?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– Model: llama-3.3-nemotron-super-49b-v1.5\n",
      "\n",
      "<think>\n",
      "Okay, so the user is asking if they should issue a flood alert given a streamflow of 2500 CFS that's rising at 200 CFS per hour. Let me think through this step by step.\n",
      "\n",
      "First, I need to recall what CFS means. CFS stands for cubic feet per second, which is a measure of the volume of water flowing in a stream or river. So 2500 CFS is the current flow rate, and it's increasing by 200 CFS every hour. That's a pretty rapid rise. \n",
      "\n",
      "Now, the key here is to determine if these numbers meet the criteria for issuing a flood alert. Flood alerts are typically based on specific thresholds set by agencies like the National Weather Service (NWS) in the US. These thresholds can vary depending on the location, the river's history, and the potential impact on the surrounding area. \n",
      "\n",
      "But since the user hasn't provided specific thresholds, I need to consider general guidelines.\n",
      "\n",
      "ğŸ“ˆ Tokens: 250\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ¤– Model: llama-3.1-70b-instruct\n",
      "\n",
      "Streamflow is already high (2500 CFS) and rising rapidly (200 CFS/hour). I recommend issuing a flood alert immediately. The rapid increase in streamflow indicates a high risk of flooding, and prompt action is necessary to protect people and property.\n",
      "\n",
      "ğŸ“ˆ Tokens: 103\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if USE_NVIDIA_API:\n",
    "    models_to_test = [\n",
    "        \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "        \"meta/llama-3.1-70b-instruct\",\n",
    "    ]\n",
    "else:\n",
    "    models_to_test = [model]\n",
    "\n",
    "question = \"Given streamflow of 2500 CFS and rising 200 CFS/hour, should we issue a flood alert?\"\n",
    "\n",
    "print(f\"ğŸ“Š Comparing model responses for:\\n'{question}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\nğŸ¤– Model: {model_name.split('/')[-1]}\\n\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a flood emergency expert. Be concise.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "\n",
    "        print(response.choices[0].message.content)\n",
    "        print(f\"\\nğŸ“ˆ Tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-Judge Evaluation\n",
    "\n",
    "The system includes an automatic evaluation feature using **cross-provider LLM-as-Judge**:\n",
    "- When NVIDIA models generate responses, h2oGPTe judges them\n",
    "- When h2oGPTe generates responses, NVIDIA models judge them\n",
    "- This provides unbiased evaluation of response quality\n",
    "\n",
    "Let's evaluate the NVIDIA model responses using our evaluation API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Evaluating NVIDIA model response using LLM-as-Judge...\n",
      "\n",
      "âœ… Evaluation Complete!\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š Evaluation Metrics:\n",
      "\n",
      "   ğŸ¯ Overall Score:    5.0/10\n",
      "   ğŸ¤ Helpfulness:      5.0/10\n",
      "   âœ… Accuracy:         5.0/10\n",
      "   ğŸ¯ Relevance:        5.0/10\n",
      "   ğŸ“ Coherence:        5.0/10\n",
      "   ğŸ›¡ï¸  Safety:           8.0/10\n",
      "   ğŸ’ª Confidence:       0.0%\n",
      "\n",
      "ğŸ’­ Judge's Reasoning:\n",
      "   Evaluation failed: UnauthorizedError: Unauthorized\n",
      "\n",
      "\n",
      "â±ï¸  Evaluation Duration: 457ms\n",
      "ğŸ†” Evaluation ID: 72fa457f-dc9e-4322-bc4a-b1860c6a366f\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate one of the model responses from above\n",
    "question = \"Given streamflow of 2500 CFS and rising 200 CFS/hour, should we issue a flood alert?\"\n",
    "\n",
    "# Response from meta/llama-3.1-70b-instruct (from cell 12)\n",
    "response_text = \"\"\"Streamflow is already high (2500 CFS) and rising rapidly (200 CFS/hour). I recommend issuing a flood alert immediately. The rapid increase in streamflow indicates a high risk of flooding, and prompt action is necessary to protect people and property.\"\"\"\n",
    "\n",
    "print(\"ğŸ” Evaluating NVIDIA model response using LLM-as-Judge...\\n\")\n",
    "\n",
    "# Call the evaluation API\n",
    "eval_payload = {\n",
    "    \"question\": question,\n",
    "    \"response\": response_text,\n",
    "    \"model\": \"meta/llama-3.1-70b-instruct\",\n",
    "    \"agent_used\": False,\n",
    "    \"response_provider\": \"nvidia\",  # This will trigger h2oGPTe as the judge\n",
    "}\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/evaluation/evaluate\", json=eval_payload, headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    eval_result = response.json()\n",
    "\n",
    "    print(\"âœ… Evaluation Complete!\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ“Š Evaluation Metrics:\\n\")\n",
    "\n",
    "    metrics = eval_result.get(\"metrics\", {})\n",
    "    print(f\"   ğŸ¯ Overall Score:    {metrics.get('overall', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ¤ Helpfulness:      {metrics.get('helpfulness', 0):.1f}/10\")\n",
    "    print(f\"   âœ… Accuracy:         {metrics.get('accuracy', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ¯ Relevance:        {metrics.get('relevance', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ“ Coherence:        {metrics.get('coherence', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ›¡ï¸  Safety:           {metrics.get('safety', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ’ª Confidence:       {metrics.get('confidence', 0):.1%}\")\n",
    "\n",
    "    print(\"\\nğŸ’­ Judge's Reasoning:\")\n",
    "    print(f\"   {eval_result.get('reasoning', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\nâ±ï¸  Evaluation Duration: {eval_result.get('duration_ms', 0)}ms\")\n",
    "    print(f\"ğŸ†” Evaluation ID: {eval_result.get('evaluation_id', 'N/A')}\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: h2oGPTe Agent (A2A) Integration\n",
    "\n",
    "## ğŸ§  h2oGPTe - Enterprise AI with Agent Mode\n",
    "\n",
    "h2oGPTe provides advanced AutoML capabilities through its agent mode, enabling:\n",
    "\n",
    "- **Driverless AI Integration**: Automated machine learning with minimal code\n",
    "- **Agent-to-Agent (A2A)**: AI agents that can invoke other AI agents\n",
    "- **Feature Engineering**: Automatic feature creation for time-series data\n",
    "- **Model Interpretability**: Explainable AI for emergency response decisions\n",
    "\n",
    "### Setting up h2oGPTe Client\n",
    "\n",
    "**Note**: This section requires h2oGPTe credentials. If you don't have access, you can skip to the next section.\n",
    "\n",
    "Get your credentials at: [H2O.ai Enterprise](https://h2o.ai/platform/enterprise-h2ogpte/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h2oGPTe for Flood Prediction ML\n",
    "\n",
    "Let's use h2oGPTe's agent mode to get guidance on training a flood prediction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Consulting h2oGPTe agent for AutoML guidance...\n",
      "\n",
      "ğŸ“¡ Provider: h2ogpte\n",
      "ğŸ¤– Model: unknown\n",
      "\n",
      "ğŸ“ Response:\n",
      "\n",
      "\n",
      "\n",
      "âœ… Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "if H2OGPTE_AVAILABLE:\n",
    "    headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "    # Using FastAPI streaming endpoint for h2oGPTe\n",
    "    url = f\"{api_server_base_url}/api/ai/chat/enhanced/stream\"\n",
    "\n",
    "    payload = {\n",
    "        \"message\": \"\"\"I have flood prediction data with these features:\n",
    "        - streamflow_cfs: Current river flow rate\n",
    "        - rainfall_24h: Rainfall in last 24 hours\n",
    "        - river_stage_ft: Water level\n",
    "        - soil_moisture: Ground saturation\n",
    "        - elevation_ft: Location elevation\n",
    "\n",
    "        How should I approach building an ML model to predict flood risk in the next 24 hours?\n",
    "        What feature engineering would you recommend?\"\"\",\n",
    "        \"provider\": \"h2ogpte\",\n",
    "        \"use_agent\": True,\n",
    "        \"max_tokens\": 8192 * 10,\n",
    "    }\n",
    "\n",
    "    print(\"ğŸ§  Consulting h2oGPTe agent for AutoML guidance...\\n\")\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        last_content = \"\"\n",
    "        # Stream the response\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode(\"utf-8\")\n",
    "                if line_str.startswith(\"data: \"):\n",
    "                    data_str = line_str[6:]  # Remove 'data: ' prefix\n",
    "                    try:\n",
    "                        data = json.loads(data_str)\n",
    "\n",
    "                        # First message contains provider info\n",
    "                        if \"provider\" in data:\n",
    "                            print(f\"ğŸ“¡ Provider: {data.get('provider')}\")\n",
    "                            print(f\"ğŸ¤– Model: {data.get('model')}\\n\")\n",
    "                            print(\"ğŸ“ Response:\\n\")\n",
    "\n",
    "                        # h2oGPTe sends incremental chunks with full content\n",
    "                        elif \"chunk\" in data and not data.get(\"done\", False):\n",
    "                            new_content = data[\"chunk\"]\n",
    "                            # Only print the new portion\n",
    "                            if new_content.startswith(last_content):\n",
    "                                new_part = new_content[len(last_content) :]\n",
    "                                print(new_part, end=\"\", flush=True)\n",
    "                                last_content = new_content\n",
    "\n",
    "                        # Check for completion\n",
    "                        elif data.get(\"done\", False):\n",
    "                            break\n",
    "\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "\n",
    "        print(\"\\n\\nâœ… Streaming complete!\")\n",
    "    else:\n",
    "        print(f\"âŒ Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping h2oGPTe demo (credentials not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Multi-Agent System with FastMCP\n",
    "\n",
    "## ğŸ¤ FastMCP - Model Context Protocol Server\n",
    "\n",
    "Our flood prediction system uses FastMCP to expose 20+ specialized tools across 5 intelligent agents:\n",
    "\n",
    "### The 5 Agents\n",
    "\n",
    "1. **Data Collector Agent** ğŸ“Š\n",
    "   - Collects USGS water data\n",
    "   - Retrieves NOAA flood forecasts\n",
    "   - Gathers weather information\n",
    "   - Monitors data quality\n",
    "\n",
    "2. **Risk Analyzer Agent** âš ï¸\n",
    "   - Calculates flood risk scores\n",
    "   - Analyzes trends and patterns\n",
    "   - Identifies high-risk areas\n",
    "\n",
    "3. **Emergency Responder Agent** ğŸš¨\n",
    "   - Assesses emergency readiness\n",
    "   - Activates alerts\n",
    "   - Coordinates evacuations\n",
    "\n",
    "4. **AI Predictor Agent** ğŸ”®\n",
    "   - Generates flood forecasts\n",
    "   - Predicts critical conditions\n",
    "   - Analyzes prediction accuracy\n",
    "\n",
    "5. **H2OGPTE ML Agent** ğŸ§ \n",
    "   - Trains ML models\n",
    "   - Optimizes features\n",
    "   - Analyzes model performance\n",
    "\n",
    "### Let's explore the available tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Available Agents and Their Status\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸŸ¢ Data Collector\n",
      "   Description: Continuously pulls real-time flood data from USGS, NOAA, and weather APIs\n",
      "   Status: Running\n",
      "   Last Check: 2025-10-21T21:53:46.823795+00:00\n",
      "   Check Interval: 300 seconds\n",
      "   Insights: 4\n",
      "\n",
      "ğŸŸ¢ Risk Analyzer\n",
      "   Description: AI-powered analysis of flood risk conditions and trend detection\n",
      "   Status: Running\n",
      "   Last Check: 2025-10-21T21:53:46.824597+00:00\n",
      "   Check Interval: 600 seconds\n",
      "   Insights: 4\n",
      "   Active Alerts: 1\n",
      "\n",
      "ğŸŸ¢ Emergency Responder\n",
      "   Description: Coordinates emergency response activities and manages critical alerts\n",
      "   Status: Running\n",
      "   Last Check: 2025-10-21T21:53:46.824826+00:00\n",
      "   Check Interval: 180 seconds\n",
      "   Insights: 5\n",
      "   Active Alerts: 2\n",
      "\n",
      "ğŸŸ¢ AI Predictor\n",
      "   Description: Advanced AI forecasting and predictive analysis for flood conditions\n",
      "   Status: Running\n",
      "   Last Check: 2025-10-21T21:53:46.825397+00:00\n",
      "   Check Interval: 900 seconds\n",
      "   Insights: 4\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get list of all agents and their capabilities\n",
    "response = requests.get(f\"{api_server_base_url}/api/agents\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    print(\"ğŸ¤– Available Agents and Their Status\\n\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # The API returns a nested structure with agents dictionary\n",
    "    agents_dict = data.get(\"agents\", {})\n",
    "\n",
    "    for agent_key, agent_data in agents_dict.items():\n",
    "        status = \"ğŸŸ¢\" if agent_data.get(\"is_running\") else \"ğŸ”´\"\n",
    "        print(f\"\\n{status} {agent_data.get('name', agent_key)}\")\n",
    "        print(f\"   Description: {agent_data.get('description', 'N/A')}\")\n",
    "        print(f\"   Status: {'Running' if agent_data.get('is_running') else 'Stopped'}\")\n",
    "        if agent_data.get(\"last_check\"):\n",
    "            print(f\"   Last Check: {agent_data.get('last_check')}\")\n",
    "        if agent_data.get(\"check_interval\"):\n",
    "            print(f\"   Check Interval: {agent_data.get('check_interval')} seconds\")\n",
    "        if agent_data.get(\"insights_count\"):\n",
    "            print(f\"   Insights: {agent_data.get('insights_count')}\")\n",
    "        if agent_data.get(\"active_alerts_count\"):\n",
    "            print(f\"   Active Alerts: {agent_data.get('active_alerts_count')}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(f\"âŒ Error fetching agents: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Agent Insights\n",
    "\n",
    "Agents continuously monitor flood conditions and generate insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Refreshing USGS data first...\n",
      "\n",
      "âš ï¸  Data refresh returned status 500\n",
      "   Proceeding with existing data...\n",
      "\n",
      "ğŸ’¡ Agent Insights\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– Data Collector\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”µ ğŸ”„ Data Freshness: 100% current\n",
      "\n",
      "ğŸ”µ ğŸŒ API Connectivity: 5/5 active\n",
      "\n",
      "ğŸ”µ ğŸ“Š Data Quality: 9.3/10\n",
      "   Change: +0.0\n",
      "\n",
      "ğŸ”µ âš¡ Update Frequency: 12 updates/hour\n",
      "\n",
      "ğŸ¤– Risk Analyzer\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”µ ğŸ¯ Overall Risk Level: MODERATE (5.0/10)\n",
      "   Change: +0.0\n",
      "\n",
      "ğŸ”µ ğŸš¨ Critical Watersheds: 1 areas\n",
      "\n",
      "ğŸ”µ ğŸ“ˆ Risk Trend Analysis: Stable\n",
      "   Change: 0.0% per hour\n",
      "\n",
      "ğŸ”µ ğŸ§  AI Confidence: 0%\n",
      "   Change: Low\n",
      "\n",
      "ğŸ¤– Emergency Responder\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”µ ğŸš¨ Active Incidents: 0 ongoing\n",
      "\n",
      "ğŸ”µ ğŸš Response Readiness: GOOD (85%)\n",
      "   Change: 8 teams ready\n",
      "\n",
      "ğŸ”µ ğŸƒ Evacuation Status: No active evacuations\n",
      "   Change: 0 zones active\n",
      "\n",
      "ğŸ”µ ğŸ“¡ Communication Systems: 100% operational\n",
      "   Change: 6 channels active\n",
      "\n",
      "ğŸ”µ ğŸ“¢ Alert Distribution: 0 alerts sent\n",
      "   Change: 0 today\n",
      "\n",
      "ğŸ¤– Predictor\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”µ ğŸ¯ Model Accuracy: 85.0%\n",
      "   Change: +0.0% vs yesterday\n",
      "\n",
      "ğŸ”µ ğŸ§  Prediction Confidence: 72%\n",
      "   Change: High\n",
      "\n",
      "================================================================================\n",
      "Generated at: 2025-10-21T21:56:53.160825+00:00\n"
     ]
    }
   ],
   "source": [
    "# Refresh USGS data before getting insights\n",
    "print(\"ğŸ”„ Refreshing USGS data first...\\n\")\n",
    "\n",
    "# Note: Using local-token for development mode (server.py allows this when OIDC is disabled)\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "refresh_response = requests.post(\n",
    "    f\"{api_server_base_url}/api/dashboard/refresh-usgs-data\", headers=headers\n",
    ")\n",
    "\n",
    "if refresh_response.status_code == 200:\n",
    "    refresh_result = refresh_response.json()\n",
    "    print(f\"âœ… {refresh_result.get('message', 'Data refresh initiated')}\")\n",
    "\n",
    "    # Wait a moment for background job to start\n",
    "    print(\"â³ Waiting for data refresh to process...\\n\")\n",
    "    time.sleep(3)\n",
    "else:\n",
    "    print(f\"âš ï¸  Data refresh returned status {refresh_response.status_code}\")\n",
    "    print(\"   Proceeding with existing data...\\n\")\n",
    "\n",
    "# Get insights from all agents\n",
    "response = requests.get(f\"{api_server_base_url}/api/agents/insights\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    print(\"ğŸ’¡ Agent Insights\\n\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # The API returns insights grouped by agent\n",
    "    insights_by_agent = data.get(\"insights\", {})\n",
    "\n",
    "    count = 0\n",
    "    for agent_name, agent_insights in insights_by_agent.items():\n",
    "        print(f\"\\nğŸ¤– {agent_name.replace('_', ' ').title()}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for insight in agent_insights:\n",
    "            title = insight.get(\"title\", \"N/A\")\n",
    "            value = insight.get(\"value\", \"N/A\")\n",
    "            change = insight.get(\"change\")\n",
    "            urgency = insight.get(\"urgency\", \"normal\")\n",
    "            timestamp = insight.get(\"timestamp\", \"\")\n",
    "\n",
    "            urgency_icon = {\n",
    "                \"critical\": \"ğŸ”´\",\n",
    "                \"high\": \"ğŸŸ¡\",\n",
    "                \"normal\": \"ğŸ”µ\",\n",
    "                \"low\": \"ğŸŸ¢\",\n",
    "            }.get(urgency, \"âšª\")\n",
    "\n",
    "            print(f\"\\n{urgency_icon} {title}: {value}\")\n",
    "            if change:\n",
    "                print(f\"   Change: {change}\")\n",
    "\n",
    "            count += 1\n",
    "            if count >= 15:  # Limit total insights shown\n",
    "                break\n",
    "\n",
    "        if count >= 15:\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Generated at: {data.get('generated_at', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"âŒ Error fetching insights: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Risk Analyzer Agent\n",
    "\n",
    "Calculates comprehensive flood risk scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Risk Analyzer Agent...\n",
      "\n",
      "ğŸš€ Starting risk_analyzer agent...\n",
      "\n",
      "[WARNING] Using provided input_schema for multi-argument function\n",
      "[WARNING] Using provided input_schema for multi-argument function\n",
      "[WARNING] Using provided input_schema for multi-argument function\n",
      "ğŸ’¬ \n",
      "------------------------------\n",
      "[AGENT]\n",
      "\u001b[33mAgent input: Analyze current flood risk for the Texas:\n",
      "    1. Calculate detailed risk scores for all factors\n",
      "    2. Identify the highest risk components\n",
      "    3. Provide trend analysis\n",
      "    4. Give recommendations for monitoring\n",
      "\n",
      "    Be specific about the risk levels and factors.\n",
      "\u001b[36mAgent's thoughts: \n",
      "\n",
      "\n",
      "Question: Analyze current flood risk for the Texas:\n",
      "    1. Calculate detailed risk scores for all factors\n",
      "    2. Identify the highest risk components\n",
      "    3. Provide trend analysis\n",
      "    4. Give recommendations for monitoring\n",
      "\n",
      "    Be specific about the risk levels and factors.\n",
      "Thought: I need to use the comprehensive_flood_analysis_tool to get a detailed analysis for Texas, including risk scores, components, trends, and recommendations.\n",
      "Action: comprehensive_flood_analysis_tool\n",
      "Action Input: {\"location\": \"Texas\"}\n",
      "\u001b[39m\n",
      "------------------------------\n",
      "ğŸ’¬ \n",
      "------------------------------\n",
      "[AGENT]\n",
      "\u001b[37mCalling tools: comprehensive_flood_analysis_tool\n",
      "\u001b[33mTool's input: {'location': 'Texas'}\n",
      "\u001b[36mTool's response: \n",
      "{\"status\":\"success\",\"location\":\"Texas\",\"analysis_timestamp\":\"2025-10-21T21:57:20.853700+00:00\",\"data_collection\":{\"agent\":\"Data Collector\",\"data_sources\":{\"usgs_sites\":10,\"noaa_alerts\":0,\"weather_locations\":7},\"insights\":[{\"title\":\"ğŸ”„ Data Freshness\",\"value\":\"100% current\",\"trend\":\"up\",\"urgency\":\"normal\"},{\"title\":\"ğŸŒ API Connectivity\",\"value\":\"5/5 active\",\"trend\":\"up\",\"urgency\":\"normal\"},{\"title\":\"ğŸ“Š Data Quality\",\"value\":\"9.3/10\",\"trend\":\"up\",\"urgency\":\"normal\"},{\"title\":\"âš¡ Update Frequency\",\"value\":\"12 updates/hour\",\"trend\":\"stable\",\"urgency\":\"normal\"}],\"alerts\":[]},\"risk_analysis\":{\"agent\":\"Risk Analyzer\",\"insights\":[{\"title\":\"ğŸ¯ Overall Risk Level\",\"value\":\"Unknown (0.0/10)\",\"trend\":\"stable\",\"urgency\":\"normal\"},{\"title\":\"ğŸš¨ Critical Watersheds\",\"value\":\"0 areas\",\"trend\":\"stable\",\"urgency\":\"normal\"},{\"title\":\"ğŸ“ˆ Risk Trend Analysis\",\"value\":\"Insufficient data\",\"trend\":\"stable\",\"urgency\":\"normal\"},{\"title\":\"ğŸ§  AI Confidence\",\"value\":\"0%\",\"trend\":\"stable\",\"urgency\":\"normal\"}],\"alerts\":[]},\"...(rest of response truncated)\u001b[39m\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     25\u001b[39m     final_output = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mline_str\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github.com/h2oai/h2oai-flood-prediction-agent/.venv/lib/python3.12/site-packages/requests/models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github.com/h2oai/h2oai-flood-prediction-agent/.venv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github.com/h2oai/h2oai-flood-prediction-agent/.venv/lib/python3.12/site-packages/urllib3/response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github.com/h2oai/h2oai-flood-prediction-agent/.venv/lib/python3.12/site-packages/urllib3/response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github.com/h2oai/h2oai-flood-prediction-agent/.venv/lib/python3.12/site-packages/urllib3/response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run Risk Analyzer Agent via NAT\n",
    "payload = {\n",
    "    \"agent\": \"risk_analyzer\",\n",
    "    \"message\": \"\"\"Analyze current flood risk for the Texas:\n",
    "    1. Calculate detailed risk scores for all factors\n",
    "    2. Identify the highest risk components\n",
    "    3. Provide trend analysis\n",
    "    4. Give recommendations for monitoring\n",
    "\n",
    "    Be specific about the risk levels and factors.\"\"\",\n",
    "}\n",
    "\n",
    "print(\"Running Risk Analyzer Agent...\\n\")\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/nat/chat/stream\",\n",
    "    json=payload,\n",
    "    headers=headers,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    final_output = None\n",
    "\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode(\"utf-8\")\n",
    "            if line_str.startswith(\"data: \"):\n",
    "                data_str = line_str[6:]  # Remove 'data: ' prefix\n",
    "                try:\n",
    "                    data = json.loads(data_str)\n",
    "\n",
    "                    # Handle different event types\n",
    "                    event_type = data.get(\"type\")\n",
    "\n",
    "                    if event_type == \"start\":\n",
    "                        print(f\"ğŸš€ Starting {data.get('agent_type')} agent...\")\n",
    "                        print()\n",
    "\n",
    "                    elif event_type == \"log\":\n",
    "                        log_entry = data.get(\"log\", {})\n",
    "                        level = log_entry.get(\"level\", \"INFO\")\n",
    "                        message = log_entry.get(\"message\", \"\")\n",
    "\n",
    "                        # Show important logs\n",
    "                        if level in [\"WARNING\", \"ERROR\"]:\n",
    "                            print(f\"[{level}] {message}\")\n",
    "                        elif (\n",
    "                            \"Agent\" in message\n",
    "                            or \"Final Answer\" in message\n",
    "                            or \"Tool\" in message\n",
    "                        ):\n",
    "                            print(f\"ğŸ’¬ {message}\")\n",
    "\n",
    "                    elif event_type == \"result\":\n",
    "                        final_output = data.get(\"output\")\n",
    "                        print(\"\\n\" + \"=\" * 80)\n",
    "                        print(\"âœ… Data Collection Complete!\\n\")\n",
    "\n",
    "                    elif event_type == \"error\":\n",
    "                        print(f\"\\nâŒ Error: {data.get('error')}\")\n",
    "\n",
    "                    elif event_type == \"done\":\n",
    "                        break\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "\n",
    "    # Display final output\n",
    "    if final_output:\n",
    "        print(final_output)\n",
    "        print(\"=\" * 80)\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: H2OGPTE ML Agent\n",
    "\n",
    "AutoML agent for model training and optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run H2OGPTE ML Agent via NAT\n",
    "payload = {\n",
    "    \"agent\": \"h2ogpte_agent\",\n",
    "    \"message\": \"\"\"Help me design an ML pipeline for flood prediction:\n",
    "    1. What features should I engineer from raw sensor data?\n",
    "    2. What model types work best for flood forecasting?\n",
    "    3. How should I handle imbalanced flood event data?\n",
    "    4. What validation strategy is appropriate for time-series?\n",
    "\n",
    "    Provide actionable AutoML recommendations.\"\"\",\n",
    "}\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "print(\"ğŸ§  Running H2OGPTE ML Agent...\\n\")\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/nat/chat\", json=payload, headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"âœ… ML Recommendations Complete!\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result.get(\"response\", result))\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Real-World Data Integration\n",
    "\n",
    "## ğŸŒ Live Data from Government APIs\n",
    "\n",
    "Our system integrates with real-time data sources:\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "1. **USGS Water Services**\n",
    "   - Real-time streamflow (CFS)\n",
    "   - Gage height (feet)\n",
    "   - 12 monitoring stations in Texas\n",
    "   - Updated every 15 minutes\n",
    "\n",
    "2. **NOAA Weather Service**\n",
    "   - Flood warnings and watches\n",
    "   - Weather alerts\n",
    "   - Forecast data\n",
    "\n",
    "3. **Open-Meteo**\n",
    "   - Weather forecasts\n",
    "   - Flood API predictions\n",
    "   - Historical data\n",
    "\n",
    "### Let's view live watershed data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refresh USGS Data Manually\n",
    "\n",
    "Trigger a fresh data collection from USGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ Triggering USGS data refresh...\\n\")\n",
    "header = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/dashboard/refresh-usgs-data\", headers=header\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    time.sleep(5)\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "\n",
    "# Get current watershed data\n",
    "response = requests.get(f\"{api_server_base_url}/api/watersheds\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    watersheds = response.json()\n",
    "\n",
    "    # Convert to DataFrame for nice display\n",
    "    df = pd.DataFrame(watersheds)\n",
    "\n",
    "    # Select key columns\n",
    "    display_cols = [\n",
    "        \"name\",\n",
    "        \"current_streamflow_cfs\",\n",
    "        \"risk_score\",\n",
    "        \"trend_rate_cfs_per_hour\",\n",
    "        \"last_updated\",\n",
    "    ]\n",
    "\n",
    "    available_cols = [col for col in display_cols if col in df.columns]\n",
    "\n",
    "    print(\"ğŸŒŠ Live Watershed Data\\n\")\n",
    "    print(\"=\" * 100)\n",
    "    print(df[available_cols].to_string(index=False))\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\nğŸ“Š Total Watersheds Monitored: {len(watersheds)}\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    if \"risk_score\" in df.columns:\n",
    "        high_risk = len(df[df[\"risk_score\"] > 7.0])\n",
    "        medium_risk = len(df[(df[\"risk_score\"] >= 4.0) & (df[\"risk_score\"] <= 7.0)])\n",
    "        low_risk = len(df[df[\"risk_score\"] < 4.0])\n",
    "\n",
    "        print(\"\\nâš ï¸  Risk Distribution:\")\n",
    "        print(f\"   ğŸ”´ High Risk (>7.0): {high_risk}\")\n",
    "        print(f\"   ğŸŸ¡ Medium Risk (4.0-7.0): {medium_risk}\")\n",
    "        print(f\"   ğŸŸ¢ Low Risk (<4.0): {low_risk}\")\n",
    "else:\n",
    "    print(f\"âŒ Error fetching watersheds: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Summary & Next Steps\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "In this notebook, you've learned how to:\n",
    "\n",
    "âœ… **Set up a multi-agent AI system** for disaster response  \n",
    "âœ… **Integrate h2oGPTe** for AutoML and model training  \n",
    "âœ… **Use NVIDIA NIM** for high-performance inference  \n",
    "âœ… **Build NAT agent workflows** with React patterns  \n",
    "âœ… **Implement FastMCP servers** with custom tools  \n",
    "âœ… **Integrate real-time data** from government APIs  \n",
    "âœ… **Coordinate multiple AI agents** for complex tasks  \n",
    "âœ… **Evaluate responses** using LLM-as-Judge  \n",
    "\n",
    "## Architecture Highlights\n",
    "\n",
    "- **5 Specialized Agents**: Data Collector, Risk Analyzer, Emergency Responder, Predictor, H2OGPTE ML\n",
    "- **20+ MCP Tools**: Via FastMCP server on port 8001\n",
    "- **NVIDIA NIM Models**: Nemotron Super 49B, Llama 3.1 variants\n",
    "- **h2oGPTe A2A**: Agent-to-agent AutoML capabilities\n",
    "- **Real-time Data**: USGS, NOAA, Weather APIs\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **NVIDIA NIM**: [build.nvidia.com](https://build.nvidia.com)\n",
    "- **h2oGPTe**: [h2o.ai/platform/enterprise-h2ogpte](https://h2o.ai/platform/enterprise-h2ogpte/)\n",
    "- **NVIDIA NAT**: [docs.nvidia.com/nat](https://docs.nvidia.com/nat)\n",
    "- **FastMCP**: [github.com/jlowin/fastmcp](https://github.com/jlowin/fastmcp)\n",
    "- **USGS Water Data**: [waterdata.usgs.gov](https://waterdata.usgs.gov)\n",
    "- **NOAA Weather**: [weather.gov](https://weather.gov)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Customize Agents**: Modify agent configs for your specific use case\n",
    "2. **Add Data Sources**: Integrate additional APIs and sensors\n",
    "3. **Train ML Models**: Use h2oGPTe to train production models\n",
    "4. **Deploy to Production**: Use Docker/Kubernetes deployment\n",
    "5. **Monitor Performance**: Add logging and metrics\n",
    "\n",
    "## Contributing\n",
    "\n",
    "This is an open-source AI for Good project. Contributions welcome!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŠ Thank you for exploring the Flood Prediction Blueprint!\n",
    "\n",
    "**Built with â¤ï¸ using h2oGPTe and NVIDIA NIM**\n",
    "\n",
    "For questions and support, please open an issue in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h2oai-flood-prediction-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
