# H2O.ai Flood Prediction - Docker Compose Environment Configuration
# ==============================================================================
# Copy this file to .env and fill in your actual values
# NEVER commit .env to version control - it contains sensitive secrets!
#
# Quick setup:
#   cp .env.example .env
#   # Edit .env with your API keys and tokens
#   docker-compose up -d

# ==============================================================================
# REQUIRED SECRETS
# ==============================================================================

# NVIDIA API Key (REQUIRED)
# Get your API key from: https://build.nvidia.com/
# This is required for NVIDIA AI endpoints and model inference
NVIDIA_API_KEY=

# H2OGPTE API Key (REQUIRED)
# Your H2O GPT Enterprise API key for LLM integration
H2OGPTE_API_KEY=

# Jupyter Notebook Token (REQUIRED)
# Generate a secure random token for notebook authentication:
#   python3 -c "import secrets; print(secrets.token_urlsafe(32))"
# Alternative: openssl rand -hex 32
# Users will need this token to access the notebook at http://localhost:8888/jupyter
JUPYTER_TOKEN=

# NGC API Key (REQUIRED for NIM LLM only)
# Get your NGC API key from: https://ngc.nvidia.com/
# Only needed if running with docker-compose.nimllm.yml
# Also use this to login: docker login nvcr.io -u '$oauthtoken' -p '<NGC_API_KEY>'
NGC_API_KEY=

# ==============================================================================
# H2OGPTE CONFIGURATION
# ==============================================================================

# H2OGPTE service URL
H2OGPTE_URL=https://h2ogpte.cloud-dev.h2o.dev

# H2OGPTE model to use
H2OGPTE_MODEL=claude-sonnet-4-20250514

# ==============================================================================
# WEB APPLICATION IMAGE CONFIGURATION
# ==============================================================================

# Web application container image settings
WEB_IMAGE_REGISTRY=h2oairelease
WEB_IMAGE_REPOSITORY=h2oai-floodprediction-app
WEB_IMAGE_TAG=v0.3.0

# Web service port mapping (external:internal)
WEB_PORT=8000

# Web image registry authentication (only needed for private registries)
# Leave empty if using a public registry or if already logged in
# Usage: Run ./docker-login.sh before docker-compose up
WEB_REGISTRY_URL=
WEB_REGISTRY_USER=
WEB_REGISTRY_PASS=

# ==============================================================================
# NOTEBOOK IMAGE CONFIGURATION
# ==============================================================================

# Jupyter notebook container image settings
NOTEBOOK_IMAGE_REGISTRY=h2oairelease
NOTEBOOK_IMAGE_REPOSITORY=h2oai-floodprediction-notebook
NOTEBOOK_IMAGE_TAG=v0.3.0

# Notebook service port mapping
NOTEBOOK_PORT=8888

# Allow users to change password in Jupyter (true/false)
NOTEBOOK_ALLOW_PASSWORD_CHANGE=False

# Notebook image registry authentication (only needed for private registries)
# Leave empty if using a public registry or if already logged in
# Leave empty if same as web registry (will reuse WEB_REGISTRY_* credentials)
NOTEBOOK_REGISTRY_URL=
NOTEBOOK_REGISTRY_USER=
NOTEBOOK_REGISTRY_PASS=

# ==============================================================================
# REDIS CONFIGURATION
# ==============================================================================

# Redis container image settings
REDIS_IMAGE_REGISTRY=docker.io
REDIS_IMAGE_REPOSITORY=redis
REDIS_IMAGE_TAG=8.2.1

# Enable Redis for task queue and caching (true/false)
REDIS_ENABLED=true

# ==============================================================================
# NVIDIA NIM LLM CONFIGURATION (Optional)
# ==============================================================================
# These settings are only used when running with docker-compose.nimllm.yml
# Usage: docker-compose -f docker-compose.yml -f docker-compose.nimllm.yml up

# NIM LLM container image
# Available models: https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/llama-3_3-nemotron-super-49b-v1_5
NIMLLM_IMAGE=nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0

# NIM LLM service port mapping (external port, internal is always 8000)
NIMLLM_PORT=8001

# Local cache directory for NIM model storage
# NOTE: Model cache requires ~50GB+ of disk space
# The directory will be created if it doesn't exist
LOCAL_NIM_CACHE=${HOME}/.cache/nim

# User/Group IDs for NIM container (defaults to current user)
# Used for proper file permissions on the cache directory
UID=1000
GID=1000

# NIM LLM Base URL (automatically set when using docker-compose.nimllm.yml)
# Leave empty when not using NIM
NIM_LLM_BASE_URL=

# ==============================================================================
# ADVANCED CONFIGURATION
# ==============================================================================

# Additional environment variables can be added here as needed
# Refer to the Helm chart values.yaml for more configuration options
