{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI-Powered Flood Intelligence System with h2oGPTe and NVIDIA NIM\n",
    "\n",
    "[![Deploy on NVIDIA](https://img.shields.io/badge/Deploy%20on-NVIDIA%20AI%20Blueprints-76B900?logo=nvidia&logoColor=white)](https://build.nvidia.com)\n",
    "[![H2O.ai](https://img.shields.io/badge/Powered%20by-H2O.ai-FFD500)](https://h2o.ai)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒŠ Overview\n",
    "\n",
    "This blueprint demonstrates an **AI-powered flood intelligence and disaster response system** that combines:\n",
    "\n",
    "- **h2oGPTe Agent-to-Agent (A2A)**: Advanced AutoML capabilities with Driverless AI for model training and feature engineering\n",
    "- **NVIDIA NIM**: State-of-the-art inference with `nvidia/llama-3.3-nemotron-super-49b-v1.5` and other NVIDIA models\n",
    "- **NVIDIA NAT Pipeline**: React Agent workflows for multi-agent orchestration\n",
    "- **FastMCP Server**: 20+ specialized tools across 5 intelligent agents\n",
    "- **Real-time Data Integration**: USGS Water Services, NOAA Forecasts, and Weather APIs\n",
    "\n",
    "### ğŸ¯ Use Case: AI for Good - Disaster Response\n",
    "\n",
    "This system provides:\n",
    "- **Real-time flood monitoring** with live data from watersheds and monitoring stations\n",
    "- **AI-powered risk assessment** using advanced machine learning models\n",
    "- **Emergency response coordination** with automated alerts and evacuation planning\n",
    "- **Predictive analytics** for flood forecasting 24-72 hours ahead\n",
    "- **AutoML model training** for continuous improvement of prediction accuracy\n",
    "\n",
    "### ğŸ—ï¸ Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      Flood Intelligence System                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚   h2oGPTe    â”‚  â”‚  NVIDIA NIM  â”‚  â”‚  FastMCP     â”‚         â”‚\n",
    "â”‚  â”‚  (A2A Mode)  â”‚  â”‚  (Nemotron)  â”‚  â”‚   Server     â”‚         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚         â”‚                 â”‚                 â”‚                   â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                  â”‚                 â”‚                            â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚         â”‚    NVIDIA NAT Agent Pipeline      â”‚                  â”‚\n",
    "â”‚         â”‚      (React Agent Workflow)       â”‚                  â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                  â”‚                                              â”‚\n",
    "â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\n",
    "â”‚    â”‚    5 Specialized Agents   â”‚                               â”‚\n",
    "â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â”‚\n",
    "â”‚    â”‚  1. Data Collector        â”‚ â—„â”€â”€ USGS Water Data           â”‚\n",
    "â”‚    â”‚  2. Risk Analyzer         â”‚ â—„â”€â”€ NOAA Flood Alerts         â”‚\n",
    "â”‚    â”‚  3. Emergency Responder   â”‚ â—„â”€â”€ Weather APIs              â”‚\n",
    "â”‚    â”‚  4. AI Predictor          â”‚                               â”‚\n",
    "â”‚    â”‚  5. H2OGPTE ML Agent      â”‚                               â”‚\n",
    "â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  Output: Real-time Monitoring, Alerts, Predictions, ML Models  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ”‘ Key Technologies\n",
    "\n",
    "1. **h2oGPTe**: Enterprise AI platform with agent mode for AutoML and advanced analytics\n",
    "2. **NVIDIA NIM**: Optimized inference microservices for AI models\n",
    "3. **NVIDIA NAT**: Agent orchestration framework with React-based workflows\n",
    "4. **FastMCP**: Model Context Protocol server for tool integration\n",
    "5. **FastAPI**: High-performance API server for real-time operations\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ What You'll Learn\n",
    "\n",
    "- Setting up multi-agent AI systems for disaster response\n",
    "- Integrating h2oGPTe for AutoML and model training\n",
    "- Using NVIDIA NIM for high-performance inference\n",
    "- Building NAT agent workflows with React patterns\n",
    "- Implementing FastMCP servers with custom tools\n",
    "- Real-time data integration from government APIs\n",
    "- Coordinating multiple AI agents for complex tasks\n",
    "\n",
    "Let's get started! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "Before running this setup, make sure you have:\n",
    "\n",
    "### Required:\n",
    "- âœ… **NVIDIA API Key** - Get it from [build.nvidia.com](https://build.nvidia.com)\n",
    "  - This key is used for NVIDIA NIM models and services\n",
    "\n",
    "### Optional (but recommended):\n",
    "- ğŸ”¹ **H2OGPTE API Key** - For AutoML and advanced AI features\n",
    "  - Get access at [h2o.ai](https://h2o.ai/platform/enterprise-h2ogpte/)\n",
    "  - If you don't have this, you can skip it - the app will still work with reduced features\n",
    "\n",
    "---\n",
    "\n",
    "Ready? Let's collect your API keys! ğŸ”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Dependencies\n",
    "\n",
    "*Please **restart** the kernel after this step. Do not repeat this step after restarting the kernel.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "python = sys.executable\n",
    "\n",
    "!{python} -m ensurepip --upgrade\n",
    "!{python} -m pip install --upgrade pip setuptools wheel\n",
    "!{python} -m pip install --upgrade --force-reinstall pandas openai requests python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” Step 1: Collect API Keys\n",
    "\n",
    "Run the cells below to securely enter your API keys. Your inputs will be hidden for security.\n",
    "\n",
    "### What you'll provide:\n",
    "1. **NVIDIA API Key** (required)\n",
    "2. **H2OGPTE API Key** (optional)\n",
    "3. **H2OGPTE URL** (optional, default provided)\n",
    "\n",
    "**Note**: Ports are pre-configured in this Launchable (8090 for Web UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API keys and URLs\n",
    "nvidia_api_key = \"\"\n",
    "ngc_api_key = \"\"\n",
    "h2ogpte_url = \"https://h2ogpte.cloud-dev.h2o.dev\"\n",
    "h2ogpte_api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect NVIDIA API Key\n",
    "print(\"ğŸ”‘ Enter your NVIDIA API Key\")\n",
    "print(\"   Get it from: https://build.nvidia.com\")\n",
    "print()\n",
    "\n",
    "nvidia_api_key = getpass.getpass(\"NVIDIA API Key: \")\n",
    "\n",
    "if not nvidia_api_key or nvidia_api_key.strip() == \"\":\n",
    "    raise ValueError(\n",
    "        \"âŒ NVIDIA API Key is required! Please run this cell again and provide the key.\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… NVIDIA API Key collected successfully!\")\n",
    "print(f\"   Preview: {nvidia_api_key[:10]}...{nvidia_api_key[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect NGC API Key (optional)\n",
    "print(\"ğŸ”‘ Enter your NGC API Key\")\n",
    "print(\"   Get it from: https://catalog.ngc.nvidia.com/\")\n",
    "print()\n",
    "\n",
    "ngc_api_key = getpass.getpass(\"NGC API Key: \")\n",
    "\n",
    "if not ngc_api_key or ngc_api_key.strip() == \"\":\n",
    "    ngc_api_key = \"\"\n",
    "    print(\"âš ï¸  No API key provided, skipping Local NIM LLM setup\")\n",
    "else:\n",
    "    print(\"âœ… NGC API Key collected successfully!\")\n",
    "    print(f\"   Preview: {ngc_api_key[:10]}...{ngc_api_key[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect H2OGPTE Credentials (Optional)\n",
    "print(\"ğŸ¤– H2OGPTE Configuration (Optional)\")\n",
    "print(\"   H2OGPTE provides advanced AutoML and AI agent capabilities\")\n",
    "print(\"   If you don't have access, just press Enter to skip\")\n",
    "print()\n",
    "\n",
    "use_h2ogpte = input(\"Do you have H2OGPTE access? (yes/no) [no]: \").strip().lower()\n",
    "\n",
    "if use_h2ogpte in [\"yes\", \"y\", \"Yes\", \"Y\", \"YES\"]:\n",
    "    h2ogpte_url = input(\"H2OGPTE URL [https://h2ogpte.cloud-dev.h2o.dev]: \").strip()\n",
    "    h2ogpte_api_key = getpass.getpass(\"H2OGPTE API Key: \")\n",
    "\n",
    "    if not h2ogpte_url:\n",
    "        h2ogpte_url = \"https://h2ogpte.cloud-dev.h2o.dev\"\n",
    "\n",
    "    if not h2ogpte_api_key or h2ogpte_api_key.strip() == \"\":\n",
    "        print(\"âš ï¸  No API key provided, skipping H2OGPTE setup\")\n",
    "        h2ogpte_api_key = \"\"\n",
    "        h2ogpte_url = \"\"\n",
    "    else:\n",
    "        print(\"âœ… H2OGPTE configured successfully!\")\n",
    "        print(f\"   URL: {h2ogpte_url}\")\n",
    "        print(f\"   API Key Preview: {h2ogpte_api_key[:10]}...{h2ogpte_api_key[-4:]}\")\n",
    "else:\n",
    "    h2ogpte_api_key = \"\"\n",
    "    h2ogpte_url = \"https://h2ogpte.cloud-dev.h2o.dev\"\n",
    "    print(\"â­ï¸  Skipping H2OGPTE setup - application will run with NVIDIA NIM only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Step 2: Generate Configuration File\n",
    "\n",
    "Now we'll create the environment configuration file with all the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env_file(\n",
    "    nvidia_api_key,\n",
    "    ngc_api_key,\n",
    "    h2ogpte_api_key,\n",
    "    h2ogpte_url,\n",
    "    output_path=\"./flood_intelligence.env\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create the environment configuration file for the Flood Intelligence application.\n",
    "\n",
    "    Parameters:\n",
    "    - nvidia_api_key: NVIDIA API key (used for both NVIDIA_API_KEY)\n",
    "    - ngc_api_key: NGC API key (used for NGC_API_KEY)\n",
    "    - h2ogpte_api_key: H2OGPTE API key (can be empty string if not used)\n",
    "    - h2ogpte_url: H2OGPTE service URL\n",
    "    - output_path: Where to save the env file\n",
    "\n",
    "    Note: Ports are pre-configured in the Launchable:\n",
    "    - WEB_PORT: 8090 (fixed)\n",
    "    \"\"\"\n",
    "\n",
    "    env_content = f\"\"\"# ==============================================================================\n",
    "# API KEYS\n",
    "# ==============================================================================\n",
    "\n",
    "NVIDIA_API_KEY={nvidia_api_key}\n",
    "NGC_API_KEY={ngc_api_key}\n",
    "H2OGPTE_API_KEY={h2ogpte_api_key}\n",
    "\n",
    "# ==============================================================================\n",
    "# H2OGPTE CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# H2OGPTE service URL\n",
    "H2OGPTE_URL={h2ogpte_url}\n",
    "\n",
    "# H2OGPTE model to use\n",
    "H2OGPTE_MODEL=claude-sonnet-4-20250514\n",
    "\n",
    "# ==============================================================================\n",
    "# WEB APPLICATION IMAGE CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "WEB_IMAGE_REGISTRY=h2oairelease\n",
    "WEB_IMAGE_REPOSITORY=h2oai-floodintelligence-app\n",
    "WEB_IMAGE_TAG=v1.2.0\n",
    "\n",
    "WEB_PORT=8090\n",
    "\n",
    "# ==============================================================================\n",
    "# REDIS CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "REDIS_IMAGE_REGISTRY=docker.io\n",
    "REDIS_IMAGE_REPOSITORY=redis\n",
    "REDIS_IMAGE_TAG=8.2.1\n",
    "\n",
    "REDIS_ENABLED=true\n",
    "\n",
    "# ==============================================================================\n",
    "# NVIDIA NIM LLM CONFIGURATION (Optional)\n",
    "# ==============================================================================\n",
    "\n",
    "NIMLLM_IMAGE=nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0\n",
    "\n",
    "NIMLLM_PORT=8989\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(env_content)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "print(\"âœ… Helper function created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the environment file\n",
    "print(\"ğŸ”¨ Generating environment configuration file...\")\n",
    "print()\n",
    "\n",
    "env_file_path = create_env_file(\n",
    "    nvidia_api_key=nvidia_api_key,\n",
    "    ngc_api_key=ngc_api_key,\n",
    "    h2ogpte_api_key=h2ogpte_api_key,\n",
    "    h2ogpte_url=h2ogpte_url,\n",
    ")\n",
    "\n",
    "print(\"âœ… Configuration file created successfully!\")\n",
    "print(f\"   Location: {os.path.abspath(env_file_path)}\")\n",
    "print()\n",
    "print(\"ğŸ“‹ Configuration Summary:\")\n",
    "print(f\"   NVIDIA API Key: {nvidia_api_key[:10]}...{nvidia_api_key[-4:]}\")\n",
    "if h2ogpte_api_key:\n",
    "    print(f\"   H2OGPTE API Key: {h2ogpte_api_key[:10]}...{h2ogpte_api_key[-4:]}\")\n",
    "    print(f\"   H2OGPTE URL: {h2ogpte_url}\")\n",
    "else:\n",
    "    print(\"   H2OGPTE: Not configured (optional)\")\n",
    "print()\n",
    "print(\"ğŸ”Œ Pre-configured Ports:\")\n",
    "print(\"   Web UI Port: 8090 (fixed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from the created file\n",
    "\n",
    "load_dotenv(\"./flood_intelligence.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ³ Step 3: Pull Docker Images\n",
    "\n",
    "Now we'll authenticate with Docker registries and pull the required images.\n",
    "\n",
    "This may take 5-10 minutes depending on your connection speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Web application image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull h2oairelease/h2oai-floodintelligence-app:v1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### OPTIONAL - Locally deployed NIM Microservices. \n",
    "\n",
    "\n",
    "\n",
    "<p style=\"background-color: #bdf55d;\">\n",
    "By default, this notebook will use hosted endpoints for the NVIDIA LLMs. If Nvidia H200 GPU or equivalent is available, you may host a local NIM LLM. \n",
    "Otherwise, skip the next few cells\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if Nvidia Drivers are installed and GPU is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate Docker with NGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull Nvidia NIM LLM Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "\n",
    "# The image is defined in the env file as NIMLLM_IMAGE\n",
    "nim_llm_image = os.getenv(\n",
    "    \"NIMLLM_IMAGE\", \"nvcr.io/nim/nvidia/llama-3_3-nemotron-super-49b-v1_5:1.12.0\"\n",
    ")\n",
    "nim_llm_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "\n",
    "# pull image\n",
    "!docker pull \"${NIMLLM_IMAGE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "\n",
    "# check available images\n",
    "!docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the application using Docker Compose\n",
    "\n",
    "Run only one of the `docker compose` commands below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy with a local NVIDIA NIM LLM - **Only if NVIDIA GPU is available and NIM LLM Image was successfully pulled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Skip if not using locally hosted NIM Microservices.\n",
    "\n",
    "!docker compose -f ./deployment/nvidia-launchable/docker-compose.yml -f ./deployment/nvidia-launchable/docker-compose.nimllm.yml --env-file ./flood_intelligence.env up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If not using local NVIDIA NIM LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f ./deployment/nvidia-launchable/docker-compose.yml --env-file ./flood_intelligence.env up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for all containers to be healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the deployed containers. Wait till the STATUS is healthy\n",
    "\n",
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_key(value: str) -> str:\n",
    "    if not value:\n",
    "        return \"Not set\"\n",
    "    if len(value) <= 14:\n",
    "        return f\"{value[:4]}{'.' * len(value) - 4}\"\n",
    "    dots = max(1, len(value) - 14)\n",
    "    return f\"{value[:10]}{'.' * dots}{value[-4:]}\"\n",
    "\n",
    "\n",
    "def check_service(base_url, path, name, headers=None):\n",
    "    url = f\"{base_url}/{path.lstrip('/')}\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "    except requests.exceptions.RequestException as exc:\n",
    "        print(f\"âŒ {name} is not responding: {exc}\")\n",
    "        return False\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f\"âœ… {name} is running\")\n",
    "        return True\n",
    "    if response.status_code == 401:\n",
    "        print(f\"âš ï¸  {name} requires authentication (401)\")\n",
    "        return True\n",
    "\n",
    "    print(f\"âš ï¸  {name} responded with status {response.status_code}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NVIDIA_API_KEY = os.getenv(\"NVIDIA_API_KEY\")\n",
    "if NVIDIA_API_KEY:\n",
    "    print(f\"âœ… NVIDIA_API_KEY: {preview_key(NVIDIA_API_KEY)}\")\n",
    "else:\n",
    "    print(\"â— NVIDIA_API_KEY NOT SET\")\n",
    "\n",
    "H2OGPTE_URL = os.getenv(\"H2OGPTE_URL\")\n",
    "if H2OGPTE_URL:\n",
    "    print(f\"âœ… H2OGPTE_URL: {H2OGPTE_URL}\")\n",
    "else:\n",
    "    print(\"âš ï¸ H2OGPTE_URL NOT SET\")\n",
    "\n",
    "H2OGPTE_API_KEY = os.getenv(\"H2OGPTE_API_KEY\")\n",
    "if H2OGPTE_API_KEY:\n",
    "    print(f\"âœ… H2OGPTE_API_KEY: {preview_key(H2OGPTE_API_KEY)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ H2OGPTE_API_KEY NOT SET\")\n",
    "\n",
    "if H2OGPTE_URL and H2OGPTE_API_KEY:\n",
    "    print(\"âœ… h2oGPTe credentials found\")\n",
    "    H2OGPTE_AVAILABLE = True\n",
    "else:\n",
    "    print(\"âš ï¸  h2oGPTe credentials not set\")\n",
    "    print(\"This section will be skipped. To enable:\")\n",
    "    print(\"  export H2OGPTE_URL='<your-url>'\")\n",
    "    print(\"  export H2OGPTE_API_KEY='<your-key>'\")\n",
    "    H2OGPTE_AVAILABLE = False\n",
    "\n",
    "api_port = os.getenv(\"WEB_PORT\")\n",
    "if not api_port:\n",
    "    print(\"âš ï¸ WEB_PORT not set. API interactions will be skipped.\")\n",
    "\n",
    "api_server_base_url = os.getenv(\"API_SERVER_URL\", f\"http://localhost:{api_port}\")\n",
    "if api_port and api_server_base_url:\n",
    "    print(f\"âœ… API_SERVER_URL detected: {api_server_base_url}\")\n",
    "else:\n",
    "    print(\"âš ï¸ API_SERVER_URL not set. API interactions will be skipped.\")\n",
    "\n",
    "# Set this to False Here.\n",
    "# Later, we will verify that a Local NIM LLM is deployed and is reachable. If yes, we will set this to True\n",
    "LOCAL_NIM_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not api_server_base_url:\n",
    "    print(\"âš ï¸  API_SERVER_URL not set. Export API_SERVER_URL before continuing.\")\n",
    "else:\n",
    "    base_url = api_server_base_url.rstrip(\"/\")\n",
    "    print(\"ğŸ“ Service Endpoints:\")\n",
    "    print(f\"   - FastAPI Server: {base_url}\")\n",
    "    print(f\"   - API Docs: {base_url}/docs\")\n",
    "    print(f\"   - Agents API: {base_url}/api/agents\")\n",
    "    print(\"\\nUse the next cell to verify service health.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify Services\n",
    "\n",
    "Let's check that all services are running correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” Checking service health...\\n\")\n",
    "\n",
    "if not api_server_base_url:\n",
    "    print(\"âš ï¸  API_SERVER_URL not set. Skipping health checks.\")\n",
    "    fastapi_ok = False\n",
    "else:\n",
    "    base_url = api_server_base_url.rstrip(\"/\")\n",
    "\n",
    "    fastapi_ok = check_service(\n",
    "        base_url,\n",
    "        \"api/dashboard\",\n",
    "        \"FastAPI Dashboard\",\n",
    "    )\n",
    "    agents_ok = check_service(\n",
    "        base_url,\n",
    "        \"api/agents\",\n",
    "        \"Agents API\",\n",
    "        headers={\"Authorization\": \"Bearer local-token\"},\n",
    "    )\n",
    "    watersheds_ok = check_service(\n",
    "        base_url,\n",
    "        \"api/watersheds\",\n",
    "        \"Watersheds API\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    if fastapi_ok and agents_ok and watersheds_ok:\n",
    "        print(\"âœ… All services are responding!\")\n",
    "    else:\n",
    "        print(\"âš ï¸  One or more endpoints did not respond as expected.\")\n",
    "        print(\"Review Helm deployment status or API logs if needed.\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that a Local NIM LLM is deployed and is reachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base url for NIM\n",
    "nimllm_port = os.getenv(\"NIMLLM_PORT\", \"8989\")\n",
    "nim_llm_base_url = f\"http://localhost:{nimllm_port}/v1\"\n",
    "print(f\"If Local NIM LLM is running, it will be available at: {nim_llm_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the NIM container is up. This will take 15 to 30 mins. Repeat this step until you see a positive message.\n",
    "if nim_llm_base_url:\n",
    "    try:\n",
    "        response = requests.get(f\"{nim_llm_base_url}/models\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"âœ… Local NIM LLM is running\")\n",
    "            LOCAL_NIM_AVAILABLE = True\n",
    "        else:\n",
    "            print(\"âš ï¸ No locally deployed NIM LLM is found. Will be using NIMs via API.\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ No locally deployed NIM LLM is found. Will be using NIMs via API.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No locally deployed NIM LLM is found. Will be using NIMs via API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: NVIDIA NIM Integration\n",
    "\n",
    "## ğŸš€ NVIDIA NIM - Optimized Inference Microservices\n",
    "\n",
    "NVIDIA NIM provides high-performance inference for state-of-the-art language models. Our flood intelligence system uses several NVIDIA models:\n",
    "\n",
    "### Available Models\n",
    "\n",
    "1. **nvidia/llama-3.3-nemotron-super-49b-v1.5** (Default)\n",
    "   - Latest Nemotron model optimized for instruction following\n",
    "   - Excellent for agent workflows and tool calling\n",
    "   - 49B parameters with superior efficiency\n",
    "\n",
    "2. **meta/llama-3.1-70b-instruct**\n",
    "   - Strong general-purpose reasoning\n",
    "   - Great for complex analysis tasks\n",
    "\n",
    "### Let's test NVIDIA NIM integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NVIDIA NIM client\n",
    "# If LOCAL_NIM_AVAILABLE is True, it will be preferred. Please modify this cell to use external NVIDIA API.\n",
    "\n",
    "USE_NVIDIA_API = False\n",
    "\n",
    "if LOCAL_NIM_AVAILABLE:\n",
    "    print(f\"Using local NIM LLM base URL: {nim_llm_base_url}\")\n",
    "    model = \"nvidia/llama-3_3-nemotron-super-49b-v1_5\"\n",
    "    client = OpenAI(\n",
    "        base_url=nim_llm_base_url,\n",
    "        api_key=\"no-key\",\n",
    "    )\n",
    "else:\n",
    "    USE_NVIDIA_API = True\n",
    "\n",
    "    # Test with Nemotron Super 49B\n",
    "    model = \"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "\n",
    "    # From Nvidia API\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key=NVIDIA_API_KEY,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ğŸ¤– Testing NVIDIA NIM with {model}\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert flood prediction assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the key factors that indicate an increased risk of flooding in a river basin?\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nğŸ“Š Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Response Example\n",
    "\n",
    "NVIDIA NIM supports streaming for real-time responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸŒŠ Streaming response about flood intelligence...\\n\")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise flood intelligence expert.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain how AI can improve flood forecasting accuracy.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nâœ… Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "Let's compare responses from different NVIDIA models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_NVIDIA_API:\n",
    "    models_to_test = [\n",
    "        \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "        \"meta/llama-3.1-70b-instruct\",\n",
    "    ]\n",
    "else:\n",
    "    models_to_test = [model]\n",
    "\n",
    "question = \"Given streamflow of 2500 CFS and rising 200 CFS/hour, should we issue a flood alert?\"\n",
    "\n",
    "print(f\"ğŸ“Š Comparing model responses for:\\n'{question}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\nğŸ¤– Model: {model_name.split('/')[-1]}\\n\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a flood emergency expert. Be concise.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "\n",
    "        print(response.choices[0].message.content)\n",
    "        print(f\"\\nğŸ“ˆ Tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-Judge Evaluation\n",
    "\n",
    "The system includes an automatic evaluation feature using **cross-provider LLM-as-Judge**:\n",
    "- When NVIDIA models generate responses, h2oGPTe judges them\n",
    "- When h2oGPTe generates responses, NVIDIA models judge them\n",
    "- This provides unbiased evaluation of response quality\n",
    "\n",
    "Let's evaluate the NVIDIA model responses using our evaluation API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate one of the model responses from above\n",
    "question = \"Given streamflow of 2500 CFS and rising 200 CFS/hour, should we issue a flood alert?\"\n",
    "\n",
    "# Response from meta/llama-3.1-70b-instruct (from cell 12)\n",
    "response_text = \"\"\"Streamflow is already high (2500 CFS) and rising rapidly (200 CFS/hour). I recommend issuing a flood alert immediately. The rapid increase in streamflow indicates a high risk of flooding, and prompt action is necessary to protect people and property.\"\"\"\n",
    "\n",
    "print(\"ğŸ” Evaluating NVIDIA model response using LLM-as-Judge...\\n\")\n",
    "\n",
    "# Call the evaluation API\n",
    "eval_payload = {\n",
    "    \"question\": question,\n",
    "    \"response\": response_text,\n",
    "    \"model\": \"meta/llama-3.1-70b-instruct\",\n",
    "    \"agent_used\": False,\n",
    "    \"response_provider\": \"nvidia\",  # This will trigger h2oGPTe as the judge\n",
    "}\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/evaluation/evaluate\", json=eval_payload, headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    eval_result = response.json()\n",
    "\n",
    "    print(\"âœ… Evaluation Complete!\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ“Š Evaluation Metrics:\\n\")\n",
    "\n",
    "    metrics = eval_result.get(\"metrics\", {})\n",
    "    print(f\"   ğŸ¯ Overall Score:    {metrics.get('overall', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ¤ Helpfulness:      {metrics.get('helpfulness', 0):.1f}/10\")\n",
    "    print(f\"   âœ… Accuracy:         {metrics.get('accuracy', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ¯ Relevance:        {metrics.get('relevance', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ“ Coherence:        {metrics.get('coherence', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ›¡ï¸  Safety:           {metrics.get('safety', 0):.1f}/10\")\n",
    "    print(f\"   ğŸ’ª Confidence:       {metrics.get('confidence', 0):.1%}\")\n",
    "\n",
    "    print(\"\\nğŸ’­ Judge's Reasoning:\")\n",
    "    print(f\"   {eval_result.get('reasoning', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\nâ±ï¸  Evaluation Duration: {eval_result.get('duration_ms', 0)}ms\")\n",
    "    print(f\"ğŸ†” Evaluation ID: {eval_result.get('evaluation_id', 'N/A')}\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: h2oGPTe Agent (A2A) Integration\n",
    "\n",
    "## ğŸ§  h2oGPTe - Enterprise AI with Agent Mode\n",
    "\n",
    "h2oGPTe provides advanced AutoML capabilities through its agent mode, enabling:\n",
    "\n",
    "- **Driverless AI Integration**: Automated machine learning with minimal code\n",
    "- **Agent-to-Agent (A2A)**: AI agents that can invoke other AI agents\n",
    "- **Feature Engineering**: Automatic feature creation for time-series data\n",
    "- **Model Interpretability**: Explainable AI for emergency response decisions\n",
    "\n",
    "### Setting up h2oGPTe Client\n",
    "\n",
    "**Note**: This section requires h2oGPTe credentials. If you don't have access, you can skip to the next section.\n",
    "\n",
    "Get your credentials at: [H2O.ai Enterprise](https://h2o.ai/platform/enterprise-h2ogpte/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h2oGPTe for Flood Prediction ML\n",
    "\n",
    "Let's use h2oGPTe's agent mode to get guidance on training a flood prediction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if H2OGPTE_AVAILABLE:\n",
    "    headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "    # Using FastAPI streaming endpoint for h2oGPTe\n",
    "    url = f\"{api_server_base_url}/api/ai/chat/enhanced/stream\"\n",
    "\n",
    "    payload = {\n",
    "        \"message\": \"\"\"I have flood prediction data with these features:\n",
    "        - streamflow_cfs: Current river flow rate\n",
    "        - rainfall_24h: Rainfall in last 24 hours\n",
    "        - river_stage_ft: Water level\n",
    "        - soil_moisture: Ground saturation\n",
    "        - elevation_ft: Location elevation\n",
    "\n",
    "        How should I approach building an ML model to predict flood risk in the next 24 hours?\n",
    "        What feature engineering would you recommend?\"\"\",\n",
    "        \"provider\": \"h2ogpte\",\n",
    "        \"use_agent\": True,\n",
    "        \"max_tokens\": 8192 * 10,\n",
    "    }\n",
    "\n",
    "    print(\"ğŸ§  Consulting h2oGPTe agent for AutoML guidance...\\n\")\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        last_content = \"\"\n",
    "        # Stream the response\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode(\"utf-8\")\n",
    "                if line_str.startswith(\"data: \"):\n",
    "                    data_str = line_str[6:]  # Remove 'data: ' prefix\n",
    "                    try:\n",
    "                        data = json.loads(data_str)\n",
    "\n",
    "                        # First message contains provider info\n",
    "                        if \"provider\" in data:\n",
    "                            print(f\"ğŸ“¡ Provider: {data.get('provider')}\")\n",
    "                            print(f\"ğŸ¤– Model: {data.get('model')}\\n\")\n",
    "                            print(\"ğŸ“ Response:\\n\")\n",
    "\n",
    "                        # h2oGPTe sends incremental chunks with full content\n",
    "                        elif \"chunk\" in data and not data.get(\"done\", False):\n",
    "                            new_content = data[\"chunk\"]\n",
    "                            # Only print the new portion\n",
    "                            if new_content.startswith(last_content):\n",
    "                                new_part = new_content[len(last_content) :]\n",
    "                                print(new_part, end=\"\", flush=True)\n",
    "                                last_content = new_content\n",
    "\n",
    "                        # Check for completion\n",
    "                        elif data.get(\"done\", False):\n",
    "                            break\n",
    "\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "\n",
    "        print(\"\\n\\nâœ… Streaming complete!\")\n",
    "    else:\n",
    "        print(f\"âŒ Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping h2oGPTe demo (credentials not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Multi-Agent System with FastMCP\n",
    "\n",
    "## ğŸ¤ FastMCP - Model Context Protocol Server\n",
    "\n",
    "Our flood intelligence system uses FastMCP to expose 20+ specialized tools across 5 intelligent agents:\n",
    "\n",
    "### The 5 Agents\n",
    "\n",
    "1. **Data Collector Agent** ğŸ“Š\n",
    "   - Collects USGS water data\n",
    "   - Retrieves NOAA flood forecasts\n",
    "   - Gathers weather information\n",
    "   - Monitors data quality\n",
    "\n",
    "2. **Risk Analyzer Agent** âš ï¸\n",
    "   - Calculates flood risk scores\n",
    "   - Analyzes trends and patterns\n",
    "   - Identifies high-risk areas\n",
    "\n",
    "3. **Emergency Responder Agent** ğŸš¨\n",
    "   - Assesses emergency readiness\n",
    "   - Activates alerts\n",
    "   - Coordinates evacuations\n",
    "\n",
    "4. **AI Predictor Agent** ğŸ”®\n",
    "   - Generates flood forecasts\n",
    "   - Predicts critical conditions\n",
    "   - Analyzes prediction accuracy\n",
    "\n",
    "5. **H2OGPTE ML Agent** ğŸ§ \n",
    "   - Trains ML models\n",
    "   - Optimizes features\n",
    "   - Analyzes model performance\n",
    "\n",
    "### Let's explore the available tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all agents and their capabilities\n",
    "response = requests.get(f\"{api_server_base_url}/api/agents\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    print(\"ğŸ¤– Available Agents and Their Status\\n\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # The API returns a nested structure with agents dictionary\n",
    "    agents_dict = data.get(\"agents\", {})\n",
    "\n",
    "    for agent_key, agent_data in agents_dict.items():\n",
    "        status = \"ğŸŸ¢\" if agent_data.get(\"is_running\") else \"ğŸ”´\"\n",
    "        print(f\"\\n{status} {agent_data.get('name', agent_key)}\")\n",
    "        print(f\"   Description: {agent_data.get('description', 'N/A')}\")\n",
    "        print(f\"   Status: {'Running' if agent_data.get('is_running') else 'Stopped'}\")\n",
    "        if agent_data.get(\"last_check\"):\n",
    "            print(f\"   Last Check: {agent_data.get('last_check')}\")\n",
    "        if agent_data.get(\"check_interval\"):\n",
    "            print(f\"   Check Interval: {agent_data.get('check_interval')} seconds\")\n",
    "        if agent_data.get(\"insights_count\"):\n",
    "            print(f\"   Insights: {agent_data.get('insights_count')}\")\n",
    "        if agent_data.get(\"active_alerts_count\"):\n",
    "            print(f\"   Active Alerts: {agent_data.get('active_alerts_count')}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(f\"âŒ Error fetching agents: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Agent Insights\n",
    "\n",
    "Agents continuously monitor flood conditions and generate insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh USGS data before getting insights\n",
    "print(\"ğŸ”„ Refreshing USGS data first...\\n\")\n",
    "\n",
    "# Note: Using local-token for development mode (server.py allows this when OIDC is disabled)\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "refresh_response = requests.post(\n",
    "    f\"{api_server_base_url}/api/dashboard/refresh-usgs-data\", headers=headers\n",
    ")\n",
    "\n",
    "if refresh_response.status_code == 200:\n",
    "    refresh_result = refresh_response.json()\n",
    "    print(f\"âœ… {refresh_result.get('message', 'Data refresh initiated')}\")\n",
    "\n",
    "    # Wait a moment for background job to start\n",
    "    print(\"â³ Waiting for data refresh to process...\\n\")\n",
    "    time.sleep(3)\n",
    "else:\n",
    "    print(f\"âš ï¸  Data refresh returned status {refresh_response.status_code}\")\n",
    "    print(\"   Proceeding with existing data...\\n\")\n",
    "\n",
    "# Get insights from all agents\n",
    "response = requests.get(f\"{api_server_base_url}/api/agents/insights\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    print(\"ğŸ’¡ Agent Insights\\n\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # The API returns insights grouped by agent\n",
    "    insights_by_agent = data.get(\"insights\", {})\n",
    "\n",
    "    count = 0\n",
    "    for agent_name, agent_insights in insights_by_agent.items():\n",
    "        print(f\"\\nğŸ¤– {agent_name.replace('_', ' ').title()}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for insight in agent_insights:\n",
    "            title = insight.get(\"title\", \"N/A\")\n",
    "            value = insight.get(\"value\", \"N/A\")\n",
    "            change = insight.get(\"change\")\n",
    "            urgency = insight.get(\"urgency\", \"normal\")\n",
    "            timestamp = insight.get(\"timestamp\", \"\")\n",
    "\n",
    "            urgency_icon = {\n",
    "                \"critical\": \"ğŸ”´\",\n",
    "                \"high\": \"ğŸŸ¡\",\n",
    "                \"normal\": \"ğŸ”µ\",\n",
    "                \"low\": \"ğŸŸ¢\",\n",
    "            }.get(urgency, \"âšª\")\n",
    "\n",
    "            print(f\"\\n{urgency_icon} {title}: {value}\")\n",
    "            if change:\n",
    "                print(f\"   Change: {change}\")\n",
    "\n",
    "            count += 1\n",
    "            if count >= 15:  # Limit total insights shown\n",
    "                break\n",
    "\n",
    "        if count >= 15:\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Generated at: {data.get('generated_at', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"âŒ Error fetching insights: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Risk Analyzer Agent\n",
    "\n",
    "Calculates comprehensive flood risk scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Risk Analyzer Agent via NAT\n",
    "payload = {\n",
    "    \"agent\": \"risk_analyzer\",\n",
    "    \"message\": \"\"\"Analyze current flood risk for the Texas:\n",
    "    1. Calculate detailed risk scores for all factors\n",
    "    2. Identify the highest risk components\n",
    "    3. Provide trend analysis\n",
    "    4. Give recommendations for monitoring\n",
    "\n",
    "    Be specific about the risk levels and factors.\"\"\",\n",
    "}\n",
    "\n",
    "print(\"Running Risk Analyzer Agent...\\n\")\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/nat/chat/stream\",\n",
    "    json=payload,\n",
    "    headers=headers,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    final_output = None\n",
    "\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode(\"utf-8\")\n",
    "            if line_str.startswith(\"data: \"):\n",
    "                data_str = line_str[6:]  # Remove 'data: ' prefix\n",
    "                try:\n",
    "                    data = json.loads(data_str)\n",
    "\n",
    "                    # Handle different event types\n",
    "                    event_type = data.get(\"type\")\n",
    "\n",
    "                    if event_type == \"start\":\n",
    "                        print(f\"ğŸš€ Starting {data.get('agent_type')} agent...\")\n",
    "                        print()\n",
    "\n",
    "                    elif event_type == \"log\":\n",
    "                        log_entry = data.get(\"log\", {})\n",
    "                        level = log_entry.get(\"level\", \"INFO\")\n",
    "                        message = log_entry.get(\"message\", \"\")\n",
    "\n",
    "                        # Show important logs\n",
    "                        if level in [\"WARNING\", \"ERROR\"]:\n",
    "                            print(f\"[{level}] {message}\")\n",
    "                        elif (\n",
    "                            \"Agent\" in message\n",
    "                            or \"Final Answer\" in message\n",
    "                            or \"Tool\" in message\n",
    "                        ):\n",
    "                            print(f\"ğŸ’¬ {message}\")\n",
    "\n",
    "                    elif event_type == \"result\":\n",
    "                        final_output = data.get(\"output\")\n",
    "                        print(\"\\n\" + \"=\" * 80)\n",
    "                        print(\"âœ… Data Collection Complete!\\n\")\n",
    "\n",
    "                    elif event_type == \"error\":\n",
    "                        print(f\"\\nâŒ Error: {data.get('error')}\")\n",
    "\n",
    "                    elif event_type == \"done\":\n",
    "                        break\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "\n",
    "    # Display final output\n",
    "    if final_output:\n",
    "        print(final_output)\n",
    "        print(\"=\" * 80)\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: H2OGPTE ML Agent\n",
    "\n",
    "AutoML agent for model training and optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run H2OGPTE ML Agent via NAT\n",
    "payload = {\n",
    "    \"agent\": \"h2ogpte_agent\",\n",
    "    \"message\": \"\"\"Help me design an ML pipeline for flood prediction:\n",
    "    1. What features should I engineer from raw sensor data?\n",
    "    2. What model types work best for flood forecasting?\n",
    "    3. How should I handle imbalanced flood event data?\n",
    "    4. What validation strategy is appropriate for time-series?\n",
    "\n",
    "    Provide actionable AutoML recommendations.\"\"\",\n",
    "}\n",
    "headers = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "print(\"ğŸ§  Running H2OGPTE ML Agent...\\n\")\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/nat/chat\", json=payload, headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"âœ… ML Recommendations Complete!\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result.get(\"response\", result))\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Real-World Data Integration\n",
    "\n",
    "## ğŸŒ Live Data from Government APIs\n",
    "\n",
    "Our system integrates with real-time data sources:\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "1. **USGS Water Services**\n",
    "   - Real-time streamflow (CFS)\n",
    "   - Gage height (feet)\n",
    "   - 12 monitoring stations in Texas\n",
    "   - Updated every 15 minutes\n",
    "\n",
    "2. **NOAA Weather Service**\n",
    "   - Flood warnings and watches\n",
    "   - Weather alerts\n",
    "   - Forecast data\n",
    "\n",
    "3. **Open-Meteo**\n",
    "   - Weather forecasts\n",
    "   - Flood API predictions\n",
    "   - Historical data\n",
    "\n",
    "### Let's view live watershed data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refresh USGS Data Manually\n",
    "\n",
    "Trigger a fresh data collection from USGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ Triggering USGS data refresh...\\n\")\n",
    "header = {\"Authorization\": \"Bearer local-token\"}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{api_server_base_url}/api/dashboard/refresh-usgs-data\", headers=header\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    time.sleep(5)\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "\n",
    "# Get current watershed data\n",
    "response = requests.get(f\"{api_server_base_url}/api/watersheds\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    watersheds = response.json()\n",
    "\n",
    "    # Convert to DataFrame for nice display\n",
    "    df = pd.DataFrame(watersheds)\n",
    "\n",
    "    # Select key columns\n",
    "    display_cols = [\n",
    "        \"name\",\n",
    "        \"current_streamflow_cfs\",\n",
    "        \"risk_score\",\n",
    "        \"trend_rate_cfs_per_hour\",\n",
    "        \"last_updated\",\n",
    "    ]\n",
    "\n",
    "    available_cols = [col for col in display_cols if col in df.columns]\n",
    "\n",
    "    print(\"ğŸŒŠ Live Watershed Data\\n\")\n",
    "    print(\"=\" * 100)\n",
    "    print(df[available_cols].to_string(index=False))\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\nğŸ“Š Total Watersheds Monitored: {len(watersheds)}\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    if \"risk_score\" in df.columns:\n",
    "        high_risk = len(df[df[\"risk_score\"] > 7.0])\n",
    "        medium_risk = len(df[(df[\"risk_score\"] >= 4.0) & (df[\"risk_score\"] <= 7.0)])\n",
    "        low_risk = len(df[df[\"risk_score\"] < 4.0])\n",
    "\n",
    "        print(\"\\nâš ï¸  Risk Distribution:\")\n",
    "        print(f\"   ğŸ”´ High Risk (>7.0): {high_risk}\")\n",
    "        print(f\"   ğŸŸ¡ Medium Risk (4.0-7.0): {medium_risk}\")\n",
    "        print(f\"   ğŸŸ¢ Low Risk (<4.0): {low_risk}\")\n",
    "else:\n",
    "    print(f\"âŒ Error fetching watersheds: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Summary & Next Steps\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "In this notebook, you've learned how to:\n",
    "\n",
    "âœ… **Set up a multi-agent AI system** for disaster response  \n",
    "âœ… **Integrate h2oGPTe** for AutoML and model training  \n",
    "âœ… **Use NVIDIA NIM** for high-performance inference  \n",
    "âœ… **Build NAT agent workflows** with React patterns  \n",
    "âœ… **Implement FastMCP servers** with custom tools  \n",
    "âœ… **Integrate real-time data** from government APIs  \n",
    "âœ… **Coordinate multiple AI agents** for complex tasks  \n",
    "âœ… **Evaluate responses** using LLM-as-Judge  \n",
    "\n",
    "## Architecture Highlights\n",
    "\n",
    "- **5 Specialized Agents**: Data Collector, Risk Analyzer, Emergency Responder, Predictor, H2OGPTE ML\n",
    "- **20+ MCP Tools**: Via FastMCP server on port 8001\n",
    "- **NVIDIA NIM Models**: Nemotron Super 49B, Llama 3.1 variants\n",
    "- **h2oGPTe A2A**: Agent-to-agent AutoML capabilities\n",
    "- **Real-time Data**: USGS, NOAA, Weather APIs\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **NVIDIA NIM**: [build.nvidia.com](https://build.nvidia.com)\n",
    "- **h2oGPTe**: [h2o.ai/platform/enterprise-h2ogpte](https://h2o.ai/platform/enterprise-h2ogpte/)\n",
    "- **NVIDIA NAT**: [docs.nvidia.com/nat](https://docs.nvidia.com/nat)\n",
    "- **FastMCP**: [github.com/jlowin/fastmcp](https://github.com/jlowin/fastmcp)\n",
    "- **USGS Water Data**: [waterdata.usgs.gov](https://waterdata.usgs.gov)\n",
    "- **NOAA Weather**: [weather.gov](https://weather.gov)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Customize Agents**: Modify agent configs for your specific use case\n",
    "2. **Add Data Sources**: Integrate additional APIs and sensors\n",
    "3. **Train ML Models**: Use h2oGPTe to train production models\n",
    "4. **Deploy to Production**: Use Docker/Kubernetes deployment\n",
    "5. **Monitor Performance**: Add logging and metrics\n",
    "\n",
    "## Contributing\n",
    "\n",
    "This is an open-source AI for Good project. Contributions welcome!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŠ Thank you for exploring the Flood Intelligence Blueprint!\n",
    "\n",
    "**Built with â¤ï¸ using h2oGPTe and NVIDIA NIM**\n",
    "\n",
    "For questions and support, please open an issue in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h2oai-flood-prediction-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
